{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqaWEy4NUscxspGx8goFPP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanDCunha/Hands-On-Machine-Learning-with-Scikit-Learn-and-PyTorch/blob/main/Chapter7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7. Dimensionality Reduction\n"
      ],
      "metadata": {
        "id": "OEReYhze1VT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many machine learning problems involve thousands or even millions\n",
        "of features for each training instance.\n"
      ],
      "metadata": {
        "id": "9kCZTndz1Wwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not only do all these features make training extremely slow,\n",
        "but they can also make it much harder to find a good solution.\n"
      ],
      "metadata": {
        "id": "pPIq0AJb1Xqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This problem is often referred to as the **curse of dimensionality**.\n"
      ],
      "metadata": {
        "id": "ZxvjPX7T1Yg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fortunately, in real-world problems, it is often possible\n",
        "to reduce the number of features considerably,\n",
        "turning an intractable problem into a tractable one.\n"
      ],
      "metadata": {
        "id": "d_gWnQK91b3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, consider the MNIST images.\n"
      ],
      "metadata": {
        "id": "xKjqiE811c0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pixels on the image borders are almost always white,\n",
        "so they can be dropped without losing much information.\n"
      ],
      "metadata": {
        "id": "MHWsvElY1eCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, neighboring pixels are often highly correlated.\n"
      ],
      "metadata": {
        "id": "98YqgALW1e5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If two neighboring pixels are merged into one\n",
        "(for example, by averaging their intensities),\n",
        "little information is lost while redundancy is reduced.\n"
      ],
      "metadata": {
        "id": "sBGlGc-F1gGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing redundancy can sometimes reduce noise\n",
        "and improve model performance.\n"
      ],
      "metadata": {
        "id": "DNZXBK391hMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⚠️ WARNING\n"
      ],
      "metadata": {
        "id": "Qs4yIg-f1is7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reducing dimensionality can also drop useful information.\n"
      ],
      "metadata": {
        "id": "hPsaruLM1jhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is similar to compressing an image to JPEG:\n",
        "the file becomes smaller, but quality may degrade.\n"
      ],
      "metadata": {
        "id": "RSbc78Bd1kgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If dimensionality is reduced too aggressively,\n",
        "model performance may suffer.\n"
      ],
      "metadata": {
        "id": "vcOniGco1mIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moreover, some models—such as neural networks—\n",
        "can handle high-dimensional data efficiently.\n"
      ],
      "metadata": {
        "id": "SfdkITwu1nDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These models can often learn useful representations\n",
        "and reduce dimensionality internally.\n"
      ],
      "metadata": {
        "id": "0Pf0zq7O1oLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a result, dimensionality reduction is not always helpful\n",
        "and should be applied thoughtfully.\n"
      ],
      "metadata": {
        "id": "KXkyRjwe1pEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a result, dimensionality reduction is not always helpful\n",
        "and should be applied thoughtfully.\n"
      ],
      "metadata": {
        "id": "SI37QDBW1p9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction is also extremely useful\n",
        "for data visualization.\n"
      ],
      "metadata": {
        "id": "zfiZYuhS1rtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reducing data to two or three dimensions\n",
        "makes it possible to plot high-dimensional datasets.\n"
      ],
      "metadata": {
        "id": "IdyPqvKG1s3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization often reveals patterns such as clusters,\n",
        "outliers, or nonlinear structures.\n"
      ],
      "metadata": {
        "id": "rzZBlVA82QlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is especially important when communicating results\n",
        "to non–data scientists and decision makers.\n"
      ],
      "metadata": {
        "id": "GzILyacp2RX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Curse of Dimensionality\n"
      ],
      "metadata": {
        "id": "baIkNpty3QFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are so used to living in three dimensions\n",
        "that our intuition fails us in high-dimensional spaces.\n"
      ],
      "metadata": {
        "id": "Dw1qDry53RRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even a simple 4D hypercube is extremely difficult to visualize,\n",
        "let alone a 200-dimensional ellipsoid in a 1,000-dimensional space.\n"
      ],
      "metadata": {
        "id": "KFe1zYOT3Sqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 7-1 illustrates hypercubes from 0D to 4D:\n",
        "a point, a segment, a square, a cube, and a tesseract.\n"
      ],
      "metadata": {
        "id": "ICiC7s_q3Tv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many properties of space behave very differently\n",
        "as dimensionality increases.\n"
      ],
      "metadata": {
        "id": "htJepBlP3Vfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a random point inside a unit square (1 × 1).\n"
      ],
      "metadata": {
        "id": "AplE9UHR301y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is only about a 0.4% chance that the point\n",
        "lies within 0.001 of the border.\n"
      ],
      "metadata": {
        "id": "e3Rdr4af312V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In other words, points in low-dimensional space\n",
        "are rarely \"extreme\" along any dimension.\n"
      ],
      "metadata": {
        "id": "P7qfVR0f36ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now consider a 10,000-dimensional unit hypercube.\n"
      ],
      "metadata": {
        "id": "1FszqLRa377a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The probability that a random point lies within 0.001\n",
        "of the border exceeds 99.999999%.\n"
      ],
      "metadata": {
        "id": "VWRYeRTj3856"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In high dimensions, most points lie very close to the border.\n"
      ],
      "metadata": {
        "id": "8yRafIKd3gEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distances Grow with Dimensionality\n"
      ],
      "metadata": {
        "id": "ECzQc08C3jjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distances between random points also behave strangely\n",
        "in high-dimensional space.\n"
      ],
      "metadata": {
        "id": "K7gEE0Wr3loq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a unit square, the average distance between two random points\n",
        "is approximately 0.52.\n"
      ],
      "metadata": {
        "id": "TULevnuy3sDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a 3D unit cube, the average distance increases to about 0.66.\n"
      ],
      "metadata": {
        "id": "vvtp99Ar3s5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But in a 1,000,000-dimensional unit hypercube,\n",
        "the average distance is about 408.25.\n"
      ],
      "metadata": {
        "id": "xCfbVtZq3uC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This seems impossible at first:\n",
        "how can points be so far apart inside a unit hypercube?\n"
      ],
      "metadata": {
        "id": "OECsevAx3u9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer is that high-dimensional spaces\n",
        "contain an enormous amount of volume.\n"
      ],
      "metadata": {
        "id": "iGl4sY023vxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Consequences for Machine Learning\n"
      ],
      "metadata": {
        "id": "RIPt014O3whx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "High-dimensional datasets are typically very sparse.\n"
      ],
      "metadata": {
        "id": "NTiHKQbp3xYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most training instances are far away from each other,\n",
        "which makes distance-based methods less effective.\n"
      ],
      "metadata": {
        "id": "yOPopQFb3y5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algorithms such as k-nearest neighbors\n",
        "suffer significantly from this effect.\n"
      ],
      "metadata": {
        "id": "8J63ZGDJ3zw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some models scale poorly with dimensionality\n",
        "and may become impractical.\n"
      ],
      "metadata": {
        "id": "NimI-Epy30aw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples include support vector machines\n",
        "and dense neural networks.\n"
      ],
      "metadata": {
        "id": "_BmZv8I_31MN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "New instances are also likely to be far from all training points,\n",
        "making predictions less reliable.\n"
      ],
      "metadata": {
        "id": "vIAGUDSf31_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patterns become harder to detect,\n",
        "so models are more likely to fit noise.\n"
      ],
      "metadata": {
        "id": "vLAyLWEM33Np"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As dimensionality increases,\n",
        "regularization becomes increasingly important.\n"
      ],
      "metadata": {
        "id": "8GkURDgQ34F4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models also become more difficult to interpret.\n"
      ],
      "metadata": {
        "id": "omE1Wp4I3435"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why More Data Is Not a Practical Fix\n"
      ],
      "metadata": {
        "id": "6SUrb0v835nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In theory, these problems could be mitigated\n",
        "by increasing the size of the training set.\n"
      ],
      "metadata": {
        "id": "OZT6FSut36UV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, the number of required training instances\n",
        "grows exponentially with dimensionality.\n"
      ],
      "metadata": {
        "id": "k_0I56yy37IJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With just 100 features ranging from 0 to 1,\n",
        "you would need more training instances\n",
        "than atoms in the observable universe\n",
        "to achieve reasonable density.\n"
      ],
      "metadata": {
        "id": "zxZSb_gV38U-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This makes dimensionality reduction\n",
        "a practical necessity in many real-world problems.\n"
      ],
      "metadata": {
        "id": "3bkqw-rZ39Fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Approaches for Dimensionality Reduction\n"
      ],
      "metadata": {
        "id": "YZ-F2Eoi4B9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before diving into specific dimensionality reduction algorithms,\n",
        "it is useful to understand the two main approaches:\n",
        "projection and manifold learning.\n"
      ],
      "metadata": {
        "id": "x2mX_pPz41we"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Projection\n"
      ],
      "metadata": {
        "id": "eBkI4kLb42sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most real-world problems, training instances\n",
        "are not spread uniformly across all dimensions.\n"
      ],
      "metadata": {
        "id": "S97xZpxc43lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many features are nearly constant,\n",
        "while others are highly correlated.\n"
      ],
      "metadata": {
        "id": "UTmxg6Sn44Zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a result, the data often lies within\n",
        "(or very close to) a lower-dimensional subspace\n",
        "of the higher-dimensional space.\n"
      ],
      "metadata": {
        "id": "8x4EAjBF45Yf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although this may sound abstract,\n",
        "it becomes clearer with an example.\n"
      ],
      "metadata": {
        "id": "1umxFXM546Ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a 3D dataset represented by points in space,\n",
        "as shown in Figure 7-2.\n"
      ],
      "metadata": {
        "id": "peWq45OZ47Hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All training instances lie close to a plane,\n",
        "which is a 2D subspace of the 3D space.\n"
      ],
      "metadata": {
        "id": "8BeM7WkF48mv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we project each instance perpendicularly\n",
        "onto this plane,\n",
        "we obtain a new 2D dataset.\n"
      ],
      "metadata": {
        "id": "20ujcpkr4-aG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This projection is illustrated by dashed lines\n",
        "connecting the original points to the plane.\n"
      ],
      "metadata": {
        "id": "xzIsScwT4_c0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 7-3 shows the resulting 2D dataset\n",
        "after projection.\n"
      ],
      "metadata": {
        "id": "SmlOntPn5AS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset’s dimensionality has been reduced\n",
        "from 3D to 2D.\n"
      ],
      "metadata": {
        "id": "PNEcczu35BPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new axes correspond to new features,\n",
        "denoted z₁ and z₂.\n"
      ],
      "metadata": {
        "id": "8Gy6LPYn5CTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These features are simply the coordinates\n",
        "of the projected points on the plane.\n"
      ],
      "metadata": {
        "id": "QIjoXzOl5DL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manifold Learning\n"
      ],
      "metadata": {
        "id": "Cg7FzG6P5EHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Projection is fast and often effective,\n",
        "but it is not always the best approach.\n"
      ],
      "metadata": {
        "id": "VpD6J1YJ5FAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In some datasets, the lower-dimensional structure\n",
        "twists and turns through the higher-dimensional space.\n"
      ],
      "metadata": {
        "id": "z08l8QvH5LTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A classic example is the Swiss roll dataset,\n",
        "shown in Figure 7-4.\n"
      ],
      "metadata": {
        "id": "grFOoOxM5MMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of 3D points\n",
        "arranged in the shape of a rolled sheet.\n"
      ],
      "metadata": {
        "id": "tcHrI9bw5ND6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simply projecting the data onto a plane\n",
        "would squash different layers of the Swiss roll together.\n"
      ],
      "metadata": {
        "id": "6Ppbej2z5N0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This effect is shown on the left side of Figure 7-5.\n"
      ],
      "metadata": {
        "id": "AVJOyc-X5Oly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we actually want is to \"unroll\" the Swiss roll\n",
        "into a flat 2D representation.\n"
      ],
      "metadata": {
        "id": "r8HLTLET5Puc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct unrolled result\n",
        "is shown on the right side of Figure 7-5.\n"
      ],
      "metadata": {
        "id": "eBbo8WFX5Qgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Swiss roll is an example of a 2D manifold.\n"
      ],
      "metadata": {
        "id": "j_iHX3Cl5R79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A 2D manifold is a surface that can be bent and twisted\n",
        "within a higher-dimensional space.\n"
      ],
      "metadata": {
        "id": "p1L8eNoS5S0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More generally, a d-dimensional manifold\n",
        "is a subset of an n-dimensional space (where d < n)\n",
        "that locally resembles a d-dimensional hyperplane.\n"
      ],
      "metadata": {
        "id": "-UB-GVXf5TrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Swiss roll example,\n",
        "d = 2 and n = 3.\n"
      ],
      "metadata": {
        "id": "fQ6cCUVd5UuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many dimensionality reduction algorithms\n",
        "are based on modeling this manifold structure.\n"
      ],
      "metadata": {
        "id": "3sS3QC7t5VxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This approach is called manifold learning.\n"
      ],
      "metadata": {
        "id": "Ec5WfL9t5W7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples include LLE, Isomap, t-SNE, and UMAP.\n"
      ],
      "metadata": {
        "id": "h9btgsIp5YQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manifold learning relies on the manifold assumption,\n",
        "also known as the manifold hypothesis.\n"
      ],
      "metadata": {
        "id": "0eRtSVFV5Zha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This hypothesis states that most real-world\n",
        "high-dimensional datasets lie close to\n",
        "a much lower-dimensional manifold.\n"
      ],
      "metadata": {
        "id": "AjkXI3mx5aXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assumption is frequently observed in practice.\n"
      ],
      "metadata": {
        "id": "ikhXap9s5bIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the MNIST dataset as an example.\n"
      ],
      "metadata": {
        "id": "ao4nvZZM5b6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handwritten digits share strong structural constraints:\n",
        "connected strokes, white borders, and centered content.\n"
      ],
      "metadata": {
        "id": "wg7-V0Ju5dCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only a tiny fraction of all possible images\n",
        "resemble handwritten digits.\n"
      ],
      "metadata": {
        "id": "xFgA97dd5d0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dramatically reduces the effective degrees of freedom,\n",
        "compressing the data into a lower-dimensional manifold.\n"
      ],
      "metadata": {
        "id": "qYrJfKI25elq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The manifold assumption is often paired\n",
        "with another implicit assumption.\n"
      ],
      "metadata": {
        "id": "1xNmbJFQ5fun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The learning task may become simpler\n",
        "when expressed in the manifold’s lower-dimensional space.\n"
      ],
      "metadata": {
        "id": "9UoDvosE5-Z4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Figure 7-6 (top row),\n",
        "a complex decision boundary in 3D\n",
        "becomes a straight line in the unrolled 2D space.\n"
      ],
      "metadata": {
        "id": "v8Gti2w65_S5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, this simplification does not always occur.\n"
      ],
      "metadata": {
        "id": "4GS_eDNa6AIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the bottom row of Figure 7-6,\n",
        "a simple decision boundary in 3D\n",
        "becomes more complex in the manifold space.\n"
      ],
      "metadata": {
        "id": "L4kp6khQ6BGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows that dimensionality reduction\n",
        "does not guarantee simpler decision boundaries.\n"
      ],
      "metadata": {
        "id": "yD9ZgXsB67NA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, dimensionality reduction usually speeds up training,\n",
        "but it may not always improve performance.\n"
      ],
      "metadata": {
        "id": "edvIp03U68rU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is most effective when:\n",
        "- the dataset is small relative to the number of features\n",
        "- the data is noisy\n",
        "- many features are highly correlated\n"
      ],
      "metadata": {
        "id": "EpwtEyNY69wj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If domain knowledge suggests\n",
        "the data-generating process is simple,\n",
        "the manifold assumption is likely valid.\n"
      ],
      "metadata": {
        "id": "jNKqMN2f7Av9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In such cases,\n",
        "dimensionality reduction can be very beneficial.\n"
      ],
      "metadata": {
        "id": "eBmv5aGq7Bgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The remainder of this chapter\n",
        "covers popular dimensionality reduction algorithms\n",
        "that exploit these ideas.\n"
      ],
      "metadata": {
        "id": "i_D-ODRD7CVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA (Principal Component Analysis)\n",
        "\n",
        "Principal Component Analysis (PCA) is the most popular dimensionality reduction algorithm.  \n",
        "It identifies the hyperplane that lies closest to the data and then projects the data onto it.\n",
        "\n",
        "The main goal of PCA is to **preserve as much variance as possible** while reducing the number of dimensions.\n"
      ],
      "metadata": {
        "id": "5dD-11xQ9cbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preserving the Variance\n",
        "\n",
        "When projecting data onto a lower-dimensional space, PCA selects the axes (principal components)\n",
        "that maximize the variance of the projected data.\n",
        "\n",
        "Preserving variance usually means preserving information.\n"
      ],
      "metadata": {
        "id": "6GPl8gE39dd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "aEi_lbQz1Vmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal Components via SVD\n",
        "\n",
        "PCA can be computed using **Singular Value Decomposition (SVD)**.\n",
        "Before applying SVD, the data must be **centered** around the origin.\n"
      ],
      "metadata": {
        "id": "yvUyBXU89f81"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL8MNO0Luqs1"
      },
      "outputs": [],
      "source": [
        "# Example 3D dataset\n",
        "X = np.array([\n",
        "    [2.5, 2.4, 1.2],\n",
        "    [0.5, 0.7, 0.3],\n",
        "    [2.2, 2.9, 1.7],\n",
        "    [1.9, 2.2, 1.1],\n",
        "    [3.1, 3.0, 1.9]\n",
        "])\n",
        "\n",
        "# Center the data\n",
        "X_centered = X - X.mean(axis=0)\n",
        "\n",
        "# Perform SVD\n",
        "U, s, Vt = np.linalg.svd(X_centered)\n",
        "\n",
        "# First two principal components\n",
        "c1 = Vt[0]\n",
        "c2 = Vt[1]\n",
        "\n",
        "c1, c2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each principal component is a **unit vector** pointing in the direction of maximum variance.\n",
        "\n",
        "Note:\n",
        "- The sign of principal components is not guaranteed.\n",
        "- PCA must be retrained if the dataset changes.\n"
      ],
      "metadata": {
        "id": "ETMaTNTY9h4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Projecting Down to d Dimensions\n",
        "\n",
        "Once the principal components are found, we can project the data\n",
        "onto the first `d` components to reduce dimensionality.\n"
      ],
      "metadata": {
        "id": "8xuhQrN19isE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Project onto the first 2 principal components\n",
        "W2 = Vt[:2].T\n",
        "X_2D = X_centered @ W2\n",
        "\n",
        "X_2D\n"
      ],
      "metadata": {
        "id": "_9rVvg-y9jo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Scikit-Learn PCA\n",
        "\n",
        "Scikit-Learn provides a PCA implementation that:\n",
        "- Automatically centers the data\n",
        "- Uses SVD internally\n"
      ],
      "metadata": {
        "id": "nh2t4F4g9keZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_2D_sklearn = pca.fit_transform(X)\n",
        "\n",
        "X_2D_sklearn\n"
      ],
      "metadata": {
        "id": "f7157pGJ9lPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `components_` attribute stores the principal components.\n",
        "Each row corresponds to one principal component.\n"
      ],
      "metadata": {
        "id": "gPlwSdC59mgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explained Variance Ratio\n",
        "\n",
        "The explained variance ratio tells us how much variance\n",
        "each principal component captures.\n"
      ],
      "metadata": {
        "id": "OCt0UWFS9nPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_ratio_\n"
      ],
      "metadata": {
        "id": "OspUw7Y79oDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing the Right Number of Dimensions\n",
        "\n",
        "A common approach is to keep enough components to preserve\n",
        "a target percentage (e.g., 95%) of the variance.\n"
      ],
      "metadata": {
        "id": "pECUJu-C9ozw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load MNIST\n",
        "mnist = fetch_openml('mnist_784', as_frame=False)\n",
        "X_train = mnist.data[:60_000]\n",
        "y_train = mnist.target[:60_000]\n",
        "\n",
        "# Fit PCA without dimensionality reduction\n",
        "pca = PCA()\n",
        "pca.fit(X_train)\n",
        "\n",
        "# Cumulative explained variance\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Minimum dimensions to preserve 95% variance\n",
        "d = np.argmax(cumsum >= 0.95) + 1\n",
        "d\n"
      ],
      "metadata": {
        "id": "gdZNM2yd9pnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of manually selecting `d`, you can directly specify\n",
        "the variance ratio to preserve.\n"
      ],
      "metadata": {
        "id": "JaC02_Rh9qfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X_train)\n",
        "\n",
        "pca.n_components_\n"
      ],
      "metadata": {
        "id": "0eKj0TIQ9rSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA as a Preprocessing Step (Pipeline)\n",
        "\n",
        "PCA is often combined with supervised models using a pipeline.\n"
      ],
      "metadata": {
        "id": "4eda420E-KcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "clf = make_pipeline(\n",
        "    PCA(random_state=42),\n",
        "    RandomForestClassifier(random_state=42)\n",
        ")\n",
        "\n",
        "param_distrib = {\n",
        "    \"pca__n_components\": np.arange(10, 80),\n",
        "    \"randomforestclassifier__n_estimators\": np.arange(50, 500)\n",
        "}\n",
        "\n",
        "rnd_search = RandomizedSearchCV(\n",
        "    clf,\n",
        "    param_distrib,\n",
        "    n_iter=10,\n",
        "    cv=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rnd_search.fit(X_train[:1000], y_train[:1000])\n",
        "rnd_search.best_params_\n"
      ],
      "metadata": {
        "id": "l_gmqDdC-LRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA for Compression\n",
        "\n",
        "PCA can compress data by reducing dimensionality and later\n",
        "reconstructing it using the inverse transformation.\n"
      ],
      "metadata": {
        "id": "ErYtPtps-MaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstruct the data\n",
        "X_recovered = pca.inverse_transform(X_reduced)\n"
      ],
      "metadata": {
        "id": "I4DIs4X5-Nya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Incremental PCA\n",
        "\n",
        "Incremental PCA allows PCA to be trained on large datasets\n",
        "using mini-batches.\n"
      ],
      "metadata": {
        "id": "wBpinvpX-OmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "n_batches = 100\n",
        "inc_pca = IncrementalPCA(n_components=154)\n",
        "\n",
        "for X_batch in np.array_split(X_train, n_batches):\n",
        "    inc_pca.partial_fit(X_batch)\n",
        "\n",
        "X_reduced_inc = inc_pca.transform(X_train)\n"
      ],
      "metadata": {
        "id": "O-WbTUDg-PYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Projection\n",
        "\n",
        "Random Projection reduces dimensionality by projecting data onto a\n",
        "randomly generated lower-dimensional subspace.\n",
        "\n",
        "Surprisingly, this technique preserves distances fairly well with high probability,\n",
        "as guaranteed by the **Johnson–Lindenstrauss lemma**.\n"
      ],
      "metadata": {
        "id": "7BEB2OD5-cxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Projection\n",
        "\n",
        "Random Projection reduces dimensionality by projecting data onto a\n",
        "randomly generated lower-dimensional subspace.\n",
        "\n",
        "Surprisingly, this technique preserves distances fairly well with high probability,\n",
        "as guaranteed by the **Johnson–Lindenstrauss lemma**.\n"
      ],
      "metadata": {
        "id": "1V_6aN5W-g1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "UokWUPtd-h69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing the Target Dimensionality\n",
        "\n",
        "Scikit-Learn provides a helper function that computes the minimum number\n",
        "of dimensions required to preserve distances within a tolerance ε.\n"
      ],
      "metadata": {
        "id": "fFzZBR6W-l-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
        "\n",
        "m = 5_000     # number of samples\n",
        "epsilon = 0.1  # maximum distortion\n",
        "\n",
        "d = johnson_lindenstrauss_min_dim(m, eps=epsilon)\n",
        "d\n"
      ],
      "metadata": {
        "id": "_C2NydsC-m4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, with 5,000 samples and ε = 0.1, we only need about 7,300 dimensions,\n",
        "even if the original dataset has tens of thousands of features.\n"
      ],
      "metadata": {
        "id": "qKmD2Y-O-nkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 20_000  # original number of features\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "# Random projection matrix\n",
        "P = rng.standard_normal((d, n)) / np.sqrt(d)\n",
        "\n",
        "# Fake high-dimensional dataset\n",
        "X = rng.standard_normal((m, n))\n",
        "\n",
        "# Project data\n",
        "X_reduced = X @ P.T\n",
        "\n",
        "X_reduced.shape\n"
      ],
      "metadata": {
        "id": "wKhTYcb5-oWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random projection is extremely fast:\n",
        "- No training required\n",
        "- Only depends on data shape\n",
        "- Very memory efficient compared to PCA\n",
        "\n",
        "This makes it ideal for text, genomics, and other very high-dimensional data.\n"
      ],
      "metadata": {
        "id": "nOoVWXs8-pY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gaussian Random Projection (Scikit-Learn)\n",
        "\n",
        "Scikit-Learn provides a built-in transformer that performs\n",
        "Gaussian random projection automatically.\n"
      ],
      "metadata": {
        "id": "sk5Xgxyj_9EH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "\n",
        "gaussian_rnd_proj = GaussianRandomProjection(\n",
        "    eps=epsilon,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_reduced_sklearn = gaussian_rnd_proj.fit_transform(X)\n",
        "X_reduced_sklearn.shape\n"
      ],
      "metadata": {
        "id": "prw7kL4L_9zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The random projection matrix is stored in the `components_` attribute.\n"
      ],
      "metadata": {
        "id": "WaQZgMjM_-08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gaussian_rnd_proj.components_.shape\n"
      ],
      "metadata": {
        "id": "3psXuUzT__ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sparse Random Projection\n",
        "\n",
        "Sparse Random Projection uses a sparse random matrix:\n",
        "- Much lower memory usage\n",
        "- Faster computation\n",
        "- Preserves sparsity of input data\n",
        "\n",
        "It usually performs just as well as Gaussian random projection.\n"
      ],
      "metadata": {
        "id": "1vZKQaLRAAjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.random_projection import SparseRandomProjection\n",
        "\n",
        "sparse_rnd_proj = SparseRandomProjection(\n",
        "    eps=epsilon,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_reduced_sparse = sparse_rnd_proj.fit_transform(X)\n",
        "X_reduced_sparse.shape\n"
      ],
      "metadata": {
        "id": "7-H6V36hABn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Sparse Random Projection?\n",
        "\n",
        "- Uses far less memory\n",
        "- Faster to compute\n",
        "- Ideal for large or sparse datasets (e.g., text data)\n",
        "\n",
        "By default, only about 1 in √n entries in the projection matrix is nonzero.\n"
      ],
      "metadata": {
        "id": "KQviyAhXACqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inverse Transformation (Approximate Reconstruction)\n",
        "\n",
        "Random projection does not directly support inverse transformation.\n",
        "However, we can approximate it using the pseudoinverse of the components matrix.\n"
      ],
      "metadata": {
        "id": "fhDHyrshADdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute pseudoinverse of projection matrix\n",
        "components_pinv = np.linalg.pinv(gaussian_rnd_proj.components_)\n",
        "\n",
        "# Reconstruct the data\n",
        "X_recovered = X_reduced_sklearn @ components_pinv.T\n",
        "\n",
        "X_recovered.shape\n"
      ],
      "metadata": {
        "id": "jZmIA-y_AELZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ Warning:\n",
        "\n",
        "Computing the pseudoinverse is expensive:\n",
        "- Complexity is O(dn²) or O(nd²)\n",
        "- Not recommended for very large projection matrices\n",
        "\n",
        "Random projection is mainly used for speed and scalability,\n",
        "not reconstruction accuracy.\n"
      ],
      "metadata": {
        "id": "tK4O_FhuAFJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "Random Projection is:\n",
        "- Simple\n",
        "- Extremely fast\n",
        "- Memory efficient\n",
        "- Surprisingly good at preserving distances\n",
        "\n",
        "It is especially useful for:\n",
        "- Very high-dimensional data\n",
        "- Sparse datasets\n",
        "- Situations where PCA is too slow or too expensive\n"
      ],
      "metadata": {
        "id": "9ALp-0jhAFzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Locally Linear Embedding (LLE)\n",
        "\n",
        "Locally Linear Embedding (LLE) is a **nonlinear dimensionality reduction**\n",
        "technique based on **manifold learning**.\n",
        "\n",
        "Unlike PCA or Random Projection, LLE does **not use projections**.\n",
        "Instead, it preserves **local neighborhood relationships**, making it\n",
        "especially effective at unrolling twisted manifolds.\n"
      ],
      "metadata": {
        "id": "xODZIwd_BKMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Idea Behind LLE\n",
        "\n",
        "LLE works in two main steps:\n",
        "\n",
        "1. For each data point, find its *k* nearest neighbors and compute\n",
        "   weights that best reconstruct the point as a linear combination\n",
        "   of its neighbors.\n",
        "\n",
        "2. Find a low-dimensional representation that preserves these\n",
        "   local linear relationships as closely as possible.\n"
      ],
      "metadata": {
        "id": "Pe3d1deoBLER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "7LghHspgBLxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Swiss Roll Dataset\n",
        "\n",
        "The Swiss roll is a classic example of a nonlinear manifold.\n",
        "Although it lives in 3D space, it can be unrolled into 2D.\n"
      ],
      "metadata": {
        "id": "c9SlE201BMog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "\n",
        "X_swiss, t = make_swiss_roll(\n",
        "    n_samples=1000,\n",
        "    noise=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_swiss.shape\n"
      ],
      "metadata": {
        "id": "iDRTom0sBNcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `X_swiss` contains the 3D coordinates of each point\n",
        "- `t` represents the position along the rolled dimension\n",
        "\n",
        "We will **not** use `t` here, but it is useful for visualization\n",
        "or regression tasks.\n"
      ],
      "metadata": {
        "id": "Q3WpGX0ZBOK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying Locally Linear Embedding\n",
        "\n",
        "We now use Scikit-Learn’s `LocallyLinearEmbedding` class to unroll\n",
        "the Swiss roll into 2 dimensions.\n"
      ],
      "metadata": {
        "id": "yLXQ4GYXBRhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "\n",
        "lle = LocallyLinearEmbedding(\n",
        "    n_components=2,\n",
        "    n_neighbors=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_unrolled = lle.fit_transform(X_swiss)\n",
        "X_unrolled.shape\n"
      ],
      "metadata": {
        "id": "gksyJB7lBPEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Swiss roll has now been unrolled into a 2D representation.\n",
        "\n",
        "LLE preserves **local distances** very well, even though\n",
        "**global distances may be distorted**.\n"
      ],
      "metadata": {
        "id": "njP57JQ-BUnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How LLE Works (Step 1: Local Linear Reconstruction)\n",
        "\n",
        "For each data point x(i):\n",
        "\n",
        "- Find its k nearest neighbors\n",
        "- Compute weights wᵢⱼ that best reconstruct x(i)\n",
        "  as a linear combination of its neighbors\n",
        "- Weights sum to 1\n",
        "- Non-neighbors have weight 0\n",
        "\n",
        "This step captures the **local geometry** of the manifold.\n"
      ],
      "metadata": {
        "id": "0YtIK1nDBVbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How LLE Works (Step 2: Low-Dimensional Embedding)\n",
        "\n",
        "Using the fixed weights from step 1:\n",
        "\n",
        "- Find low-dimensional points z(i)\n",
        "- Ensure that each z(i) is reconstructed from its neighbors\n",
        "  using the same weights\n",
        "\n",
        "This step produces the final low-dimensional embedding.\n"
      ],
      "metadata": {
        "id": "oM_NvYnjBWUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why LLE Works Well\n",
        "\n",
        "LLE excels when:\n",
        "- The data lies on a **nonlinear manifold**\n",
        "- Noise is relatively low\n",
        "- The dataset is small to medium in size\n",
        "\n",
        "It is particularly good at **unrolling twisted manifolds**.\n"
      ],
      "metadata": {
        "id": "qHbcYh74BXPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computational Complexity\n",
        "\n",
        "Scikit-Learn’s LLE implementation has the following costs:\n",
        "\n",
        "- Nearest neighbors search: O(m log(m) · n log(k))\n",
        "- Weight optimization: O(m n k³)\n",
        "- Low-dimensional embedding: O(d m²)\n",
        "\n",
        "⚠️ The **m² term** makes LLE scale poorly for very large datasets.\n"
      ],
      "metadata": {
        "id": "-Ws6MgJMBYKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations of LLE\n",
        "\n",
        "- Does not scale well to large datasets\n",
        "- Sensitive to noise\n",
        "- Global distances are not preserved\n",
        "- Choosing the number of neighbors `k` is critical\n",
        "\n",
        "Despite this, LLE often produces excellent results\n",
        "for clean, nonlinear manifolds.\n"
      ],
      "metadata": {
        "id": "MM0dAmcPBZUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "Locally Linear Embedding (LLE):\n",
        "\n",
        "- Is a **nonlinear** dimensionality reduction technique\n",
        "- Preserves **local neighborhood relationships**\n",
        "- Excels at unrolling nonlinear manifolds\n",
        "- Does **not** rely on projections\n",
        "- Is best suited for small to medium datasets\n"
      ],
      "metadata": {
        "id": "pWhQkiHQBaVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Dimensionality Reduction Techniques\n",
        "\n",
        "In addition to PCA, Random Projection, and LLE, Scikit-Learn provides\n",
        "several other powerful dimensionality reduction techniques.\n",
        "\n",
        "In this section, we explore:\n",
        "- Multidimensional Scaling (MDS)\n",
        "- Isomap\n",
        "- t-SNE\n",
        "- Linear Discriminant Analysis (LDA)\n"
      ],
      "metadata": {
        "id": "cKdiG7bsB0nI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "CkYYHosqB1hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset: Swiss Roll\n",
        "\n",
        "We will reuse the Swiss roll dataset to compare different\n",
        "dimensionality reduction techniques.\n"
      ],
      "metadata": {
        "id": "Gh6ab2ATB2fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "\n",
        "X_swiss, t = make_swiss_roll(\n",
        "    n_samples=1000,\n",
        "    noise=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_swiss.shape\n"
      ],
      "metadata": {
        "id": "IBfwlgSRB2SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multidimensional Scaling (MDS)\n",
        "\n",
        "Multidimensional Scaling (MDS) reduces dimensionality while trying\n",
        "to preserve **pairwise distances** between instances.\n",
        "\n",
        "It works well for **low-dimensional data**, but does not scale well\n",
        "to very large datasets.\n"
      ],
      "metadata": {
        "id": "zZ6f5wLxB4Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import MDS\n",
        "\n",
        "mds = MDS(\n",
        "    n_components=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_mds = mds.fit_transform(X_swiss)\n",
        "X_mds.shape\n"
      ],
      "metadata": {
        "id": "hyaewBLLB5U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MDS flattens the Swiss roll while preserving **global distance structure**.\n",
        "It tends to keep the overall curvature of the data.\n"
      ],
      "metadata": {
        "id": "oWnX93JhB6Lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Isomap\n",
        "\n",
        "Isomap builds a graph connecting each point to its nearest neighbors,\n",
        "then computes **geodesic distances** (shortest paths on the graph).\n",
        "\n",
        "It works best when the data lies on a smooth, low-dimensional manifold\n",
        "with a single global structure.\n"
      ],
      "metadata": {
        "id": "BJFRNrDRB6-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import Isomap\n",
        "\n",
        "isomap = Isomap(\n",
        "    n_components=2,\n",
        "    n_neighbors=10\n",
        ")\n",
        "\n",
        "X_isomap = isomap.fit_transform(X_swiss)\n",
        "X_isomap.shape\n"
      ],
      "metadata": {
        "id": "UXM1O8EIB70E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isomap often **completely unrolls** the Swiss roll, removing its curvature.\n",
        "This is great for some tasks, but can destroy useful global structure.\n"
      ],
      "metadata": {
        "id": "oFWUggCyB8oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
        "\n",
        "t-SNE focuses on keeping **similar instances close together**\n",
        "and **dissimilar ones far apart**.\n",
        "\n",
        "It is mainly used for **visualization**, not as a preprocessing step\n",
        "for machine learning models.\n"
      ],
      "metadata": {
        "id": "mE8XYOPnB9px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(\n",
        "    n_components=2,\n",
        "    perplexity=30,\n",
        "    learning_rate=\"auto\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_tsne = tsne.fit_transform(X_swiss)\n",
        "X_tsne.shape\n"
      ],
      "metadata": {
        "id": "rs2rV-ShB-W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-SNE:\n",
        "- Preserves **local neighborhoods**\n",
        "- Amplifies **clusters**\n",
        "- Often distorts global structure\n",
        "\n",
        "Excellent for visualization, poor for downstream modeling.\n"
      ],
      "metadata": {
        "id": "2c7OrjJWB_Gk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Discriminant Analysis (LDA)\n",
        "\n",
        "Linear Discriminant Analysis (LDA) is a **supervised** dimensionality\n",
        "reduction technique.\n",
        "\n",
        "It learns axes that **maximize class separability**, making it useful\n",
        "before classification models.\n"
      ],
      "metadata": {
        "id": "ug4R8ocYCAGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ LDA requires **class labels**, so it cannot be applied directly\n",
        "to the Swiss roll dataset.\n",
        "\n",
        "Below is a minimal example using synthetic labeled data.\n"
      ],
      "metadata": {
        "id": "NrSxL7EsCA_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "X_cls, y_cls = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_classes=3,\n",
        "    n_informative=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "lda = LinearDiscriminantAnalysis(n_components=2)\n",
        "X_lda = lda.fit_transform(X_cls, y_cls)\n",
        "\n",
        "X_lda.shape\n"
      ],
      "metadata": {
        "id": "aqZW4vrRCB3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA is especially effective when:\n",
        "- Classes are well separated\n",
        "- You want to reduce dimensionality **and** classify\n",
        "- A linear decision boundary is appropriate\n"
      ],
      "metadata": {
        "id": "NKlmPaRiCIZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UMAP (Not in Scikit-Learn)\n",
        "\n",
        "UMAP (Uniform Manifold Approximation and Projection) is a popular\n",
        "dimensionality reduction technique for visualization.\n",
        "\n",
        "- Preserves both local and global structure\n",
        "- Scales better than t-SNE\n",
        "- Not included in Scikit-Learn\n",
        "\n",
        "Available via the `umap-learn` package.\n"
      ],
      "metadata": {
        "id": "Z3-2ssxoCJFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of Techniques\n",
        "\n",
        "| Method | Type | Preserves | Typical Use |\n",
        "|------|-----|----------|------------|\n",
        "| MDS | Unsupervised | Pairwise distances | Small datasets |\n",
        "| Isomap | Unsupervised | Geodesic distances | Smooth manifolds |\n",
        "| t-SNE | Unsupervised | Local neighborhoods | Visualization |\n",
        "| LDA | Supervised | Class separation | Classification |\n",
        "| UMAP | Unsupervised | Local + global | Visualization |\n"
      ],
      "metadata": {
        "id": "1fZmgmSSCJ_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Takeaway\n",
        "\n",
        "There is **no single best dimensionality reduction technique**.\n",
        "\n",
        "The right choice depends on:\n",
        "- Dataset size\n",
        "- Linearity vs nonlinearity\n",
        "- Supervised vs unsupervised\n",
        "- Visualization vs modeling\n",
        "\n",
        "Understanding the trade-offs is key.\n"
      ],
      "metadata": {
        "id": "V_z97ZjeCK3m"
      }
    }
  ]
}