{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/ZhpWtoXwXNWoxcw7x8k2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanDCunha/Hands-On-Machine-Learning-with-Scikit-Learn-and-PyTorch/blob/main/Chapter6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Ensemble Learning and Random Forests\n"
      ],
      "metadata": {
        "id": "POOgylyxnFD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose you pose a complex question to thousands of random people,\n",
        "then aggregate their answers.\n"
      ],
      "metadata": {
        "id": "sxZ5iQ3GnGW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In many cases, this aggregated answer\n",
        "is better than an expert’s answer.\n"
      ],
      "metadata": {
        "id": "22OX-MbVnIU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This phenomenon is known as **the wisdom of the crowd**.\n"
      ],
      "metadata": {
        "id": "FU0jBDV-nJK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A similar idea applies in machine learning.\n"
      ],
      "metadata": {
        "id": "XXGED1dWnKC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you aggregate the predictions of multiple predictors\n",
        "(such as classifiers or regressors),\n",
        "you will often obtain better predictions\n",
        "than from the best individual predictor.\n"
      ],
      "metadata": {
        "id": "GzvTKOQYnK2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A group of predictors is called an **ensemble**.\n"
      ],
      "metadata": {
        "id": "WTAxbrtsnLqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This approach is known as **ensemble learning**,\n",
        "and algorithms that implement it\n",
        "are called **ensemble methods**.\n"
      ],
      "metadata": {
        "id": "o13gccjOnahE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An Example: Decision Tree Ensembles\n"
      ],
      "metadata": {
        "id": "4pJXfZntnbTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One simple example of an ensemble method\n",
        "is to train several decision tree classifiers.\n"
      ],
      "metadata": {
        "id": "VH01wKnJncFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each tree is trained on a **different random subset**\n",
        "of the training set.\n"
      ],
      "metadata": {
        "id": "JkgbXE62ndXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make a prediction,\n",
        "each tree casts a vote for a class.\n"
      ],
      "metadata": {
        "id": "dLaGyXjMneDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class that receives the **most votes**\n",
        "becomes the ensemble’s final prediction.\n"
      ],
      "metadata": {
        "id": "yWBMF15RneyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An ensemble of decision trees built this way\n",
        "is called a **random forest**.\n"
      ],
      "metadata": {
        "id": "13YBZPj9nflk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite its conceptual simplicity,\n",
        "random forests are among\n",
        "the most powerful machine learning algorithms available today.\n"
      ],
      "metadata": {
        "id": "e10EDCIcngbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When to Use Ensemble Methods\n"
      ],
      "metadata": {
        "id": "9QX2tiB3nqtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed in Chapter 2,\n",
        "ensemble methods are often used\n",
        "near the end of a machine learning project.\n"
      ],
      "metadata": {
        "id": "GhaWpuRInral"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At that stage, you may already have\n",
        "several reasonably good predictors.\n"
      ],
      "metadata": {
        "id": "qBQXcbKqnsDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining them into an ensemble\n",
        "can produce an even better model.\n"
      ],
      "metadata": {
        "id": "4EUPraDCns0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fact, many winning solutions\n",
        "in machine learning competitions\n",
        "rely heavily on ensemble methods.\n"
      ],
      "metadata": {
        "id": "qtD9lhB6nthN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A famous example is the Netflix Prize competition,\n",
        "where ensembles played a central role.\n"
      ],
      "metadata": {
        "id": "sgIBtGUInuQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pros and Cons of Ensemble Learning\n"
      ],
      "metadata": {
        "id": "d2JgPfignvND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble methods are powerful,\n",
        "but they do come with some downsides.\n"
      ],
      "metadata": {
        "id": "_TRaJT7SnwNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "They generally require **more computational resources**\n",
        "than using a single model.\n"
      ],
      "metadata": {
        "id": "MAgWtamLnw4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This includes higher costs\n",
        "for both training and inference.\n"
      ],
      "metadata": {
        "id": "eki6gP-enx36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensembles can also be more complex\n",
        "to deploy and manage in production.\n"
      ],
      "metadata": {
        "id": "exgtaG8Vny00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, their predictions\n",
        "are usually harder to interpret.\n"
      ],
      "metadata": {
        "id": "gCulqqjTnzwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Voting Classifiers\n"
      ],
      "metadata": {
        "id": "dhYbzGTtrC_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose you have trained a few classifiers, each one achieving about 80% accuracy.\n"
      ],
      "metadata": {
        "id": "497ZDzbZrHZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, you might have:\n",
        "- A Logistic Regression classifier\n",
        "- A Support Vector Machine (SVM)\n",
        "- A Random Forest\n",
        "- A k-Nearest Neighbors classifier\n"
      ],
      "metadata": {
        "id": "gIo1FoRxrID0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple way to build a stronger classifier is to **aggregate their predictions**.\n"
      ],
      "metadata": {
        "id": "lVWV-ScxrI4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class that receives the **most votes** becomes the final prediction.\n"
      ],
      "metadata": {
        "id": "RXDkn52erJkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This approach is called **hard voting**, and the resulting model is known as a\n",
        "**hard voting classifier**.\n"
      ],
      "metadata": {
        "id": "KNrpTOFdrNzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Surprisingly, a voting classifier often achieves **higher accuracy**\n",
        "than the best individual classifier in the ensemble.\n"
      ],
      "metadata": {
        "id": "5HfD6ZmbrOlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even if each individual classifier is only a **weak learner**\n",
        "(performing just slightly better than random guessing),\n",
        "the ensemble can become a **strong learner**.\n"
      ],
      "metadata": {
        "id": "RBn8MrxorPaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works best when:\n",
        "- There are many classifiers\n",
        "- The classifiers are diverse\n",
        "- They make uncorrelated errors\n"
      ],
      "metadata": {
        "id": "Y1Ceuzt0rQLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This idea is closely related to the **law of large numbers**.\n"
      ],
      "metadata": {
        "id": "pceCEDzXrRW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like repeatedly tossing a slightly biased coin increases the chance\n",
        "of observing the true probability, aggregating many weak but diverse classifiers\n",
        "pushes predictions toward the correct class.\n"
      ],
      "metadata": {
        "id": "HNwRVpE3rSYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diversity Matters\n"
      ],
      "metadata": {
        "id": "FbCrWQLzrTCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If all classifiers make similar errors,\n",
        "majority voting will not help much.\n"
      ],
      "metadata": {
        "id": "Lc6wPrbbrT0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ways to increase diversity include:\n",
        "- Using very different algorithms\n",
        "- Training models with different hyperparameters\n",
        "- Training models on different subsets of the data\n"
      ],
      "metadata": {
        "id": "CNn4gzuarU8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VotingClassifier in Scikit-Learn\n"
      ],
      "metadata": {
        "id": "dFO-aD-WrW8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn provides the `VotingClassifier`,\n",
        "which makes implementing voting ensembles straightforward.\n"
      ],
      "metadata": {
        "id": "fNeqtSv3rYH9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCywGkhgmRqR"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, generate the moons dataset and split it into training and test sets.\n"
      ],
      "metadata": {
        "id": "BViS2BtAwmY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "tGVfjsWEwnSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, create a voting classifier using three diverse models:\n",
        "- Logistic Regression\n",
        "- Random Forest\n",
        "- Support Vector Classifier\n"
      ],
      "metadata": {
        "id": "rG1PQB_XwoDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        (\"lr\", LogisticRegression(random_state=42)),\n",
        "        (\"rf\", RandomForestClassifier(random_state=42)),\n",
        "        (\"svc\", SVC(random_state=42)),\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "Lm50DpXlwo_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the voting classifier to the training data.\n"
      ],
      "metadata": {
        "id": "iYHa9AtvwpqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voting_clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "q1BroW2HwqY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When a `VotingClassifier` is trained, Scikit-Learn:\n",
        "- Clones each estimator\n",
        "- Fits the cloned models\n"
      ],
      "metadata": {
        "id": "et7Rd_l3wrIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fitted models are available through the `named_estimators_` attribute.\n"
      ],
      "metadata": {
        "id": "qQQOBZJCwsVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s evaluate each individual classifier on the test set.\n"
      ],
      "metadata": {
        "id": "_UaPhcKtwtAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, clf in voting_clf.named_estimators_.items():\n",
        "    print(name, \"=\", clf.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "I_c8wDS0wtzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, the voting classifier uses **hard voting**.\n"
      ],
      "metadata": {
        "id": "7skslm9zwun8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a single test instance, the predicted class is the one\n",
        "chosen by the majority of classifiers.\n"
      ],
      "metadata": {
        "id": "2HMD0jbswvXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voting_clf.predict(X_test[:1])\n"
      ],
      "metadata": {
        "id": "Vg4qVUtVwwe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[clf.predict(X_test[:1]) for clf in voting_clf.estimators_]\n"
      ],
      "metadata": {
        "id": "46VDnNfxwxQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now evaluate the overall accuracy of the voting classifier.\n"
      ],
      "metadata": {
        "id": "JTs94NnSwyAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voting_clf.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "-YXG7HfgwytJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The voting classifier outperforms all individual classifiers.\n"
      ],
      "metadata": {
        "id": "j4nUoJCYwzUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Soft Voting\n"
      ],
      "metadata": {
        "id": "qYhtu_yvw0Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If all classifiers can estimate class probabilities,\n",
        "you can use **soft voting**.\n"
      ],
      "metadata": {
        "id": "PhrSO0Oxw66J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Soft voting predicts the class with the highest\n",
        "average predicted probability.\n"
      ],
      "metadata": {
        "id": "hDVsQ1uBw7oP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Soft voting often performs better than hard voting\n",
        "because it gives more weight to confident predictions.\n"
      ],
      "metadata": {
        "id": "6oUREvD5w8c3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `SVC` class does not support probability estimates by default,\n",
        "so we must enable them explicitly.\n"
      ],
      "metadata": {
        "id": "H65JDjsew9Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voting_clf.voting = \"soft\"\n",
        "voting_clf.named_estimators[\"svc\"].probability = True\n"
      ],
      "metadata": {
        "id": "hHXP1UwGw98s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refit the voting classifier with soft voting enabled.\n"
      ],
      "metadata": {
        "id": "AVhc-2O2w-zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voting_clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "b8POIz3Tw_X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the soft voting classifier.\n"
      ],
      "metadata": {
        "id": "sJ_7nS03xAHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voting_clf.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "435KbRICxA2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Soft voting achieves even higher accuracy.\n"
      ],
      "metadata": {
        "id": "0-BGaAZSxBq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "**Tip:**\n",
        "Soft voting works best when predicted probabilities are well-calibrated.\n",
        "If needed, use `sklearn.calibration.CalibratedClassifierCV`.\n"
      ],
      "metadata": {
        "id": "ukpSUSI5xCWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagging and Pasting\n"
      ],
      "metadata": {
        "id": "8NAgrFkuxiaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to build a diverse ensemble is to use the **same learning algorithm**\n",
        "but train each predictor on a **different random subset** of the training data.\n"
      ],
      "metadata": {
        "id": "PkcyYuH9xjpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two main approaches:\n",
        "- **Bagging** (Bootstrap Aggregating)\n",
        "- **Pasting**\n"
      ],
      "metadata": {
        "id": "pzJ9PkbVxkTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When sampling is performed **with replacement**, the method is called **bagging**.\n"
      ],
      "metadata": {
        "id": "rxzLIeraxlFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When sampling is performed **without replacement**, the method is called **pasting**.\n"
      ],
      "metadata": {
        "id": "QBi77aXZxlwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both approaches allow training instances to appear in multiple predictors,\n",
        "but **only bagging** allows the same instance to appear multiple times\n",
        "within a single predictor’s training set.\n"
      ],
      "metadata": {
        "id": "cs5Jiw9xxmlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once all predictors are trained, the ensemble makes predictions by\n",
        "**aggregating** the individual predictions.\n"
      ],
      "metadata": {
        "id": "9jtzETo-xnm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For **classification**, aggregation is usually the **mode** (majority vote)\n",
        "- For **regression**, aggregation is usually the **average**\n"
      ],
      "metadata": {
        "id": "F4YulZNmxoXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each individual predictor typically has **higher bias** than a model trained\n",
        "on the full dataset, but aggregation reduces both **bias and variance**.\n"
      ],
      "metadata": {
        "id": "wVJPibk3xphi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works especially well for **high-variance, low-bias models**\n",
        "such as decision trees.\n"
      ],
      "metadata": {
        "id": "0elOmuO4xqU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Bagging Reduces Variance\n"
      ],
      "metadata": {
        "id": "AyM60tmxxrAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Averaging predictions from independent models reduces variance.\n"
      ],
      "metadata": {
        "id": "wp-Gq7pxxr1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, predictors are not fully independent,\n",
        "but bagging still reduces correlation enough to significantly\n",
        "lower variance compared to a single model.\n"
      ],
      "metadata": {
        "id": "TvAGBA2nxs7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bagging vs. Pasting\n"
      ],
      "metadata": {
        "id": "b-iihKQ1xtw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Bagging** introduces more diversity but slightly higher bias\n",
        "- **Pasting** avoids redundant samples and is slightly more efficient\n"
      ],
      "metadata": {
        "id": "CmbQsTr-xuoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging is generally preferred, especially for noisy datasets\n",
        "or models prone to overfitting.\n"
      ],
      "metadata": {
        "id": "VcQL-3icxxMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both methods scale very well because predictors can be trained\n",
        "and evaluated **in parallel**.\n"
      ],
      "metadata": {
        "id": "XYPFr1e9xx5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bagging and Pasting in Scikit-Learn\n"
      ],
      "metadata": {
        "id": "RMjxGUeXxymx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn provides the `BaggingClassifier`\n",
        "(and `BaggingRegressor` for regression).\n"
      ],
      "metadata": {
        "id": "TZZ39LTpxzOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n"
      ],
      "metadata": {
        "id": "EosD2ea6xz_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code trains an ensemble of 500 decision trees.\n",
        "Each tree is trained on 100 instances sampled **with replacement**\n",
        "(i.e., bagging).\n"
      ],
      "metadata": {
        "id": "u88d59ejx06b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(),\n",
        "    n_estimators=500,\n",
        "    max_samples=100,\n",
        "    bootstrap=True,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "eZggQOlSydf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the bagging classifier to the training data.\n"
      ],
      "metadata": {
        "id": "aA5aJCU8yc4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "sq7UoYmnyfrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use **pasting** instead of bagging, set `bootstrap=False`.\n"
      ],
      "metadata": {
        "id": "qdfwR4X1ygaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `BaggingClassifier` automatically performs **soft voting**\n",
        "if the base estimator supports probability estimates,\n",
        "which decision trees do.\n"
      ],
      "metadata": {
        "id": "OUISm8iQyhKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Out-of-Bag (OOB) Evaluation\n"
      ],
      "metadata": {
        "id": "VoVhd8Tayh79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The remaining **37%** are called **out-of-bag (OOB)** instances.\n"
      ],
      "metadata": {
        "id": "bRh6yLYuyi8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OOB instances can be used as a **validation set**,\n",
        "eliminating the need for a separate one.\n"
      ],
      "metadata": {
        "id": "ueLabgTKyj5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enable OOB evaluation by setting `oob_score=True`.\n"
      ],
      "metadata": {
        "id": "qBgilb4Vyk_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(),\n",
        "    n_estimators=500,\n",
        "    oob_score=True,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "QqIcExnOylvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model with OOB evaluation enabled.\n"
      ],
      "metadata": {
        "id": "Zz0o2760y7xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "u1UrspZNy8k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The OOB accuracy estimate is stored in `oob_score_`.\n"
      ],
      "metadata": {
        "id": "ExDhedIEy9Po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf.oob_score_\n"
      ],
      "metadata": {
        "id": "GJr2AAejy-HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s compare this estimate with the actual test-set accuracy.\n"
      ],
      "metadata": {
        "id": "9KlJ-g24y-wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "dcIZwNF6y_ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The OOB score is usually slightly pessimistic,\n",
        "but it provides a very good estimate of test performance.\n"
      ],
      "metadata": {
        "id": "pAet-gJJzARJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The OOB decision function is also available and returns\n",
        "class probabilities for each training instance.\n"
      ],
      "metadata": {
        "id": "l_YdTzvwzBmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf.oob_decision_function_[:3]\n"
      ],
      "metadata": {
        "id": "SLfRTgbdzCOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Patches and Random Subspaces\n"
      ],
      "metadata": {
        "id": "ut7yr9kFzC66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "`BaggingClassifier` also supports **feature sampling**.\n"
      ],
      "metadata": {
        "id": "iHFwyhGqzDzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature sampling is controlled by:\n",
        "- `max_features`\n",
        "- `bootstrap_features`\n"
      ],
      "metadata": {
        "id": "TkPBSZ-fzklp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sampling both instances and features is called the\n",
        "**random patches** method.\n"
      ],
      "metadata": {
        "id": "Y8dcYzLNzlpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sampling only features (keeping all instances)\n",
        "is called the **random subspaces** method.\n"
      ],
      "metadata": {
        "id": "hBZ_UvpCzmPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature sampling increases predictor diversity,\n",
        "trading a bit more bias for lower variance.\n",
        "cachchch"
      ],
      "metadata": {
        "id": "TE3HIzjGzm74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The importances are normalized so that their sum equals 1.\n"
      ],
      "metadata": {
        "id": "OxSOmbNcU3dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can access feature importances using the\n",
        "`feature_importances_` attribute.\n"
      ],
      "metadata": {
        "id": "j1lA4_J_U4lr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following example trains a random forest on the iris dataset\n",
        "and displays feature importances.\n"
      ],
      "metadata": {
        "id": "Tz5vb-BLU5vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n"
      ],
      "metadata": {
        "id": "1fMInbKwU6dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris(as_frame=True)\n"
      ],
      "metadata": {
        "id": "JLrkiavBU7VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_clf = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "rvCWfwALU7-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the random forest.\n"
      ],
      "metadata": {
        "id": "RRcWZH_EU8s9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_clf.fit(iris.data, iris.target)\n"
      ],
      "metadata": {
        "id": "NC0cPwZrU9P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display each feature’s importance.\n"
      ],
      "metadata": {
        "id": "GRVtEnqpU-Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):\n",
        "    print(round(score, 2), name)\n"
      ],
      "metadata": {
        "id": "6FUqfPGBU-BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most important features are petal length and petal width,\n",
        "while sepal features are much less important.\n"
      ],
      "metadata": {
        "id": "ojQfUlI_WmZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, when trained on image datasets such as MNIST,\n",
        "random forests can reveal which pixels are most important\n",
        "for classification.\n"
      ],
      "metadata": {
        "id": "ybbVfEGLWnYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forests are especially useful for:\n",
        "- Feature selection\n",
        "- Quick model benchmarking\n",
        "- Strong baseline performance\n"
      ],
      "metadata": {
        "id": "XRtG0vNPWo-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting\n"
      ],
      "metadata": {
        "id": "t_n4y1t1ZYzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting (originally called hypothesis boosting) refers to ensemble methods\n",
        "that combine several weak learners into a strong learner.\n"
      ],
      "metadata": {
        "id": "8BU8CBrlZafc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most boosting methods train predictors **sequentially**,\n",
        "each one trying to correct the errors of its predecessor.\n"
      ],
      "metadata": {
        "id": "iTQ5FYsqZbTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two most popular boosting algorithms are:\n",
        "- **AdaBoost** (Adaptive Boosting)\n",
        "- **Gradient Boosting**\n"
      ],
      "metadata": {
        "id": "mPaRiyX8Zb_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s start with AdaBoost.\n"
      ],
      "metadata": {
        "id": "_j_PTVihZc1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdaBoost\n"
      ],
      "metadata": {
        "id": "B9oOy-gzZduf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBoost focuses more and more on the **hard-to-classify** training instances.\n"
      ],
      "metadata": {
        "id": "Bds07E9xZeUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each new predictor pays more attention to instances that previous predictors\n",
        "misclassified.\n"
      ],
      "metadata": {
        "id": "WHw54GbpZfHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The algorithm works as follows:\n",
        "1. Train a base classifier.\n",
        "2. Increase the weights of misclassified instances.\n",
        "3. Train a new classifier using the updated weights.\n",
        "4. Repeat.\n"
      ],
      "metadata": {
        "id": "PT3gXtUFahyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This sequential learning process is similar to gradient descent,\n",
        "except AdaBoost adds new predictors instead of adjusting parameters.\n"
      ],
      "metadata": {
        "id": "bWRpoKZcailE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Important Limitation\n"
      ],
      "metadata": {
        "id": "M1XwShihajYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting cannot be parallelized, because each predictor depends\n",
        "on the previous one.\n"
      ],
      "metadata": {
        "id": "hCXdgZ1aakP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a result, boosting does not scale as well as bagging or pasting.\n"
      ],
      "metadata": {
        "id": "4hjXefAialsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdaBoost Mathematics (Conceptual)\n"
      ],
      "metadata": {
        "id": "q7wTEi1pamXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Each training instance starts with equal weight.\n",
        "- Misclassified instances get higher weights.\n",
        "- More accurate predictors get higher voting weights.\n"
      ],
      "metadata": {
        "id": "IwNOfMR8anBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions are made by a **weighted vote** of all predictors.\n"
      ],
      "metadata": {
        "id": "_mhjIrZqan5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn uses a multiclass version of AdaBoost called **SAMME**.\n"
      ],
      "metadata": {
        "id": "XWtyAJyEaogP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When there are only two classes, SAMME is equivalent to standard AdaBoost.\n"
      ],
      "metadata": {
        "id": "8zAB6SGIapNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n"
      ],
      "metadata": {
        "id": "cHoFwLSaap-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, AdaBoost uses **decision stumps**\n",
        "(decision trees with max_depth=1).\n"
      ],
      "metadata": {
        "id": "6T4m1eVvaqwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=30,\n",
        "    learning_rate=0.5,\n",
        "    random_state=42,\n",
        "    algorithm=\"SAMME\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "RprR3wk5arpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the AdaBoost classifier.\n"
      ],
      "metadata": {
        "id": "_j6g9ORrasjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "NyFYjH8NatWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the model overfits, try:\n",
        "- Fewer estimators\n",
        "- Stronger regularization of the base estimator\n",
        "- A smaller learning rate\n"
      ],
      "metadata": {
        "id": "Kf19bNxVauUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting\n"
      ],
      "metadata": {
        "id": "8sCwIMfLavBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient boosting also builds predictors sequentially,\n",
        "but instead of reweighting instances,\n",
        "it fits each new predictor to the **residual errors**\n",
        "of the previous predictor.\n"
      ],
      "metadata": {
        "id": "luksusJcavw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll start with a regression example using decision trees.\n"
      ],
      "metadata": {
        "id": "hg4Vt9TDazPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n"
      ],
      "metadata": {
        "id": "c0xBxRzea0QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate a noisy quadratic dataset.\n"
      ],
      "metadata": {
        "id": "nvquswfaa0-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = 100\n",
        "rng = np.random.default_rng(seed=42)\n",
        "X = rng.random((m, 1)) - 0.5\n",
        "noise = 0.05 * rng.standard_normal(m)\n",
        "y = 3 * X[:, 0] ** 2 + noise\n"
      ],
      "metadata": {
        "id": "fBlV5mYra12q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the first regression tree.\n"
      ],
      "metadata": {
        "id": "t0EivdYja3RS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg1.fit(X, y)\n"
      ],
      "metadata": {
        "id": "JLwWc7KOa5V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a second tree on the residual errors of the first.\n"
      ],
      "metadata": {
        "id": "8RVQfk5Za3KA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y2 = y - tree_reg1.predict(X)\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\n",
        "tree_reg2.fit(X, y2)\n"
      ],
      "metadata": {
        "id": "K_5cXocRa7Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a third tree on the residuals of the second.\n"
      ],
      "metadata": {
        "id": "b8FhzfDCa7Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y3 = y2 - tree_reg2.predict(X)\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\n",
        "tree_reg3.fit(X, y3)\n"
      ],
      "metadata": {
        "id": "Os8NVNCVa9Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions are made by summing the predictions of all trees.\n"
      ],
      "metadata": {
        "id": "maNd2xLRa9zA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = np.array([[-0.4], [0.0], [0.5]])\n",
        "sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n"
      ],
      "metadata": {
        "id": "Z9_qffeSa-pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosting with Scikit-Learn\n"
      ],
      "metadata": {
        "id": "_xe0C5IPa_g6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n"
      ],
      "metadata": {
        "id": "u-2xOTEFbAci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates the same ensemble using a single class.\n"
      ],
      "metadata": {
        "id": "MgsmlAjibBi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbrt = GradientBoostingRegressor(\n",
        "    max_depth=2,\n",
        "    n_estimators=3,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "gbrt.fit(X, y)\n"
      ],
      "metadata": {
        "id": "DYQKMbUcbCW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The learning rate controls how much each tree contributes.\n"
      ],
      "metadata": {
        "id": "c7_9pSljbDKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A smaller learning rate requires more trees\n",
        "but often improves generalization.\n",
        "This technique is called **shrinkage**.\n"
      ],
      "metadata": {
        "id": "j1sNZYZ8bD_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Early Stopping\n"
      ],
      "metadata": {
        "id": "0nXJUW34bEqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early stopping automatically stops training\n",
        "when adding more trees no longer improves performance.\n"
      ],
      "metadata": {
        "id": "45RI1A26bFXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbrt_best = GradientBoostingRegressor(\n",
        "    max_depth=2,\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=500,\n",
        "    n_iter_no_change=10,\n",
        "    random_state=42\n",
        ")\n",
        "gbrt_best.fit(X, y)\n"
      ],
      "metadata": {
        "id": "e3Q48BLKbGWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual number of trees used is often much smaller.\n"
      ],
      "metadata": {
        "id": "yksDFADWbHQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbrt_best.n_estimators_\n"
      ],
      "metadata": {
        "id": "-TITgcAXbISo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Histogram-Based Gradient Boosting (HGB)\n"
      ],
      "metadata": {
        "id": "Wn_VmTzUbI8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram-based gradient boosting speeds up training\n",
        "by binning continuous features into discrete values.\n"
      ],
      "metadata": {
        "id": "emoSO2kAbLb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This reduces computational complexity and memory usage,\n",
        "making it ideal for large datasets.\n"
      ],
      "metadata": {
        "id": "BdDkKL2mbMS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key differences from standard GBRT:\n",
        "- Faster training\n",
        "- Built-in handling of missing values\n",
        "- Native support for categorical features\n"
      ],
      "metadata": {
        "id": "2skT6_d6bNAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n"
      ],
      "metadata": {
        "id": "aDiohW8WbKXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example pipeline for the California housing dataset.\n"
      ],
      "metadata": {
        "id": "-CimQbNhbOhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hgb_reg = make_pipeline(\n",
        "    make_column_transformer(\n",
        "        (OrdinalEncoder(), [\"ocean_proximity\"]),\n",
        "        remainder=\"passthrough\",\n",
        "        force_int_remainder_cols=False\n",
        "    ),\n",
        "    HistGradientBoostingRegressor(\n",
        "        categorical_features=[0],\n",
        "        random_state=42\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "KDMTJn2NbPTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model.\n"
      ],
      "metadata": {
        "id": "Z60j__yrbQFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hgb_reg.fit(housing, housing_labels)\n"
      ],
      "metadata": {
        "id": "QGIzRfW-bR00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram-based gradient boosting is an excellent choice for:\n",
        "- Large datasets\n",
        "- Categorical features\n",
        "- Missing values\n"
      ],
      "metadata": {
        "id": "nA6ytQrpbSe-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Popular optimized gradient boosting libraries include:\n",
        "- XGBoost\n",
        "- LightGBM\n",
        "- CatBoost\n"
      ],
      "metadata": {
        "id": "3LAFwosSbTQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacking (Stacked Generalization)\n"
      ],
      "metadata": {
        "id": "wXKtrxqphitg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last ensemble method we will discuss is **stacking**\n",
        "(short for *stacked generalization*).\n"
      ],
      "metadata": {
        "id": "30H8sWZGhj5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using a simple rule (such as majority voting)\n",
        "to aggregate predictions, stacking **trains a model**\n",
        "to perform this aggregation.\n"
      ],
      "metadata": {
        "id": "I5DdcyN9hkZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model that learns how to combine predictions\n",
        "is called a **blender** or **meta-learner**.\n"
      ],
      "metadata": {
        "id": "IqYmPDBEhktT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each base predictor makes its own prediction,\n",
        "and the blender uses these predictions as input features\n",
        "to produce the final prediction.\n"
      ],
      "metadata": {
        "id": "G-ZU1RUphlOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a Stacking Ensemble\n"
      ],
      "metadata": {
        "id": "EcnaCZxghlgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the blender, we must first create a **blending dataset**.\n"
      ],
      "metadata": {
        "id": "BFVAxbcahl1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is done by generating **out-of-sample predictions**\n",
        "for each base model using cross-validation.\n"
      ],
      "metadata": {
        "id": "lfX_BgyxhmOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These predictions become the **input features** for the blender,\n",
        "while the original targets remain unchanged.\n"
      ],
      "metadata": {
        "id": "zXOxHhXShmft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the blender is trained, the base predictors are\n",
        "retrained on the **full training set**.\n"
      ],
      "metadata": {
        "id": "XiJrVrodhm35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn handles all of this automatically via\n",
        "`StackingClassifier` and `StackingRegressor`.\n"
      ],
      "metadata": {
        "id": "HMOmGsjMhnQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacking Classifier Example (Moons Dataset)\n"
      ],
      "metadata": {
        "id": "QaUwTD2-hnq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n"
      ],
      "metadata": {
        "id": "Uh_e2w5EmNOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We build a stacking classifier using three diverse base models\n",
        "and a random forest as the final estimator.\n"
      ],
      "metadata": {
        "id": "8u09xIuFhoKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', LogisticRegression(random_state=42)),\n",
        "        ('rf', RandomForestClassifier(random_state=42)),\n",
        "        ('svc', SVC(probability=True, random_state=42))\n",
        "    ],\n",
        "    final_estimator=RandomForestClassifier(random_state=43),\n",
        "    cv=5\n",
        ")\n"
      ],
      "metadata": {
        "id": "USOyTTWtmP1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the stacking classifier on the training data.\n"
      ],
      "metadata": {
        "id": "Ja26fOzshog0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stacking_clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "h0kCnl6Fn7RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the stacking classifier on the test set.\n"
      ],
      "metadata": {
        "id": "saiW7cN6n8jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stacking_clf.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "e5f6jMuen9vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This stacking model typically achieves slightly better\n",
        "performance than soft voting, at the cost of extra complexity.\n"
      ],
      "metadata": {
        "id": "86Y3cY0Sn-9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### How Predictions Are Generated\n"
      ],
      "metadata": {
        "id": "KUS4UxmfoARR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each base estimator:\n",
        "- `predict_proba()` is used if available\n",
        "- otherwise `decision_function()`\n",
        "- otherwise `predict()`\n"
      ],
      "metadata": {
        "id": "REf27kRuo9hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If no `final_estimator` is provided:\n",
        "- `StackingClassifier` defaults to `LogisticRegression`\n",
        "- `StackingRegressor` defaults to `RidgeCV`\n"
      ],
      "metadata": {
        "id": "T7arPbjho-1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multilayer Stacking\n"
      ],
      "metadata": {
        "id": "VJGEr35JpAQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possible to stack **multiple layers of blenders**,\n",
        "where one blender’s output feeds into another.\n"
      ],
      "metadata": {
        "id": "XU_rvReSpBrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can slightly improve performance,\n",
        "but significantly increases training time and system complexity.\n"
      ],
      "metadata": {
        "id": "GCsId1LopDcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When to Use Stacking\n"
      ],
      "metadata": {
        "id": "0o72jbXWpFAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking works best when:\n",
        "- Models are **diverse**\n",
        "- Dataset is **complex or high-dimensional**\n",
        "- Maximum predictive performance is required\n"
      ],
      "metadata": {
        "id": "2dQ0wudKq08i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is especially popular in **Kaggle competitions**\n",
        "and high-stakes prediction systems.\n"
      ],
      "metadata": {
        "id": "dv5s8-CJq2ju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of Ensemble Methods\n"
      ],
      "metadata": {
        "id": "FlXHi8Gkq4mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Voting: simple, fast, strong baseline\n",
        "- Bagging: reduces variance, great for trees\n",
        "- Random forests: bagging + feature randomness\n",
        "- Boosting: sequential error correction\n",
        "- Stacking: learned aggregation for maximum accuracy\n"
      ],
      "metadata": {
        "id": "6SD2O4QSrKHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble methods are powerful, flexible, and easy to use,\n",
        "but can overfit if not carefully regularized.\n"
      ],
      "metadata": {
        "id": "oU_dhG96rLfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next up: **unsupervised learning**, starting with\n",
        "**dimensionality reduction**.\n"
      ],
      "metadata": {
        "id": "k1HdZMB-rMbV"
      }
    }
  ]
}