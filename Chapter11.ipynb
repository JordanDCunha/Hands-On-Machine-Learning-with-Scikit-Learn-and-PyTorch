{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPge5mzvQZOgtIzwl+e2B3e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanDCunha/Hands-On-Machine-Learning-with-Scikit-Learn-and-PyTorch/blob/main/Chapter11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing a Regression MLP\n",
        "\n",
        "PyTorch provides a helpful `nn.Sequential` module that chains multiple modules: when you call this module with some inputs, it feeds these inputs to the first module, then feeds the output of the first module to the second module, and so on.\n",
        "\n",
        "Most neural networks contain stacks of modules, and in fact many neural networks are just one big stack of modules: this makes the `nn.Sequential` module one of the most useful modules in PyTorch.\n",
        "\n",
        "The MLP we want to build is just that: a simple stack of modules—two hidden layers and one output layer. So let’s build it using the `nn.Sequential` module:\n"
      ],
      "metadata": {
        "id": "No7ByNSJdTQ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEpb0itDcTY9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(n_features, 50),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(50, 40),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(40, 1)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer-by-layer explanation\n",
        "\n",
        "- **First layer**  \n",
        "  The first layer must have the right number of inputs for our data: `n_features` (equal to 8 in our case).  \n",
        "  The number of outputs is a tunable hyperparameter; here we choose 50.\n",
        "\n",
        "- **ReLU activation**  \n",
        "  `nn.ReLU` implements the ReLU activation function.  \n",
        "  It has no parameters and applies the function elementwise.\n",
        "\n",
        "- **Second hidden layer**  \n",
        "  The second layer takes 50 inputs (matching the previous layer’s output) and outputs 40 features.  \n",
        "  Hidden layers do not need to have the same width, as long as dimensions match.\n",
        "\n",
        "- **Output layer**  \n",
        "  The output layer must match the dimensionality of the targets.  \n",
        "  Since our targets are scalar values, we use a single output neuron.\n"
      ],
      "metadata": {
        "id": "Or-9VrLvdWIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model\n"
      ],
      "metadata": {
        "id": "F-TyN3QFdYNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "mse = nn.MSELoss()\n",
        "\n",
        "train_bgd(model, optimizer, mse, X_train, y_train, n_epochs)\n"
      ],
      "metadata": {
        "id": "esch07DGdZEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "That’s it—you’ve trained your first neural network with PyTorch!\n",
        "\n",
        "However, we are still using **batch gradient descent**, which computes gradients over the entire training set at each iteration. This does not scale well to large datasets or models, so we will later switch to **mini-batch gradient descent**.\n"
      ],
      "metadata": {
        "id": "n8S0g2t2dcIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Vanishing / Exploding Gradients Problems\n",
        "\n",
        "During backpropagation, gradients flow from the output layer back toward the input layer. Unfortunately, these gradients often become:\n",
        "\n",
        "- **Very small** → vanishing gradients  \n",
        "- **Very large** → exploding gradients  \n",
        "\n",
        "When gradients vanish, lower layers learn extremely slowly or not at all.  \n",
        "When gradients explode, training becomes unstable and diverges.\n"
      ],
      "metadata": {
        "id": "yICtmRpSdfGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These problems were one of the main reasons deep neural networks were mostly abandoned in the early 2000s.\n",
        "\n",
        "A 2010 paper by Xavier Glorot and Yoshua Bengio showed that poor **weight initialization** combined with **sigmoid activations** was a major cause of unstable gradients.\n"
      ],
      "metadata": {
        "id": "BQ03KlxtdeqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why sigmoid causes trouble\n",
        "\n",
        "- Sigmoid saturates at 0 and 1\n",
        "- Its derivative approaches 0 for large |z|\n",
        "- Gradients shrink exponentially as they propagate backward\n",
        "\n",
        "This leaves almost no learning signal for lower layers.\n"
      ],
      "metadata": {
        "id": "ko3BiI60d2-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proper weight initialization\n",
        "\n",
        "To keep signals stable, we want:\n",
        "\n",
        "- Forward activations to keep the same variance\n",
        "- Backward gradients to keep the same variance\n",
        "\n",
        "Glorot and Bengio proposed initializing weights using:\n",
        "\n",
        "- **fan-in**: number of inputs\n",
        "- **fan-out**: number of outputs\n"
      ],
      "metadata": {
        "id": "2tg8ZQXSd3-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common initialization strategies\n",
        "\n",
        "| Initialization | Activation functions | Variance |\n",
        "|----------------|---------------------|----------|\n",
        "| Xavier (Glorot) | tanh, sigmoid | 1 / fanavg |\n",
        "| He (Kaiming) | ReLU, GELU, Swish, Mish | 2 / fanin |\n",
        "| LeCun | SELU | 1 / fanin |\n"
      ],
      "metadata": {
        "id": "bR4gcdbFd412"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual initialization (not recommended)\n"
      ],
      "metadata": {
        "id": "K9_pHNWTd51n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = nn.Linear(40, 10)\n",
        "layer.weight.data *= (6 ** 0.5)  # Kaiming init\n",
        "torch.zero_(layer.bias.data)\n"
      ],
      "metadata": {
        "id": "V4ZIrkWVddFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recommended: use torch.nn.init\n"
      ],
      "metadata": {
        "id": "JLFOfOItd9Pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.init.kaiming_uniform_(layer.weight)\n",
        "nn.init.zeros_(layer.bias)\n"
      ],
      "metadata": {
        "id": "KmrqK6PieXun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying initialization to all layers\n"
      ],
      "metadata": {
        "id": "T1so9cHid_Ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def use_he_init(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.kaiming_uniform_(module.weight)\n",
        "        nn.init.zeros_(module.bias)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(50, 40),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(40, 1),\n",
        "    nn.ReLU()\n",
        ")\n",
        "\n",
        "model.apply(use_he_init)\n"
      ],
      "metadata": {
        "id": "wGY5hUdIeWOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReLU and its problems\n",
        "\n",
        "ReLU is fast and effective, but it suffers from **dying ReLUs**:\n",
        "neurons can permanently output zero if their inputs become negative for all samples.\n"
      ],
      "metadata": {
        "id": "8Y5lCGcVea9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Leaky ReLU\n",
        "\n",
        "Leaky ReLU allows a small slope for negative values:\n",
        "\n",
        "LeakyReLUα(z) = max(αz, z)\n",
        "\n",
        "This prevents neurons from dying completely.\n"
      ],
      "metadata": {
        "id": "BkPQj6Mxef3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.2\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(50, 40),\n",
        "    nn.LeakyReLU(negative_slope=alpha)\n",
        ")\n",
        "\n",
        "nn.init.kaiming_uniform_(\n",
        "    model[0].weight,\n",
        "    alpha,\n",
        "    nonlinearity=\"leaky_relu\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "GO78bCuWehRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ELU\n",
        "\n",
        "ELU:\n",
        "- Produces negative outputs\n",
        "- Has nonzero gradients everywhere\n",
        "- Is smooth at z = 0\n",
        "\n",
        "This often speeds up training, at the cost of extra computation.\n"
      ],
      "metadata": {
        "id": "ztH7iKsdeiJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SELU\n",
        "\n",
        "SELU enables **self-normalizing networks**, but only if:\n",
        "- Inputs are standardized\n",
        "- LeCun normal initialization is used\n",
        "- No batch norm or dropout is applied\n"
      ],
      "metadata": {
        "id": "-IwlG8xRejEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modern activation functions\n",
        "\n",
        "- **GELU** – smooth, non-monotonic, widely used in transformers\n",
        "- **Swish / SiLU** – z · sigmoid(z)\n",
        "- **SwiGLU** – gated Swish variant (common in transformers)\n",
        "- **Mish** – smooth, GELU-like\n",
        "- **ReLU2** – square of ReLU, simple but powerful\n"
      ],
      "metadata": {
        "id": "opRJu4hYekBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# ReLU2\n",
        "y = F.relu(z).square()\n",
        "\n",
        "# SwiGLU\n",
        "z1, z2 = z.chunk(2, dim=-1)\n",
        "y = F.silu(beta * z1) * z2\n"
      ],
      "metadata": {
        "id": "BbWkQroeelZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Normalization (BN)\n",
        "\n",
        "Batch norm normalizes layer inputs using batch statistics, then learns:\n",
        "- A scale parameter γ\n",
        "- A shift parameter β\n",
        "\n",
        "BN reduces vanishing gradients, allows larger learning rates, and acts as a regularizer.\n"
      ],
      "metadata": {
        "id": "g4xk0QqFemRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.BatchNorm1d(28 * 28),\n",
        "    nn.Linear(28 * 28, 300),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(300),\n",
        "    nn.Linear(300, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(100),\n",
        "    nn.Linear(100, 10)\n",
        ")\n"
      ],
      "metadata": {
        "id": "O4hE8JsqenP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ Always remember:\n",
        "\n",
        "- `model.train()` during training  \n",
        "- `model.eval()` during evaluation\n"
      ],
      "metadata": {
        "id": "zAn4KkAyeoX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer Normalization (LN)\n",
        "\n",
        "LN normalizes across feature dimensions instead of the batch dimension.\n",
        "\n",
        "Advantages:\n",
        "- Same behavior during training and inference\n",
        "- Works well with RNNs and transformers\n"
      ],
      "metadata": {
        "id": "cb0D1QhTepRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.randn(32, 3, 100, 200)\n",
        "\n",
        "layer_norm = nn.LayerNorm([3, 100, 200])\n",
        "outputs = layer_norm(inputs)\n"
      ],
      "metadata": {
        "id": "SJQNfe1UeqRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Clipping\n",
        "\n",
        "Gradient clipping prevents exploding gradients by limiting their magnitude.\n"
      ],
      "metadata": {
        "id": "efeqXQ4uerZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n"
      ],
      "metadata": {
        "id": "NOZ5Qo3xesIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reusing Pretrained Layers (Transfer Learning)\n",
        "\n",
        "Training a very large deep neural network from scratch is usually not ideal. Instead, you should first try to find an existing model trained on a similar task and reuse most of its layers. This technique is called **transfer learning**.\n",
        "\n",
        "Transfer learning:\n",
        "- Speeds up training\n",
        "- Requires much less labeled data\n",
        "- Often leads to better generalization\n",
        "\n",
        "Typically:\n",
        "- Lower layers learn generic features (edges, textures, shapes)\n",
        "- Upper layers learn task-specific patterns\n"
      ],
      "metadata": {
        "id": "ilyc0OThfxM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What to Reuse\n",
        "\n",
        "When reusing a pretrained model:\n",
        "- Replace the **output layer** (it likely has the wrong number of outputs)\n",
        "- Reuse **lower hidden layers**\n",
        "- Upper layers may or may not be reused depending on task similarity\n",
        "\n",
        "> The more similar the tasks are, the more layers you should reuse.\n"
      ],
      "metadata": {
        "id": "ihk_A8pJfyAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best Practices\n",
        "\n",
        "1. Freeze reused layers initially  \n",
        "2. Train only the new output layer  \n",
        "3. Gradually unfreeze top layers  \n",
        "4. Reduce learning rate when unfreezing  \n",
        "5. More data → more layers can be unfrozen\n"
      ],
      "metadata": {
        "id": "cN_S73CPfyx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "metadata": {
        "id": "YAQ-s9wWfzgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Original Model (Model A)\n",
        "\n",
        "Assume Model A was trained on an 8-class Fashion-MNIST-like dataset.\n"
      ],
      "metadata": {
        "id": "RAN_IMS7f4oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "model_A = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(1 * 28 * 28, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 8)\n",
        ")\n",
        "\n",
        "# model_A is assumed to be trained or loaded with pretrained weights\n"
      ],
      "metadata": {
        "id": "XqDV9OTsf5mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reusing Model A for a New Binary Classification Task (Model B)\n",
        "\n",
        "We remove the output layer and add a new one with a single output.\n"
      ],
      "metadata": {
        "id": "-MP6s0dqglO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "reused_layers = copy.deepcopy(model_A[:-1])\n",
        "\n",
        "model_B_on_A = nn.Sequential(\n",
        "    *reused_layers,\n",
        "    nn.Linear(100, 1)  # binary classification\n",
        ")\n"
      ],
      "metadata": {
        "id": "avu69OmfgmCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Freezing Reused Layers\n",
        "\n",
        "This prevents large gradients from damaging pretrained weights early in training.\n"
      ],
      "metadata": {
        "id": "fyegQhU_gnG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model_B_on_A[:-1]:\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "VDkAuvccgnyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function for Binary Classification\n",
        "\n",
        "We use `BCEWithLogitsLoss`, which combines a sigmoid layer and binary cross-entropy.\n"
      ],
      "metadata": {
        "id": "sOedN4WHgo4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCEWithLogitsLoss()\n"
      ],
      "metadata": {
        "id": "-shQMDu4gpfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After Initial Training\n",
        "\n",
        "Once the new output layer has stabilized:\n",
        "- Unfreeze reused layers\n",
        "- Reduce learning rate\n",
        "- Fine-tune the entire model\n"
      ],
      "metadata": {
        "id": "O8rv8rfrgqg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model_B_on_A.parameters():\n",
        "    param.requires_grad = True\n"
      ],
      "metadata": {
        "id": "xz00mxcygrU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Important Reality Check\n",
        "\n",
        "Transfer learning:\n",
        "- Works **extremely well** for CNNs and Transformers\n",
        "- Often **does not help much** for small dense networks\n",
        "- Results can vary wildly with random seeds and dataset splits\n",
        "\n",
        "Be cautious of overly positive results — they may be cherry-picked.\n"
      ],
      "metadata": {
        "id": "NglIsVqzhdf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised Pretraining\n",
        "\n",
        "If you have:\n",
        "- Little labeled data\n",
        "- Plenty of unlabeled data\n",
        "- No similar pretrained model\n",
        "\n",
        "You can:\n",
        "1. Train an unsupervised model (e.g., autoencoder)\n",
        "2. Reuse lower layers\n",
        "3. Fine-tune using labeled data\n"
      ],
      "metadata": {
        "id": "WC26B6ZyidzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Historical Note\n",
        "\n",
        "Early deep learning relied heavily on **greedy layer-wise pretraining** using RBMs.\n",
        "\n",
        "Today:\n",
        "- Entire unsupervised models are trained in one shot\n",
        "- Autoencoders and diffusion models are preferred\n"
      ],
      "metadata": {
        "id": "L3of1wOfierP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretraining on an Auxiliary Task\n",
        "\n",
        "Another strategy is **self-supervised learning**:\n",
        "- Automatically generate labels\n",
        "- Train on a related task\n",
        "- Reuse learned representations\n",
        "\n",
        "Example:\n",
        "- Masked-word prediction for NLP\n",
        "- Same idea behind modern language models\n"
      ],
      "metadata": {
        "id": "i1DEB6HkiflE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Legal & Ethical Warning\n",
        "\n",
        "Scraping images or personal data:\n",
        "- May violate copyright law\n",
        "- Often violates privacy laws\n",
        "- Requires explicit consent in many countries\n"
      ],
      "metadata": {
        "id": "yel37RMHigbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Takeaway\n",
        "\n",
        "If you lack labeled data:\n",
        "1. Try transfer learning\n",
        "2. Try unsupervised pretraining\n",
        "3. Try self-supervised auxiliary tasks\n",
        "\n",
        "These techniques power modern deep learning.\n"
      ],
      "metadata": {
        "id": "MfWHSlUdit1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reusing Pretrained Layers (Transfer Learning)\n",
        "\n",
        "Training a very large deep neural network (DNN) from scratch is usually not a good idea if a similar pretrained model already exists. Instead, you can reuse most of the layers of an existing model and only retrain the top layers. This technique is called **transfer learning**.\n",
        "\n",
        "Transfer learning significantly speeds up training and requires far less labeled data.\n"
      ],
      "metadata": {
        "id": "Ou3TPQzrjS_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Transfer Learning Works\n",
        "\n",
        "Suppose you have a DNN trained to classify images into 100 categories (animals, plants, vehicles, etc.), and you now want to classify **specific types of vehicles**. These tasks overlap, so the lower layers of the original network—which detect edges, textures, and shapes—are still useful.\n",
        "\n",
        "Only the top layers, which learn task-specific patterns, usually need to be replaced.\n"
      ],
      "metadata": {
        "id": "VHp4d_e3jVCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Important Notes\n",
        "\n",
        "- If the new task uses images of a different size, you must resize them to match the original model’s input.\n",
        "- Transfer learning works best when the new task has **similar low-level features**.\n",
        "- Models trained on natural photos usually do **not** transfer well to medical or satellite images.\n"
      ],
      "metadata": {
        "id": "cP7B7br0jV2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Which Layers Should Be Reused?\n",
        "\n",
        "- The **output layer** should almost always be replaced.\n",
        "- Lower hidden layers are more reusable than upper layers.\n",
        "- The more similar the tasks, the more layers you should reuse.\n"
      ],
      "metadata": {
        "id": "-rIMolTwjWw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practical Strategy\n",
        "\n",
        "1. Reuse lower layers from the pretrained model.\n",
        "2. Freeze the reused layers initially.\n",
        "3. Train the new output layer.\n",
        "4. Gradually unfreeze top layers and fine-tune with a smaller learning rate.\n"
      ],
      "metadata": {
        "id": "i-Lk4WE1jXsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning with PyTorch – Example\n",
        "\n",
        "Assume a model (Model A) was trained on Fashion MNIST with **8 classes**.\n",
        "We now want to build a **binary classifier** (T-shirt vs Pullover) using only **20 labeled images**.\n"
      ],
      "metadata": {
        "id": "psV6HY7BjYeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model_A = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(1 * 28 * 28, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 8)  # output layer for 8 classes\n",
        ")\n",
        "\n",
        "# Assume model_A is already trained or pretrained\n"
      ],
      "metadata": {
        "id": "9ndj8ccYjaWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reusing All Layers Except the Output Layer\n",
        "\n",
        "We copy all layers except the last one and add a new output layer suitable for binary classification.\n"
      ],
      "metadata": {
        "id": "q9FRrQqcjeMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "reused_layers = copy.deepcopy(model_A[:-1])\n",
        "\n",
        "model_B_on_A = nn.Sequential(\n",
        "    *reused_layers,\n",
        "    nn.Linear(100, 1)  # binary classification output\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "iWXVtv19jfDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Freezing the Reused Layers\n",
        "\n",
        "To prevent large gradients from destroying pretrained weights, we freeze all reused layers at first.\n"
      ],
      "metadata": {
        "id": "hfZxVAgOjgFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model_B_on_A[:-1]:\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "xTXakqdLjgzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function and Metrics\n",
        "\n",
        "Since this is a **binary classification task**, we use `BCEWithLogitsLoss`.\n"
      ],
      "metadata": {
        "id": "1o2bi1e2jhvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n"
      ],
      "metadata": {
        "id": "T7Ak6mCFjidC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning\n",
        "\n",
        "After a few epochs:\n",
        "- Unfreeze the reused layers\n",
        "- Reduce the learning rate\n",
        "- Continue training to fine-tune the model\n"
      ],
      "metadata": {
        "id": "OIQpZeNZkZTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model_B_on_A.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Reduce learning rate in optimizer before continuing training\n"
      ],
      "metadata": {
        "id": "LFxAxGtmkaXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results and Caveats\n",
        "\n",
        "With transfer learning, test accuracy improved from **71.6% → 92.5%**.\n",
        "\n",
        "⚠️ However, this result depended heavily on:\n",
        "- Random seed\n",
        "- Class selection\n",
        "- Hyperparameter tuning\n",
        "\n",
        "This highlights the danger of **p-hacking**—reporting only the best results.\n"
      ],
      "metadata": {
        "id": "IBjlazxLkbCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When Transfer Learning Works Best\n",
        "\n",
        "Transfer learning is most effective with:\n",
        "- **Deep CNNs**\n",
        "- **Transformer architectures**\n",
        "\n",
        "It is much less effective with small dense networks.\n"
      ],
      "metadata": {
        "id": "Viuf0JHekb-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Pretraining\n",
        "\n",
        "When no similar pretrained model exists, you can:\n",
        "1. Collect large amounts of **unlabeled data**\n",
        "2. Train an **unsupervised model** (e.g., autoencoder)\n",
        "3. Reuse lower layers and fine-tune using labeled data\n"
      ],
      "metadata": {
        "id": "8OQKU2JHkc09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Historical Note\n",
        "\n",
        "Unsupervised pretraining (e.g., RBMs) was crucial to the revival of deep learning in 2006.\n",
        "Today, autoencoders and diffusion models are more common.\n"
      ],
      "metadata": {
        "id": "HwIPwMiIkddE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretraining on an Auxiliary Task\n",
        "\n",
        "If labeled data is scarce, train a model on a related task with easy-to-obtain labels, then reuse its lower layers.\n",
        "\n",
        "Example:\n",
        "- Train a model to determine whether two face images show the same person\n",
        "- Reuse its layers to build a face classifier with limited data\n"
      ],
      "metadata": {
        "id": "ppoYSVWUkg4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Supervised Learning (NLP Example)\n",
        "\n",
        "Mask words in text and train a model to predict them:\n",
        "> \"What ___ you saying?\"\n",
        "\n",
        "The model learns language structure and can later be fine-tuned for downstream tasks.\n"
      ],
      "metadata": {
        "id": "IHkEmx_Ukh31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faster Optimizers\n",
        "\n",
        "Training very large deep neural networks can be painfully slow. So far, we have seen several techniques to speed up training and improve convergence:\n",
        "\n",
        "- Better weight initialization\n",
        "- Better activation functions\n",
        "- Batch normalization or layer normalization\n",
        "- Transfer learning\n",
        "\n",
        "Another major speed boost comes from using **faster optimization algorithms** than plain gradient descent.\n"
      ],
      "metadata": {
        "id": "oLAjhgzylW07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview of Optimizers Covered\n",
        "\n",
        "In this section, we cover the most popular optimizers:\n",
        "\n",
        "- Momentum\n",
        "- Nesterov Accelerated Gradient (NAG)\n",
        "- AdaGrad\n",
        "- RMSProp\n",
        "- Adam and its variants (AdaMax, NAdam, AdamW)\n"
      ],
      "metadata": {
        "id": "MqaAaBlOmf0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Momentum Optimization\n",
        "\n",
        "Imagine a bowling ball rolling down a slope. At first it moves slowly, but it gradually accelerates as it builds momentum. This intuition inspired **momentum optimization**, proposed by Boris Polyak in 1964.\n",
        "\n",
        "Regular gradient descent never builds speed: it only reacts to the current gradient. Momentum, instead, accumulates past gradients to build velocity.\n"
      ],
      "metadata": {
        "id": "kCJ6ZrggmhJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent Recap\n",
        "\n",
        "Standard gradient descent updates parameters as:\n",
        "\n",
        "\\[\n",
        "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta J(\\theta)\n",
        "\\]\n",
        "\n",
        "If gradients are small, learning becomes very slow.\n"
      ],
      "metadata": {
        "id": "haI9fyzymim_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Momentum Update Rule\n",
        "\n",
        "Momentum introduces a velocity vector **m**:\n",
        "\n",
        "- Gradients act as acceleration\n",
        "- Parameters are updated using accumulated momentum\n",
        "- A momentum coefficient **β** controls friction\n",
        "\n",
        "Typical value: **β = 0.9**\n"
      ],
      "metadata": {
        "id": "OWFqIcAknzIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Momentum Is Faster\n",
        "\n",
        "If gradients remain constant, momentum reaches a terminal velocity:\n",
        "\n",
        "\\[\n",
        "\\text{velocity} = \\frac{\\eta}{1 - \\beta} \\nabla J\n",
        "\\]\n",
        "\n",
        "For β = 0.9, updates become roughly **10× faster** than standard gradient descent.\n"
      ],
      "metadata": {
        "id": "JOLBa_Ron1Ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Momentum Helps With:\n",
        "\n",
        "- Escaping plateaus\n",
        "- Moving faster through narrow valleys\n",
        "- Reducing training time in deep networks\n"
      ],
      "metadata": {
        "id": "ucorsmyin6f9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch: Momentum Optimizer\n"
      ],
      "metadata": {
        "id": "CtZH4WL9n7wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.05,\n",
        "    momentum=0.9\n",
        ")\n"
      ],
      "metadata": {
        "id": "pK-ZeaInn-Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drawback of Momentum\n",
        "\n",
        "Momentum introduces an extra hyperparameter (β).  \n",
        "Fortunately, **β = 0.9** works well in most cases.\n"
      ],
      "metadata": {
        "id": "gTRSm4Kin_FL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nesterov Accelerated Gradient (NAG)\n",
        "\n",
        "Nesterov momentum is a small but powerful improvement over standard momentum, proposed by Yurii Nesterov in 1983.\n"
      ],
      "metadata": {
        "id": "2A3ODoBaoLi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Idea\n",
        "\n",
        "Instead of computing the gradient at the current position θ, NAG computes it slightly **ahead**:\n",
        "\n",
        "\\[\n",
        "\\theta + \\beta m\n",
        "\\]\n",
        "\n",
        "This allows the optimizer to correct its trajectory earlier.\n"
      ],
      "metadata": {
        "id": "3TuQU_8HoMu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why NAG Is Better\n",
        "\n",
        "- More accurate gradient direction\n",
        "- Reduced oscillations\n",
        "- Faster convergence than regular momentum\n"
      ],
      "metadata": {
        "id": "-OYL-mEQoQ9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch: Nesterov Momentum\n"
      ],
      "metadata": {
        "id": "WBKymAFtoSNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.05,\n",
        "    momentum=0.9,\n",
        "    nesterov=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "KJZDyyFOoVZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaGrad\n",
        "\n",
        "AdaGrad adapts the learning rate for each parameter individually.  \n",
        "It is especially useful for problems with **uneven curvature**.\n"
      ],
      "metadata": {
        "id": "5qqD2M3Iod2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How AdaGrad Works\n",
        "\n",
        "- Accumulates squared gradients over time\n",
        "- Scales updates by the inverse square root of accumulated gradients\n",
        "- Steep dimensions slow down faster than shallow ones\n"
      ],
      "metadata": {
        "id": "TF-tXsb9ofCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benefit of AdaGrad\n",
        "\n",
        "- Automatically adapts learning rates\n",
        "- Requires less tuning of η\n",
        "- Corrects direction early toward the optimum\n"
      ],
      "metadata": {
        "id": "FRb2mIYlogQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Major Limitation\n",
        "\n",
        "AdaGrad keeps accumulating gradients forever, causing learning rates to shrink too much.\n",
        "\n",
        "➡️ Often **stops training too early** in deep neural networks.\n"
      ],
      "metadata": {
        "id": "rN0NNSVuohVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch: AdaGrad (Generally Not Recommended for DNNs)\n"
      ],
      "metadata": {
        "id": "yw58JDOPoig5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adagrad(\n",
        "    model.parameters(),\n",
        "    lr=0.05\n",
        ")\n"
      ],
      "metadata": {
        "id": "ttAJUlWXoj6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSProp\n",
        "\n",
        "RMSProp fixes AdaGrad’s main issue by using **exponentially decaying averages** instead of accumulating gradients forever.\n"
      ],
      "metadata": {
        "id": "Wyzb2h9oolLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSProp Key Idea\n",
        "\n",
        "- Keeps a moving average of squared gradients\n",
        "- Recent gradients matter more than old ones\n",
        "- Prevents learning rate from shrinking to zero\n"
      ],
      "metadata": {
        "id": "KZyz_XeyomNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Typical Hyperparameter\n",
        "\n",
        "- Decay rate α = **0.9**\n"
      ],
      "metadata": {
        "id": "vowYGY22opEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch: RMSProp\n"
      ],
      "metadata": {
        "id": "GwXwcCYdoqTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.RMSprop(\n",
        "    model.parameters(),\n",
        "    lr=0.05,\n",
        "    alpha=0.9\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ic29vq0UotLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSProp Summary\n",
        "\n",
        "- Much better than AdaGrad\n",
        "- Was the go-to optimizer before Adam\n",
        "- Still competitive on some tasks\n"
      ],
      "metadata": {
        "id": "le_ffrKGouRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adam (Adaptive Moment Estimation)\n",
        "\n",
        "Adam combines:\n",
        "\n",
        "- **Momentum** (first moment of gradients)\n",
        "- **RMSProp** (second moment of gradients)\n"
      ],
      "metadata": {
        "id": "TBE2duPUovqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Adam Tracks\n",
        "\n",
        "- Exponentially decaying average of gradients (mean)\n",
        "- Exponentially decaying average of squared gradients (variance)\n",
        "- Bias correction during early training\n"
      ],
      "metadata": {
        "id": "k43vkLqdoxrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Default Hyperparameters\n",
        "\n",
        "- β₁ = 0.9 (momentum)\n",
        "- β₂ = 0.999 (scaling)\n",
        "- ε = 1e-8\n",
        "- Learning rate η = **0.001**\n"
      ],
      "metadata": {
        "id": "XSKrO3Q6oy-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch: Adam Optimizer\n"
      ],
      "metadata": {
        "id": "0vFGhA9oo0iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=0.001,\n",
        "    betas=(0.9, 0.999)\n",
        ")\n"
      ],
      "metadata": {
        "id": "wsRh787qo1e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Adam Is Popular\n",
        "\n",
        "- Fast convergence\n",
        "- Minimal tuning required\n",
        "- Works well on many problems\n"
      ],
      "metadata": {
        "id": "vjwYtfpssLWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adam Variants\n"
      ],
      "metadata": {
        "id": "p4c_Si6BsMaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdaMax\n",
        "\n",
        "- Replaces ℓ2 norm with ℓ∞ norm\n",
        "- Can be more stable than Adam\n",
        "- Often slightly worse performance overall\n"
      ],
      "metadata": {
        "id": "Nxwn3J4XshZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adamax(\n",
        "    model.parameters(),\n",
        "    lr=0.001\n",
        ")\n"
      ],
      "metadata": {
        "id": "7S_qM3w-sjMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NAdam\n",
        "\n",
        "- Adam + Nesterov momentum\n",
        "- Often converges faster than Adam\n"
      ],
      "metadata": {
        "id": "p38kRdmjsli_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.NAdam(\n",
        "    model.parameters(),\n",
        "    lr=0.001\n",
        ")\n"
      ],
      "metadata": {
        "id": "LfOHtaOdsmv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdamW\n",
        "\n",
        "AdamW fixes how weight decay is applied in Adam.\n",
        "\n",
        "- Properly decouples weight decay from gradient updates\n",
        "- Often generalizes better than Adam\n"
      ],
      "metadata": {
        "id": "Jn4g7XDhsnth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.001,\n",
        "    weight_decay=1e-4\n",
        ")\n"
      ],
      "metadata": {
        "id": "WLRc2MkFsonn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important Warning About Adaptive Optimizers\n",
        "\n",
        "Adaptive optimizers (Adam, RMSProp, etc.) often converge fast but may **generalize poorly** on some datasets.\n",
        "\n",
        "If your model overfits or underperforms:\n",
        "➡️ Try **SGD with Nesterov momentum**\n"
      ],
      "metadata": {
        "id": "V1qzBVN4spkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second-Order Optimization (Brief Note)\n",
        "\n",
        "Second-order methods use Hessians (curvature information), but:\n",
        "\n",
        "- Require O(n²) memory\n",
        "- Too slow for large neural networks\n",
        "\n",
        "Approximate methods like **Shampoo** exist but are not built into PyTorch.\n"
      ],
      "metadata": {
        "id": "EegngGM7ssw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Sparse Models\n",
        "\n",
        "All optimizers discussed so far produce **dense models**.\n",
        "\n",
        "To get sparse models:\n",
        "- Prune small weights\n",
        "- Remove entire neurons or channels\n",
        "- Use ℓ1 regularization\n"
      ],
      "metadata": {
        "id": "zDkZGsdWstwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch: Weight Pruning Example\n"
      ],
      "metadata": {
        "id": "Lomsx6Ovsu5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "prune.l1_unstructured(\n",
        "    model.linear,\n",
        "    name=\"weight\",\n",
        "    amount=0.3\n",
        ")\n"
      ],
      "metadata": {
        "id": "ILcsejWBsqog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer Comparison Summary\n",
        "\n",
        "- SGD: slow but reliable\n",
        "- Momentum / NAG: fast and strong generalization\n",
        "- Adam / RMSProp: very fast, less tuning\n",
        "- AdamW: best Adam variant for generalization\n"
      ],
      "metadata": {
        "id": "ZmCS-V8wtxEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Rate Scheduling\n",
        "\n",
        "Finding a good learning rate is very important.\n",
        "\n",
        "- If the learning rate is too high, training will diverge.\n",
        "- If it is too low, training will be very slow and may get stuck.\n",
        "- With a constant learning rate, training may improve quickly at first but fail to converge well.\n",
        "\n",
        "A common strategy is to start with a higher learning rate and reduce it during training.\n",
        "PyTorch provides several learning rate schedulers in `torch.optim.lr_scheduler`.\n"
      ],
      "metadata": {
        "id": "3rroZJIAuYpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exponential Scheduling\n",
        "\n",
        "Exponential scheduling multiplies the learning rate by a constant factor `gamma`\n",
        "after each epoch:\n",
        "\n",
        "η_t = η_0 · gamma^t\n",
        "\n",
        "Typically:\n",
        "- gamma < 1\n",
        "- Common values: 0.9, 0.95, 0.99\n"
      ],
      "metadata": {
        "id": "70rH7ZAeuZut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(8, 50),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(50, 40),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(40, 1)\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
        "    optimizer, gamma=0.9\n",
        ")\n"
      ],
      "metadata": {
        "id": "yzepola1uhr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = mse(model(X_batch), y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()\n"
      ],
      "metadata": {
        "id": "FWaGDzgXugjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine Annealing\n",
        "\n",
        "Cosine annealing gradually decreases the learning rate from a maximum value\n",
        "to a minimum value using a cosine curve.\n",
        "\n",
        "This keeps the learning rate high for most of training and allows fine-tuning\n",
        "near the end.\n"
      ],
      "metadata": {
        "id": "5ttDYheVujmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=20,\n",
        "    eta_min=0.001\n",
        ")\n"
      ],
      "metadata": {
        "id": "YcH8hIT6ukbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Scheduling (ReduceLROnPlateau)\n",
        "\n",
        "Performance scheduling adjusts the learning rate based on a validation metric.\n",
        "If the metric stops improving, the learning rate is reduced.\n",
        "\n",
        "Common parameters:\n",
        "- mode: \"min\" or \"max\"\n",
        "- patience: number of epochs to wait\n",
        "- factor: multiplicative drop in learning rate\n"
      ],
      "metadata": {
        "id": "ULojj0esvE9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode=\"max\",\n",
        "    patience=2,\n",
        "    factor=0.1\n",
        ")\n"
      ],
      "metadata": {
        "id": "rn5xdkP7vF0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import Accuracy\n",
        "\n",
        "metric = Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = mse(model(X_batch), y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    val_acc = evaluate_tm(model, valid_loader, metric).item()\n",
        "    scheduler.step(val_acc)\n"
      ],
      "metadata": {
        "id": "8ViJRIR1vGyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Rate Warm-Up\n",
        "\n",
        "Warm-up starts training with a very small learning rate and gradually increases it.\n",
        "This helps stabilize training early on, especially with large batch sizes\n",
        "or sensitive models.\n"
      ],
      "metadata": {
        "id": "r3ytOXHKvHky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "    optimizer,\n",
        "    start_factor=0.1,\n",
        "    end_factor=1.0,\n",
        "    total_iters=3\n",
        ")\n"
      ],
      "metadata": {
        "id": "d7InflohvL_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "    warmup_scheduler.step()\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = mse(model(X_batch), y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch >= 3:\n",
        "        scheduler.step(val_metric)\n"
      ],
      "metadata": {
        "id": "yK7XY32TvMz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine Annealing with Warm Restarts\n",
        "\n",
        "This schedule repeatedly applies cosine annealing.\n",
        "The learning rate periodically jumps back up, helping escape local minima.\n",
        "\n",
        "Each cycle can be longer than the previous one.\n"
      ],
      "metadata": {
        "id": "EblIk-GrvNrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_repeat_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer,\n",
        "    T_0=2,\n",
        "    T_mult=2,\n",
        "    eta_min=0.001\n",
        ")\n"
      ],
      "metadata": {
        "id": "p52i0qUuvOgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1cycle Scheduling\n",
        "\n",
        "1cycle scheduling:\n",
        "- Warms up the learning rate\n",
        "- Gradually cools it down\n",
        "- Often converges faster and better\n",
        "\n",
        "It is implemented in PyTorch as `OneCycleLR`.\n"
      ],
      "metadata": {
        "id": "BTjwVAO5vPkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.1,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=n_epochs\n",
        ")\n"
      ],
      "metadata": {
        "id": "pg5ZYnmqvQlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "- Always reduce the learning rate near the end of training\n",
        "- Use warm-up if training is unstable at the start\n",
        "- Use ReduceLROnPlateau if progress stalls\n",
        "- 1cycle is a strong default choice\n"
      ],
      "metadata": {
        "id": "0tUgfSTavSC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avoiding Overfitting Through Regularization\n",
        "\n",
        "> “With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.”\n",
        "> — John von Neumann (via Enrico Fermi)\n",
        "\n",
        "Deep neural networks often have tens of thousands to billions of parameters.\n",
        "This flexibility allows them to fit complex datasets, but it also makes them\n",
        "highly prone to overfitting.\n",
        "\n",
        "Regularization techniques help constrain the model so it generalizes better.\n",
        "In this section we cover:\n",
        "- ℓ1 and ℓ2 regularization\n",
        "- Dropout\n",
        "- Monte Carlo (MC) Dropout\n",
        "- Max-norm regularization\n"
      ],
      "metadata": {
        "id": "xEtPgIbzvlU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ℓ1 and ℓ2 Regularization\n",
        "\n",
        "ℓ2 regularization discourages large weights and is equivalent to weight decay\n",
        "when using SGD (with or without momentum).\n",
        "\n",
        "ℓ1 regularization encourages sparsity by driving many weights to zero.\n"
      ],
      "metadata": {
        "id": "aG29PcWrvnco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ℓ2 regularization using weight decay (SGD)\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.05,\n",
        "    weight_decay=1e-4\n",
        ")\n"
      ],
      "metadata": {
        "id": "b3D4qAQTvoD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using Adam, you should use AdamW instead of Adam to get proper weight decay.\n"
      ],
      "metadata": {
        "id": "n2dINQ0Hvo1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.001,\n",
        "    weight_decay=1e-4\n",
        ")\n"
      ],
      "metadata": {
        "id": "zk43lofOvpwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual ℓ2 Regularization (Selective Parameters)\n",
        "\n",
        "Weight decay applies to all parameters by default, including biases and\n",
        "batch-norm parameters. Sometimes you want to exclude those.\n"
      ],
      "metadata": {
        "id": "bQXib0ezvqaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
        "\n",
        "params_to_regularize = [\n",
        "    param for name, param in model.named_parameters()\n",
        "    if \"bias\" not in name and \"bn\" not in name\n",
        "]\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_batch)\n",
        "        main_loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "        l2_loss = sum(param.pow(2).sum() for param in params_to_regularize)\n",
        "        loss = main_loss + 1e-4 * l2_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "ne5xJcrsvrPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter Groups for Selective Weight Decay\n",
        "\n",
        "Parameter groups allow different hyperparameters for different parts of the model.\n"
      ],
      "metadata": {
        "id": "_k_Cms30vsJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_bias_and_bn = [\n",
        "    param for name, param in model.named_parameters()\n",
        "    if \"bias\" in name or \"bn\" in name\n",
        "]\n",
        "\n",
        "optimizer = torch.optim.SGD(\n",
        "    [\n",
        "        {\"params\": params_to_regularize, \"weight_decay\": 1e-4},\n",
        "        {\"params\": params_bias_and_bn}\n",
        "    ],\n",
        "    lr=0.05\n",
        ")\n"
      ],
      "metadata": {
        "id": "wW3cnX-lvtF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ℓ1 Regularization\n",
        "\n",
        "PyTorch does not provide built-in ℓ1 regularization, so it must be added manually.\n"
      ],
      "metadata": {
        "id": "CNvyT3DIvuOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1_loss = sum(param.abs().sum() for param in params_to_regularize)\n",
        "loss = main_loss + 1e-4 * l1_loss\n"
      ],
      "metadata": {
        "id": "um27wJfevvkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropout\n",
        "\n",
        "Dropout randomly disables neurons during training.\n",
        "Each neuron has probability `p` of being dropped at each step.\n",
        "\n",
        "Typical dropout rates:\n",
        "- 20–30% for recurrent networks\n",
        "- 40–50% for convolutional networks\n",
        "\n",
        "Dropout is only active during training.\n"
      ],
      "metadata": {
        "id": "BcsGmvYDvwam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Dropout(p=0.2), nn.Linear(28 * 28, 100), nn.ReLU(),\n",
        "    nn.Dropout(p=0.2), nn.Linear(100, 100), nn.ReLU(),\n",
        "    nn.Dropout(p=0.2), nn.Linear(100, 100), nn.ReLU(),\n",
        "    nn.Dropout(p=0.2), nn.Linear(100, 10)\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "zSCaY4AsvxKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ Warning\n",
        "\n",
        "Since dropout is disabled during evaluation, training loss and validation loss\n",
        "may appear similar even when the model is overfitting.\n",
        "\n",
        "Always evaluate training loss with dropout disabled.\n"
      ],
      "metadata": {
        "id": "gZGUkAwGvyE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the model overfits, increase the dropout rate.\n",
        "If it underfits, decrease the dropout rate.\n",
        "\n",
        "Often, applying dropout only to the top hidden layers works best.\n"
      ],
      "metadata": {
        "id": "sYaOMB40vy10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alpha Dropout\n",
        "\n",
        "For self-normalizing networks using SELU activation,\n",
        "use AlphaDropout instead of standard dropout.\n"
      ],
      "metadata": {
        "id": "oklRAlQMvzka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.AlphaDropout(p=0.1)\n"
      ],
      "metadata": {
        "id": "lOkb7SSxwW0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monte Carlo (MC) Dropout\n",
        "\n",
        "MC dropout keeps dropout active during inference.\n",
        "Multiple stochastic predictions are averaged to improve accuracy\n",
        "and estimate uncertainty.\n"
      ],
      "metadata": {
        "id": "mkXKbNS4wX0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "for module in model.modules():\n",
        "    if isinstance(module, nn.Dropout):\n",
        "        module.train()\n"
      ],
      "metadata": {
        "id": "rwvszmWswZNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = X_new.to(device)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "with torch.no_grad():\n",
        "    X_new_repeated = X_new.repeat_interleave(100, dim=0)\n",
        "    y_logits_all = model(X_new_repeated).reshape(3, 100, 10)\n",
        "    y_probas_all = torch.softmax(y_logits_all, dim=-1)\n",
        "    y_probas = y_probas_all.mean(dim=1)\n"
      ],
      "metadata": {
        "id": "kOgk9pqDwZ5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average probabilities across Monte Carlo samples:\n"
      ],
      "metadata": {
        "id": "LviAKJPrwa1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_probas.round(decimals=2)\n"
      ],
      "metadata": {
        "id": "HVCA1L9VwbkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard deviation of predicted probabilities gives uncertainty estimates.\n"
      ],
      "metadata": {
        "id": "KS1I9Mntwcbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_std = y_probas_all.std(dim=1)\n",
        "y_std.round(decimals=2)\n"
      ],
      "metadata": {
        "id": "XwvGRhkLwd1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ Important\n",
        "\n",
        "Do NOT average logits before applying softmax.\n",
        "Always average probabilities to correctly reflect uncertainty.\n"
      ],
      "metadata": {
        "id": "qn907xBVwgQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom MC Dropout Layer\n",
        "\n",
        "If training from scratch, use a dedicated MC Dropout module.\n"
      ],
      "metadata": {
        "id": "9xd_0uiqwhFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class McDropout(nn.Dropout):\n",
        "    def forward(self, input):\n",
        "        return F.dropout(input, self.p, training=True)\n"
      ],
      "metadata": {
        "id": "1P3F54oEwh8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Max-Norm Regularization\n",
        "\n",
        "Max-norm regularization constrains the ℓ2 norm of incoming weights for each neuron:\n",
        "\n",
        "‖w‖₂ ≤ r\n",
        "\n",
        "Instead of adding a loss term, weights are rescaled after each update.\n"
      ],
      "metadata": {
        "id": "_Tr-A8ZYwitW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Max-Norm Regularization\n",
        "\n",
        "Max-norm regularization constrains the ℓ2 norm of incoming weights for each neuron:\n",
        "\n",
        "‖w‖₂ ≤ r\n",
        "\n",
        "Instead of adding a loss term, weights are rescaled after each update.\n"
      ],
      "metadata": {
        "id": "R79co1KcwkJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call after optimizer.step()\n",
        "optimizer.step()\n",
        "apply_max_norm(model, max_norm=2.0)\n"
      ],
      "metadata": {
        "id": "96Op3__OwlFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TIP\n",
        "\n",
        "For convolutional layers, use:\n",
        "dim = [1, 2, 3]\n",
        "\n",
        "This constrains each convolutional kernel instead of each neuron.\n"
      ],
      "metadata": {
        "id": "tCjjzC9TwnO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Guidelines\n",
        "\n",
        "In this chapter we have covered a wide range of techniques, and you may be\n",
        "wondering which ones you should use. This depends on the task, and there is no\n",
        "clear consensus yet, but the configuration below works well in most cases\n",
        "without requiring much hyperparameter tuning.\n",
        "\n",
        "⚠️ These are **guidelines**, not hard rules.\n",
        "\n",
        "---\n",
        "\n",
        "## Recommended Default Configuration\n",
        "\n",
        "| Hyperparameter            | Default value                                  |\n",
        "|---------------------------|-----------------------------------------------|\n",
        "| Kernel initializer        | He initialization                              |\n",
        "| Activation function       | ReLU if shallow; Swish if deep                 |\n",
        "| Normalization             | None if shallow; batch-norm or layer-norm if deep |\n",
        "| Regularization            | Early stopping; weight decay if needed         |\n",
        "| Optimizer                 | Nesterov accelerated gradients or AdamW        |\n",
        "| Learning rate schedule    | Performance scheduling or 1cycle               |\n",
        "\n",
        "---\n",
        "\n",
        "## Pretraining Guidelines\n",
        "\n",
        "You should also try to:\n",
        "- Reuse parts of a **pretrained neural network** if one exists for a similar task\n",
        "- Use **unsupervised pretraining** if you have a lot of unlabeled data\n",
        "- Use **pretraining on an auxiliary task** if you have labeled data for a related task\n",
        "\n",
        "---\n",
        "\n",
        "## Important Exceptions\n",
        "\n",
        "### Sparse Models\n",
        "If you need a sparse model:\n",
        "- Use **ℓ1 regularization**\n",
        "- Optionally prune small weights after training (e.g. `torch.nn.utils.prune.l1_unstructured()`)\n",
        "\n",
        "⚠️ Note: This breaks self-normalization, so avoid SELU-based architectures.\n",
        "\n",
        "---\n",
        "\n",
        "### Low-Latency Models\n",
        "If inference speed is critical:\n",
        "- Use fewer layers\n",
        "- Use fast activations such as:\n",
        "  - `nn.ReLU`\n",
        "  - `nn.LeakyReLU`\n",
        "  - `nn.Hardswish`\n",
        "- Fold batch-norm and layer-norm into previous layers after training\n",
        "- Prefer sparse models\n",
        "- Reduce numerical precision:\n",
        "  - FP16\n",
        "  - INT8\n",
        "\n",
        "Appendix B covers:\n",
        "- Reduced precision models\n",
        "- Mixed precision training\n",
        "- Quantization\n",
        "\n",
        "---\n",
        "\n",
        "### Risk-Sensitive Applications\n",
        "If uncertainty matters more than latency:\n",
        "- Use **Monte Carlo Dropout**\n",
        "- Gain:\n",
        "  - Better predictive performance\n",
        "  - Reliable probability estimates\n",
        "  - Uncertainty estimates\n",
        "\n",
        "---\n",
        "\n",
        "## Chapter Wrap-Up\n",
        "\n",
        "Over the last three chapters, we have learned:\n",
        "- What artificial neural networks are\n",
        "- How to build and train them using Scikit-Learn and PyTorch\n",
        "- Practical techniques to train deep and complex networks\n",
        "\n",
        "In the next chapter, all of this comes together as we dive into one of the most\n",
        "important applications of deep learning:\n",
        "\n",
        "👉 **Computer Vision**\n"
      ],
      "metadata": {
        "id": "AkfkOvjnxeAp"
      }
    }
  ]
}