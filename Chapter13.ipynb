{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgLTOk8Vc7pa+6Lv4ZftKx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanDCunha/Hands-On-Machine-Learning-with-Scikit-Learn-and-PyTorch/blob/main/Chapter13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neurons and Layers\n",
        "\n",
        "Up to now we have focused on feedforward neural networks, where the activations flow only in one direction, from the input layer to the output layer. A recurrent neural network (RNN) looks very much like a feedforward neural network, except it also has connections pointing backward.\n",
        "\n",
        "A recurrent neuron receives:\n",
        "- The input at time step *t*, denoted **x(t)**\n",
        "- Its own output (or hidden state) from the previous time step, **ŷ(t−1)**\n",
        "\n",
        "At the first time step, the previous output is typically initialized to zero. Representing the same neuron across multiple time steps is called **unrolling the network through time**.\n"
      ],
      "metadata": {
        "id": "N57k7NCz9ksV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Layers\n",
        "\n",
        "A recurrent layer contains multiple recurrent neurons. At each time step *t*, every neuron receives:\n",
        "- The input vector **x(t)**\n",
        "- The output vector from the previous time step **ŷ(t−1)**\n",
        "\n",
        "Each neuron has two sets of weights:\n",
        "- One for the inputs\n",
        "- One for the recurrent (previous output) connections\n",
        "\n",
        "For the whole layer, these weights are stored in matrices:\n",
        "- **Wₓ** for inputs\n",
        "- **Wᵧ̂** for recurrent connections\n"
      ],
      "metadata": {
        "id": "cm1bskw99l3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical Formulation\n",
        "\n",
        "For a single instance, the output of a recurrent layer at time step *t* is:\n",
        "\n",
        "Ŷ(t) = φ(X(t)Wₓ + Ŷ(t−1)Wᵧ̂ + b)\n",
        "\n",
        "Where:\n",
        "- φ(·) is an activation function (e.g., ReLU or tanh)\n",
        "- b is a bias vector\n",
        "\n",
        "For a mini-batch, the computation can be written as:\n",
        "\n",
        "Ŷ(t) = φ([X(t) Ŷ(t−1)]W + b)\n",
        "\n",
        "The notation [X(t) Ŷ(t−1)] represents the horizontal concatenation of the input and previous output.\n"
      ],
      "metadata": {
        "id": "c1HWJxc09nGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temporal Dependency\n",
        "\n",
        "Because Ŷ(t) depends on Ŷ(t−1), which depends on Ŷ(t−2), and so on, the output at time *t* is a function of **all previous inputs**:\n",
        "\n",
        "X(0), X(1), …, X(t)\n",
        "\n",
        "This is what gives RNNs their ability to model sequences.\n"
      ],
      "metadata": {
        "id": "N3xeAkiI9oJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory Cells\n",
        "\n",
        "A component that preserves information across time steps is called a **memory cell**.\n",
        "\n",
        "The hidden state at time *t* is denoted **h(t)** and defined as:\n",
        "\n",
        "h(t) = f(x(t), h(t−1))\n",
        "\n",
        "The output at time *t* is often simply equal to the hidden state, but in more advanced architectures this may differ.\n"
      ],
      "metadata": {
        "id": "JWvScvOu9pNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Feedback vs State Feedback\n",
        "\n",
        "- **Jordan RNN (1986):** feeds back the output\n",
        "- **Elman RNN (1990):** feeds back the hidden state (most common today)\n",
        "\n",
        "Modern RNNs almost always use **state feedback**.\n"
      ],
      "metadata": {
        "id": "uucgobky9p_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input and Output Sequences\n",
        "\n",
        "RNNs can be structured in several ways:\n",
        "\n",
        "1. **Sequence → Sequence**  \n",
        "   Example: time series forecasting\n",
        "\n",
        "2. **Sequence → Vector**  \n",
        "   Example: sentiment analysis\n",
        "\n",
        "3. **Vector → Sequence**  \n",
        "   Example: image captioning\n",
        "\n",
        "4. **Encoder–Decoder (Sequence → Vector → Sequence)**  \n",
        "   Example: machine translation\n",
        "\n",
        "Encoder–decoder models work better than direct sequence-to-sequence models because the full input sequence is processed before generating outputs.\n"
      ],
      "metadata": {
        "id": "BaYn8wnp9q_q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQyQ7DCw8Vq1"
      },
      "outputs": [],
      "source": [
        "# Simple example of a basic RNN cell in PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleRNNCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.Wx = nn.Linear(input_size, hidden_size)\n",
        "        self.Wh = nn.Linear(hidden_size, hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x_t, h_prev):\n",
        "        h_t = self.activation(self.Wx(x_t) + self.Wh(h_prev))\n",
        "        return h_t\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This basic RNN cell:\n",
        "- Takes the current input x(t)\n",
        "- Combines it with the previous hidden state h(t−1)\n",
        "- Produces a new hidden state h(t)\n",
        "\n",
        "Training such models requires **Backpropagation Through Time (BPTT)**, which introduces challenges such as vanishing and exploding gradients—motivating more advanced cells like **LSTMs** and **GRUs**.\n"
      ],
      "metadata": {
        "id": "gAb8CGrX9tDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training RNNs\n",
        "\n",
        "To train an RNN, the trick is to unroll it through time and then use regular backpropagation.  \n",
        "This strategy is called **backpropagation through time (BPTT)**.\n",
        "\n",
        "Just like in regular backpropagation, there is a forward pass through the unrolled network, followed by computing a loss over the output sequence:\n",
        "\n",
        "ℒ(Y(0), Y(1), …, Y(T); Ŷ(0), Ŷ(1), …, Ŷ(T))\n",
        "\n",
        "The gradients of that loss are then propagated backward through the unrolled network.  \n",
        "If some outputs are ignored by the loss (e.g., sequence-to-vector models), gradients only flow through the outputs that contribute to the loss.\n",
        "\n",
        "Since the same parameters are reused at each time step, their gradients are accumulated multiple times.  \n",
        "Once all gradients are computed, a gradient descent step updates the parameters.\n",
        "\n",
        "Fortunately, modern frameworks like PyTorch handle all of this automatically.\n"
      ],
      "metadata": {
        "id": "Io5pQGvoAcTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forecasting a Time Series\n",
        "\n",
        "Suppose you are tasked with forecasting daily bus and rail ridership for Chicago’s Transit Authority.  \n",
        "You have daily ridership data since 2001, and your goal is to predict tomorrow’s ridership.\n",
        "\n",
        "We begin by loading and cleaning the data.\n"
      ],
      "metadata": {
        "id": "WaU1TE4-AeId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "path = Path(\"datasets/ridership/CTA_-_Ridership_-_Daily_Boarding_Totals.csv\")\n",
        "df = pd.read_csv(path, parse_dates=[\"service_date\"])\n",
        "df.columns = [\"date\", \"day_type\", \"bus\", \"rail\", \"total\"]\n",
        "df = df.sort_values(\"date\").set_index(\"date\")\n",
        "df = df.drop(\"total\", axis=1)\n",
        "df = df.drop_duplicates()\n"
      ],
      "metadata": {
        "id": "rh-3qBzVAfKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s inspect the first few rows of the dataset.\n"
      ],
      "metadata": {
        "id": "NVMUCRbFAf_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n"
      ],
      "metadata": {
        "id": "-iuT5Qg_AhVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `day_type` column encodes:\n",
        "- `W`: weekday\n",
        "- `A`: Saturday\n",
        "- `U`: Sunday or holiday\n",
        "\n",
        "Next, let’s visualize bus and rail ridership over a few months in 2019.\n"
      ],
      "metadata": {
        "id": "B6aAqsuYAkEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df[\"2019-03\":\"2019-05\"].plot(grid=True, marker=\".\", figsize=(8, 3.5))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C3w5WjeSAk9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a **multivariate time series** since multiple values exist per time step.\n",
        "A strong **weekly seasonality** is clearly visible.\n",
        "\n",
        "A simple baseline is **naive forecasting**: copying the value from one week earlier.\n"
      ],
      "metadata": {
        "id": "kMSeM_CuAlup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Forecasting and Differencing\n",
        "\n",
        "To visualize naive forecasts, we compare the original series with a 7-day lagged version and compute their difference.\n"
      ],
      "metadata": {
        "id": "XTZem1MCAmoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diff_7 = df[[\"bus\", \"rail\"]].diff(7)[\"2019-03\":\"2019-05\"]\n",
        "\n",
        "fig, axs = plt.subplots(2, 1, sharex=True, figsize=(8, 5))\n",
        "df.plot(ax=axs[0], legend=False, marker=\".\")\n",
        "df.shift(7).plot(ax=axs[0], grid=True, legend=False, linestyle=\":\")\n",
        "diff_7.plot(ax=axs[1], grid=True, marker=\".\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NlxuUTEeAoAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lagged series tracks the original closely, indicating **autocorrelation**.\n",
        "Large deviations correspond to holidays, such as Memorial Day.\n"
      ],
      "metadata": {
        "id": "om9zvt_pAp_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(df.loc[\"2019-05-25\":\"2019-05-27\"][\"day_type\"])\n"
      ],
      "metadata": {
        "id": "6NOoI4OaAqtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Error Metrics\n",
        "\n",
        "We compute the **mean absolute error (MAE)** and **mean absolute percentage error (MAPE)**.\n"
      ],
      "metadata": {
        "id": "-OO-N01PArtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diff_7.abs().mean()\n"
      ],
      "metadata": {
        "id": "R8U8jsL2AssQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets = df[[\"bus\", \"rail\"]][\"2019-03\":\"2019-05\"]\n",
        "(diff_7 / targets).abs().mean()\n"
      ],
      "metadata": {
        "id": "Ws-DDwcRAtzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAPE is approximately:\n",
        "- 8.3% for bus\n",
        "- 9.0% for rail\n",
        "\n",
        "These provide a strong baseline.\n"
      ],
      "metadata": {
        "id": "J2xHzjeWAuoc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yearly Seasonality and Trends\n",
        "\n",
        "We now examine yearly patterns using monthly averages and rolling means.\n"
      ],
      "metadata": {
        "id": "JKaJ2uDcBBjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "period = slice(\"2001\", \"2019\")\n",
        "df_monthly = df.select_dtypes(include=\"number\").resample(\"ME\").mean()\n",
        "rolling_average_12_months = df_monthly.loc[period].rolling(window=12).mean()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "df_monthly[period].plot(ax=ax, marker=\".\")\n",
        "rolling_average_12_months.plot(ax=ax, grid=True, legend=False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QPptAIxsBCRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yearly seasonality and long-term trends are visible, especially for rail ridership.\n",
        "\n",
        "Differencing removes both seasonality and trend, producing a more stationary series.\n"
      ],
      "metadata": {
        "id": "r-BezUYHBDV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Yearly seasonality and long-term trends are visible, especially for rail ridership.\n",
        "\n",
        "Differencing removes both seasonality and trend, producing a more stationary series.\n"
      ],
      "metadata": {
        "id": "H5ar7LGjBEK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The ARMA / ARIMA / SARIMA Family\n",
        "\n",
        "- **ARMA**: weighted sum of past values and past forecast errors\n",
        "- **ARIMA**: adds differencing to handle non-stationarity\n",
        "- **SARIMA**: models seasonal patterns explicitly\n",
        "\n",
        "These models assume stationarity, which differencing helps achieve.\n"
      ],
      "metadata": {
        "id": "yPgny01oBFax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitting a SARIMA Model\n",
        "\n",
        "We forecast rail ridership using a SARIMA model with weekly seasonality.\n"
      ],
      "metadata": {
        "id": "LGGdvrNIBGxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "origin, today = \"2019-01-01\", \"2019-05-31\"\n",
        "rail_series = df.loc[origin:today][\"rail\"].asfreq(\"D\")\n",
        "\n",
        "model = ARIMA(\n",
        "    rail_series,\n",
        "    order=(1, 0, 0),\n",
        "    seasonal_order=(0, 1, 1, 7)\n",
        ")\n",
        "model = model.fit()\n",
        "model.forecast()\n"
      ],
      "metadata": {
        "id": "6mne7wJvBI_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the single-day forecast may perform poorly, retraining daily and averaging performance yields much better results.\n"
      ],
      "metadata": {
        "id": "ayhdoM55BJ98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "origin, start_date, end_date = \"2019-01-01\", \"2019-03-01\", \"2019-05-31\"\n",
        "time_period = pd.date_range(start_date, end_date)\n",
        "rail_series = df.loc[origin:end_date][\"rail\"].asfreq(\"D\")\n",
        "\n",
        "y_preds = []\n",
        "for today in time_period.shift(-1):\n",
        "    model = ARIMA(\n",
        "        rail_series[origin:today],\n",
        "        order=(1, 0, 0),\n",
        "        seasonal_order=(0, 1, 1, 7)\n",
        "    ).fit()\n",
        "    y_preds.append(model.forecast().iloc[0])\n",
        "\n",
        "y_preds = pd.Series(y_preds, index=time_period)\n",
        "(y_preds - rail_series[time_period]).abs().mean()\n"
      ],
      "metadata": {
        "id": "Y52bYCLRBKv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Data for Machine Learning Models\n",
        "\n",
        "We now prepare sliding windows of historical data to train ML models.\n",
        "Each input window contains 56 days, and the target is the following day.\n"
      ],
      "metadata": {
        "id": "WKYBLradBLvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class TimeSeriesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, series, window_length):\n",
        "        self.series = series\n",
        "        self.window_length = window_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.series) - self.window_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        end = idx + self.window_length\n",
        "        window = self.series[idx:end]\n",
        "        target = self.series[end]\n",
        "        return window, target\n"
      ],
      "metadata": {
        "id": "R6P1tZ2TBM6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the dataset on a toy example confirms correctness.\n"
      ],
      "metadata": {
        "id": "sKAvjPpIBN49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_series = torch.tensor([[0], [1], [2], [3], [4], [5]])\n",
        "my_dataset = TimeSeriesDataset(my_series, window_length=3)\n",
        "\n",
        "for window, target in my_dataset:\n",
        "    print(\"Window:\", window, \"Target:\", target)\n"
      ],
      "metadata": {
        "id": "LHwcnsfABOp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From Here On\n",
        "\n",
        "- Linear regression baselines\n",
        "- Simple RNNs\n",
        "- Deep RNNs\n",
        "- Multivariate time series\n",
        "- Multi-step forecasting\n",
        "- Sequence-to-sequence RNNs\n",
        "\n",
        "All follow the same **windowed dataset + PyTorch training loop** pattern.\n"
      ],
      "metadata": {
        "id": "fLDVqg0BBPjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Long Sequences\n",
        "\n",
        "To train an RNN on long sequences, we must run it over many time steps, making the unrolled RNN a very deep network. Like any deep neural network, it may suffer from the unstable gradients problem, and it may also gradually forget the first inputs in the sequence.\n",
        "\n",
        "We will look at:\n",
        "1. The unstable gradients problem\n",
        "2. The short-term memory problem\n"
      ],
      "metadata": {
        "id": "Gcac12WkB70R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fighting the Unstable Gradients Problem\n",
        "\n",
        "Many techniques used for deep feedforward networks also apply to RNNs:\n",
        "- Good parameter initialization\n",
        "- Faster optimizers\n",
        "- Dropout\n",
        "\n",
        "However, nonsaturating activation functions like ReLU may actually make RNNs more unstable. Since the same weights are reused at each time step, small increases in outputs can compound over time and explode.\n",
        "\n",
        "Using a smaller learning rate or a saturating activation function like `tanh` helps reduce this risk.\n"
      ],
      "metadata": {
        "id": "A9eJHF97CQfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Explosion and Clipping\n",
        "\n",
        "Gradients can also explode during backpropagation through time.  \n",
        "If training is unstable, monitor gradient norms and consider **gradient clipping**.\n"
      ],
      "metadata": {
        "id": "W6UGjtjsCRlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Normalization and Layer Normalization in RNNs\n",
        "\n",
        "Batch normalization cannot be used efficiently across time steps in RNNs. It can only be applied between recurrent layers, not within them.\n",
        "\n",
        "Layer normalization tends to work better inside recurrent layers. It is usually applied just before the activation function at each time step.\n"
      ],
      "metadata": {
        "id": "jRkYc0sDCSp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "self.memory_cell = nn.Sequential(\n",
        "    nn.Linear(input_size + hidden_size, hidden_size),\n",
        "    nn.LayerNorm(hidden_size),\n",
        "    nn.Tanh()\n",
        ")\n"
      ],
      "metadata": {
        "id": "pd5irgLsCTw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer normalization does not always help, but it is more effective in gated RNNs such as LSTM and GRU, especially when seasonality or trends have been removed from the data.\n"
      ],
      "metadata": {
        "id": "09anBPFiCU0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tackling the Short-Term Memory Problem\n",
        "\n",
        "As information passes through many time steps, an RNN gradually forgets earlier inputs.  \n",
        "This makes learning long-term dependencies difficult.\n",
        "\n",
        "To solve this, **long-term memory cells** were introduced.\n"
      ],
      "metadata": {
        "id": "oB1FinUyCVxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Cells\n",
        "\n",
        "The Long Short-Term Memory (LSTM) cell was introduced in 1997.  \n",
        "It splits the state into:\n",
        "- Short-term state: h(t)\n",
        "- Long-term state: c(t)\n",
        "\n",
        "This allows the model to decide what to store, forget, and retrieve.\n"
      ],
      "metadata": {
        "id": "g-4ksOwtCWve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gates in an LSTM Cell\n",
        "\n",
        "An LSTM cell contains four fully connected layers:\n",
        "- g(t): candidate memory\n",
        "- i(t): input gate\n",
        "- f(t): forget gate\n",
        "- o(t): output gate\n",
        "\n",
        "Each gate uses a sigmoid activation to output values in [0, 1], controlling information flow.\n"
      ],
      "metadata": {
        "id": "4pZ416awCYKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At each time step:\n",
        "- The forget gate decides what to erase from long-term memory\n",
        "- The input gate decides what to store\n",
        "- The output gate decides what to reveal as output\n",
        "\n",
        "This mechanism allows LSTMs to capture long-term patterns.\n"
      ],
      "metadata": {
        "id": "SgwOsrcQCZgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing an LSTM Manually with LSTMCell\n"
      ],
      "metadata": {
        "id": "VhftSngjCa_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LstmModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.memory_cell = nn.LSTMCell(input_size, hidden_size)\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        batch_size, window_length, dimensionality = X.shape\n",
        "        X_time_first = X.transpose(0, 1)\n",
        "        H = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "        C = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "        for X_t in X_time_first:\n",
        "            H, C = self.memory_cell(X_t, (H, C))\n",
        "        return self.output(H)\n"
      ],
      "metadata": {
        "id": "Puo-U4XQCcPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is similar to a simple RNN, except:\n",
        "- The hidden state is split into H (short-term) and C (long-term)\n",
        "- An `nn.LSTMCell` is used instead of a simple linear layer\n"
      ],
      "metadata": {
        "id": "z32p6zOuCdcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU Cells\n",
        "\n",
        "The Gated Recurrent Unit (GRU) is a simplified version of the LSTM:\n",
        "- Uses a single state vector h(t)\n",
        "- Merges forget and input gates into a single update gate z(t)\n",
        "- Removes the output gate\n"
      ],
      "metadata": {
        "id": "1cbMq1InCedN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRUs often perform as well as LSTMs while being faster and simpler.\n",
        "PyTorch provides:\n",
        "- `nn.GRU`\n",
        "- `nn.GRUCell`\n"
      ],
      "metadata": {
        "id": "4mwdS7YICgFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using 1D Convolutional Layers with Sequences\n",
        "\n",
        "1D convolutional layers slide kernels across sequences instead of images.\n",
        "They can:\n",
        "- Detect short-term patterns\n",
        "- Reduce sequence length\n",
        "- Help RNNs capture longer dependencies\n"
      ],
      "metadata": {
        "id": "-KO96egGChzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ nn.Conv1d expects input shape:\n",
        "[batch_size, features, sequence_length]\n",
        "\n",
        "So we must permute dimensions before and after the convolution.\n"
      ],
      "metadata": {
        "id": "Rga3sRfSCjWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownsamplingModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(input_size, hidden_size, kernel_size=4, stride=2)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        Z = X.permute(0, 2, 1)\n",
        "        Z = self.conv(Z)\n",
        "        Z = Z.permute(0, 2, 1)\n",
        "        Z = torch.relu(Z)\n",
        "        Z, _ = self.gru(Z)\n",
        "        return self.linear(Z)\n"
      ],
      "metadata": {
        "id": "nvLLK6v2CreQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The convolution downsamples the sequence, allowing the GRU to model longer patterns more efficiently.\n"
      ],
      "metadata": {
        "id": "5xggigODGLcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownsampledDataset(Seq2SeqDataset):\n",
        "    def __getitem__(self, idx):\n",
        "        window, target = super().__getitem__(idx)\n",
        "        return window, target[3::2]\n"
      ],
      "metadata": {
        "id": "N_AEYx1uGPQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WaveNet\n",
        "\n",
        "WaveNet uses stacked 1D convolutions with exponentially increasing dilation rates:\n",
        "1, 2, 4, 8, …\n",
        "\n",
        "This allows the network to capture extremely long-term dependencies efficiently.\n"
      ],
      "metadata": {
        "id": "Ea02nzXvGQSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Causal Convolutions\n",
        "\n",
        "Causal convolutions pad inputs on the left only, ensuring the model never looks into the future.\n"
      ],
      "metadata": {
        "id": "4WJbLCByGRK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CausalConv1d(nn.Conv1d):\n",
        "    def forward(self, X):\n",
        "        padding = (self.kernel_size[0] - 1) * self.dilation[0]\n",
        "        X = F.pad(X, (padding, 0))\n",
        "        return super().forward(X)\n"
      ],
      "metadata": {
        "id": "EEfjOpNUGSRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WaveNet Model Implementation\n"
      ],
      "metadata": {
        "id": "V_QTL46iGTFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WavenetModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for dilation in (1, 2, 4, 8) * 2:\n",
        "            conv = CausalConv1d(\n",
        "                input_size, hidden_size, kernel_size=2, dilation=dilation\n",
        "            )\n",
        "            layers += [conv, nn.ReLU()]\n",
        "            input_size = hidden_size\n",
        "        self.convs = nn.Sequential(*layers)\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        Z = X.permute(0, 2, 1)\n",
        "        Z = self.convs(Z)\n",
        "        Z = Z.permute(0, 2, 1)\n",
        "        return self.output(Z)\n"
      ],
      "metadata": {
        "id": "UsI3Pl4jGUXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to causal padding, the output sequence length matches the input length.\n",
        "No cropping or downsampling of targets is required.\n"
      ],
      "metadata": {
        "id": "rD5F94jQGVhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Notes\n",
        "\n",
        "- LSTM, GRU, CNN-RNN hybrids, and WaveNet can all model long sequences\n",
        "- Performance depends heavily on data and task\n",
        "- Models trained on past data may fail if patterns change (e.g., COVID-19)\n",
        "\n",
        "Always validate on recent data and monitor production performance.\n"
      ],
      "metadata": {
        "id": "12RBJWUOGWax"
      }
    }
  ]
}