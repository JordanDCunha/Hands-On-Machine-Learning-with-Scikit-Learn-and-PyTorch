{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhtYCvgVXJ82YNNEscjTVf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanDCunha/Hands-On-Machine-Learning-with-Scikit-Learn-and-PyTorch/blob/main/Chapter12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Architecture of the Visual Cortex\n",
        "\n",
        "David H. Hubel and Torsten Wiesel performed a series of experiments on cats in\n",
        "1958 and 1959 (and a few years later on monkeys), giving crucial insights into\n",
        "the structure of the visual cortex. The authors received the Nobel Prize in\n",
        "Physiology or Medicine in 1981 for this work.\n",
        "\n",
        "In particular, they showed that many neurons in the visual cortex have a small\n",
        "**local receptive field**, meaning they react only to visual stimuli located in\n",
        "a limited region of the visual field (see Figure 12-1, in which the local\n",
        "receptive fields of five neurons are represented by dashed circles). The\n",
        "receptive fields of different neurons may overlap, and together they tile the\n",
        "whole visual field.\n",
        "\n",
        "**Figure 12-1.** Biological neurons in the visual cortex respond to specific\n",
        "patterns in small regions of the visual field called receptive fields; as the\n",
        "visual signal makes its way through consecutive brain modules, neurons respond\n",
        "to more complex patterns in larger receptive fields.\n",
        "\n",
        "Moreover, the authors showed that some neurons react only to images of\n",
        "horizontal lines, while others react only to lines with different orientations\n",
        "(two neurons may have the same receptive field but react to different line\n",
        "orientations). They also noticed that some neurons have larger receptive\n",
        "fields, and they react to more complex patterns that are combinations of the\n",
        "lower-level patterns.\n",
        "\n",
        "These observations led to the idea that higher-level neurons are based on the\n",
        "outputs of neighboring lower-level neurons. In Figure 12-1, each neuron is\n",
        "connected only to nearby neurons from the previous layer. This powerful\n",
        "architecture is able to detect all sorts of complex patterns in any area of the\n",
        "visual field.\n",
        "\n",
        "These studies of the visual cortex inspired the **neocognitron**, introduced in\n",
        "1980, which gradually evolved into what we now call **convolutional neural\n",
        "networks (CNNs)**. An important milestone was a 1998 paper by Yann LeCun et al.\n",
        "that introduced the famous **LeNet-5** architecture, which became widely used by\n",
        "banks to recognize handwritten digits on checks.\n",
        "\n",
        "This architecture has some building blocks that you already know, such as fully\n",
        "connected layers and sigmoid activation functions, but it also introduces two\n",
        "new building blocks:\n",
        "\n",
        "- **Convolutional layers**\n",
        "- **Pooling layers**\n",
        "\n",
        "We will examine these next.\n",
        "\n",
        "---\n",
        "\n",
        "## Note\n",
        "\n",
        "Why not simply use a deep neural network with fully connected layers for image\n",
        "recognition tasks?\n",
        "\n",
        "Although this works fine for small images (e.g., Fashion MNIST), it breaks down\n",
        "for larger images because of the huge number of parameters it requires. For\n",
        "example, a 100 × 100 pixel image has 10,000 pixels. If the first layer has just\n",
        "1,000 neurons (which already severely restricts the amount of information\n",
        "transmitted to the next layer), this results in **10 million connections**—and\n",
        "that is only the first layer.\n",
        "\n",
        "CNNs solve this problem using **partially connected layers** and **weight\n",
        "sharing**.\n"
      ],
      "metadata": {
        "id": "a6O9dfX-z_GS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Layers\n",
        "\n",
        "The most important building block of a CNN is the convolutional layer. Neurons in a convolutional layer are not connected to every single pixel in the input image, but only to pixels in their receptive fields.\n",
        "\n",
        "Each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This allows the network to focus on low-level features first, then combine them into higher-level features in deeper layers.\n",
        "\n",
        "This hierarchical structure is well-suited for real-world images, which are composed of complex patterns and objects.\n"
      ],
      "metadata": {
        "id": "q8_14l40z9sa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Receptive Fields\n",
        "\n",
        "All the multilayer neural networks we looked at previously had layers composed of a long line of neurons, requiring images to be flattened into 1D vectors.\n",
        "\n",
        "In a CNN, layers are represented in 2D, making it easier to preserve spatial relationships.\n",
        "\n",
        "A neuron at row *i*, column *j* is connected to inputs in rows *i* to *i + fh − 1* and columns *j* to *j + fw − 1*, where *fh* and *fw* are the receptive field height and width.\n"
      ],
      "metadata": {
        "id": "BHLb43MB1oyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero Padding and Stride\n",
        "\n",
        "To keep the output size the same as the input size, zeros can be added around the input image — this is called **zero padding**.\n",
        "\n",
        "Stride defines how far the receptive field moves between applications. A larger stride reduces the spatial dimensions and computational cost.\n",
        "\n",
        "For stride *(sh, sw)*, the neuron at position *(i, j)* connects to input rows *i × sh* to *i × sh + fh − 1* and columns *j × sw* to *j × sw + fw − 1*.\n"
      ],
      "metadata": {
        "id": "BJuEzlRg1qiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filters (Kernels)\n",
        "\n",
        "A neuron's weights can be visualized as a small image called a **filter** or **kernel**.\n",
        "\n",
        "For example:\n",
        "- A vertical-line filter highlights vertical edges\n",
        "- A horizontal-line filter highlights horizontal edges\n",
        "\n",
        "Each filter produces a **feature map**, emphasizing areas where the filter activates strongly.\n",
        "\n",
        "Filters are learned automatically during training — you never define them manually.\n"
      ],
      "metadata": {
        "id": "CEDuFLgO1vSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacking Multiple Feature Maps\n",
        "\n",
        "A convolutional layer contains multiple filters, producing multiple feature maps.\n",
        "\n",
        "Each feature map:\n",
        "- Has one neuron per pixel\n",
        "- Shares the same weights across all locations\n",
        "- Uses a unique filter and bias\n",
        "\n",
        "This parameter sharing drastically reduces model size and enables translation invariance.\n"
      ],
      "metadata": {
        "id": "e3CzbbUO1wDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multichannel Inputs\n",
        "\n",
        "Input images usually have multiple channels:\n",
        "- RGB images → 3 channels\n",
        "- Grayscale → 1 channel\n",
        "- Satellite images → many channels\n",
        "\n",
        "Each convolutional filter spans **all input channels**, not just one.\n"
      ],
      "metadata": {
        "id": "1JWtv6eG1w4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical Definition (Equation 12-1)\n",
        "\n",
        "The output of a neuron is computed as a weighted sum of all values in its receptive field across all input channels, plus a bias term.\n",
        "\n",
        "Variables:\n",
        "- `z_{i,j,k}` → output at row i, column j, feature map k\n",
        "- `x` → input values\n",
        "- `w` → kernel weights\n",
        "- `b_k` → bias for feature map k\n",
        "- `sh, sw` → strides\n",
        "- `fh, fw` → filter dimensions\n"
      ],
      "metadata": {
        "id": "I49hdIcg1x10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Convolutional Layers with PyTorch\n"
      ],
      "metadata": {
        "id": "74TgPKVZ11Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.datasets import load_sample_images\n",
        "\n",
        "# Load sample images\n",
        "sample_images = np.stack(load_sample_images()[\"images\"])\n",
        "sample_images = torch.tensor(sample_images, dtype=torch.float32) / 255\n"
      ],
      "metadata": {
        "id": "kVICsEfC2HAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect shape\n",
        "sample_images.shape\n"
      ],
      "metadata": {
        "id": "FkB7U3Gz2LvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tensor has shape:\n",
        "[batch_size, height, width, channels]\n",
        "\n",
        "PyTorch expects:\n",
        "[batch_size, channels, height, width]\n"
      ],
      "metadata": {
        "id": "HTg6545S137a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder dimensions\n",
        "sample_images_permuted = sample_images.permute(0, 3, 1, 2)\n",
        "sample_images_permuted.shape\n"
      ],
      "metadata": {
        "id": "Wo4kWLJo2R4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms.v2 as T\n",
        "\n",
        "# Center crop images\n",
        "cropped_images = T.CenterCrop((70, 120))(sample_images_permuted)\n",
        "cropped_images.shape\n"
      ],
      "metadata": {
        "id": "VOdZBzJ22U8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Convolutional Layer\n"
      ],
      "metadata": {
        "id": "12qkpb4R17eS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lR0gaxDxqnX"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "conv_layer = nn.Conv2d(\n",
        "    in_channels=3,\n",
        "    out_channels=32,\n",
        "    kernel_size=7\n",
        ")\n",
        "\n",
        "fmaps = conv_layer(cropped_images)\n",
        "fmaps.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output Shape Explanation\n",
        "\n",
        "- 32 output channels → 32 feature maps\n",
        "- Height and width shrink due to no padding\n",
        "- Kernel size 7 removes 6 pixels total (3 per side)\n"
      ],
      "metadata": {
        "id": "YhMIOPb927_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding Options\n",
        "\n",
        "- `padding=0` or `\"valid\"` → no padding\n",
        "- `padding=\"same\"` → output size equals input size\n"
      ],
      "metadata": {
        "id": "CcUZpzX128vV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer = nn.Conv2d(\n",
        "    in_channels=3,\n",
        "    out_channels=32,\n",
        "    kernel_size=7,\n",
        "    padding=\"same\"\n",
        ")\n",
        "\n",
        "fmaps = conv_layer(cropped_images)\n",
        "fmaps.shape\n"
      ],
      "metadata": {
        "id": "CtaXwG6B29sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stride Effects\n",
        "\n",
        "A stride greater than 1 reduces spatial dimensions.\n",
        "\n",
        "Example:\n",
        "- Input: 70 × 120\n",
        "- Stride: 2\n",
        "- Output: 35 × 60\n",
        "\n",
        "Using large padding with large stride is discouraged.\n"
      ],
      "metadata": {
        "id": "p-6Z52h22-dD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting Layer Parameters\n"
      ],
      "metadata": {
        "id": "VyK1FVbM2_VA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer.weight.shape\n"
      ],
      "metadata": {
        "id": "6gIDSt4n4LKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer.bias.shape\n"
      ],
      "metadata": {
        "id": "BozmZ_HN4L8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter Shapes\n",
        "\n",
        "- Weights: [out_channels, in_channels, kernel_height, kernel_width]\n",
        "- Biases: [out_channels]\n",
        "\n",
        "Image size does NOT affect parameter size.\n"
      ],
      "metadata": {
        "id": "NHM8BsXf4M8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Functions and Initialization\n",
        "\n",
        "Convolutional layers are linear — activation functions are required.\n",
        "\n",
        "- Use **ReLU** with **He initialization**\n",
        "- Biases are typically initialized to zero\n",
        "\n",
        "Hyperparameters include:\n",
        "- Number of filters\n",
        "- Kernel size\n",
        "- Padding\n",
        "- Stride\n",
        "- Activation function\n"
      ],
      "metadata": {
        "id": "5GnU86kC4N4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pooling Layers\n",
        "\n",
        "Once you understand how convolutional layers work, the pooling layers are quite easy to grasp. Their goal is to subsample (i.e., shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters (thereby limiting the risk of overfitting).\n",
        "\n",
        "Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has **no weights or biases**; all it does is aggregate the inputs using an aggregation function such as the **max** or **mean**.\n",
        "\n",
        "## Max Pooling\n",
        "\n",
        "Figure 12-9 shows a **max pooling layer**, which is the most common type of pooling layer. In this example, we use a **2 × 2 pooling kernel** with a **stride of 2** and **no padding**. Only the maximum input value in each receptive field is propagated to the next layer, while the other inputs are dropped.\n",
        "\n",
        "For example, in the lower-left receptive field in Figure 12-9, the input values are 1, 5, 3, and 2, so only the maximum value, **5**, is kept. Because of the stride of 2, the output image has **half the height and half the width** of the input image (rounded down since no padding is used).\n",
        "\n",
        "> **NOTE**  \n",
        "> A pooling layer typically operates independently on each input channel, so the output depth (number of channels) remains the same as the input depth.\n",
        "\n",
        "## Translation Invariance\n",
        "\n",
        "In addition to reducing computation and memory usage, max pooling introduces a degree of **invariance to small translations**. Figure 12-10 illustrates this effect using three images (A, B, C) processed through a max pooling layer with a 2 × 2 kernel and stride 2.\n",
        "\n",
        "Images B and C are shifted versions of image A. The outputs for images A and B are identical, demonstrating **translation invariance**. For image C, the output is shifted by one pixel, showing partial invariance. By inserting max pooling layers periodically, CNNs can gain translation invariance at larger scales.\n",
        "\n",
        "Max pooling also provides limited **rotational** and **scale invariance**, which can be useful for tasks like image classification where exact spatial alignment is not critical.\n",
        "\n",
        "## Limitations of Max Pooling\n",
        "\n",
        "Max pooling is highly **destructive**. Even with a small 2 × 2 kernel and stride of 2, the output area is reduced by a factor of four, discarding **75% of the input values**.\n",
        "\n",
        "In some tasks, invariance is undesirable. For example, in **semantic segmentation**, where each pixel must be classified, the output should shift exactly when the input shifts. This property is called **equivariance**, not invariance: small changes in the input should result in corresponding small changes in the output.\n"
      ],
      "metadata": {
        "id": "ACh7GSAX4O48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Pooling Layers with PyTorch\n",
        "\n",
        "Pooling layers are used to subsample (shrink) feature maps in convolutional neural networks. This helps reduce computational cost, memory usage, and overfitting.\n",
        "\n",
        "The most common pooling operation is **max pooling**, which keeps only the maximum value in each local receptive field.\n"
      ],
      "metadata": {
        "id": "XpsXgNYA8R91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_pool = nn.MaxPool2d(kernel_size=2)\n"
      ],
      "metadata": {
        "id": "mZJN2T-18Sx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average Pooling\n",
        "\n",
        "To create an **average pooling** layer, we use `nn.AvgPool2d`.  \n",
        "It behaves exactly like max pooling, except it computes the **mean** instead of the maximum.\n",
        "\n",
        "Although average pooling loses less information, **max pooling is more popular** because it preserves the strongest features, introduces stronger translation invariance, and is slightly more efficient.\n"
      ],
      "metadata": {
        "id": "Stf1JSSb8Tcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_pool = nn.AvgPool2d(kernel_size=2)\n"
      ],
      "metadata": {
        "id": "RKfoBXPg8aDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Depthwise Pooling\n",
        "\n",
        "Pooling can also be applied along the **depth (channel) dimension** instead of the spatial dimensions.  \n",
        "This can help a CNN become invariant to transformations such as rotation, thickness, brightness, or color.\n",
        "\n",
        "PyTorch does not include a built-in depthwise max pooling layer, but we can implement one using `max_pool1d`.\n"
      ],
      "metadata": {
        "id": "jxYmwI6H8br9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class DepthPool(nn.Module):\n",
        "    def __init__(self, kernel_size, stride=None, padding=0):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride if stride is not None else kernel_size\n",
        "        self.padding = padding\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch, channels, height, width = inputs.shape\n",
        "        Z = inputs.view(batch, channels, height * width)  # merge spatial dims\n",
        "        Z = Z.permute(0, 2, 1)  # swap spatial and channel dims\n",
        "        Z = F.max_pool1d(\n",
        "            Z,\n",
        "            kernel_size=self.kernel_size,\n",
        "            stride=self.stride,\n",
        "            padding=self.padding\n",
        "        )\n",
        "        Z = Z.permute(0, 2, 1)  # swap back\n",
        "        return Z.view(batch, -1, height, width)  # restore spatial dims\n"
      ],
      "metadata": {
        "id": "qh_0Q7Xh8ckL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Shape Transformation\n",
        "\n",
        "Assume the input has shape `[2, 32, 70, 120]`:\n",
        "- 2 images\n",
        "- 32 channels\n",
        "- spatial size 70 × 120\n",
        "\n",
        "Using `kernel_size = 4`, stride = 4, and no padding:\n",
        "\n",
        "1. Merge spatial dimensions → `[2, 32, 8400]`\n",
        "2. Permute dimensions → `[2, 8400, 32]`\n",
        "3. Apply depthwise max pooling → `[2, 8400, 8]`\n",
        "4. Permute back → `[2, 8, 8400]`\n",
        "5. Restore spatial dimensions → `[2, 8, 50, 100]`\n"
      ],
      "metadata": {
        "id": "7AaVAcXM9YJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global Average Pooling\n",
        "\n",
        "Global average pooling computes the mean of each entire feature map, producing **one value per channel per image**.\n",
        "\n",
        "This is often used just before the output layer to reduce parameters and overfitting.\n"
      ],
      "metadata": {
        "id": "N7c6DXIT9ZhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\n"
      ],
      "metadata": {
        "id": "tRtmYWsC9aU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same result can also be achieved using `torch.mean` directly:\n"
      ],
      "metadata": {
        "id": "h8XFQpaC9bEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = cropped_images.mean(dim=(2, 3), keepdim=True)\n"
      ],
      "metadata": {
        "id": "4la9MDWy9b6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You now know all the key pooling layers used in modern convolutional neural networks.  \n",
        "Next, these components will be assembled into full CNN architectures.\n"
      ],
      "metadata": {
        "id": "ANgjGCaf9cxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Architectures\n",
        "\n",
        "Typical CNN architectures stack a few convolutional layers (each generally followed by a ReLU layer), then a pooling layer, then another few convolutional layers (+ReLU), then another pooling layer, and so on.\n",
        "\n",
        "As the image progresses through the network, its spatial resolution decreases, but the number of feature maps (depth) typically increases. At the top of the stack, a regular feedforward neural network is added, composed of fully connected layers, and the final layer outputs the prediction (e.g., class probabilities using softmax).\n"
      ],
      "metadata": {
        "id": "thUc4TnL-s3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Design Tip\n",
        "\n",
        "Instead of using a convolutional layer with a 5 × 5 kernel, it is generally preferable to stack two layers with 3 × 3 kernels. This uses fewer parameters, requires fewer computations, and usually performs better.\n",
        "\n",
        "An exception is the first convolutional layer, which can use a large kernel (e.g., 5 × 5 or 7 × 7) with a stride of 2 or more to reduce spatial dimensions early without losing much information.\n"
      ],
      "metadata": {
        "id": "SJW5OBaQ-xQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "metadata": {
        "id": "cm3by3jv-yRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Basic CNN for Fashion MNIST\n",
        "\n",
        "Below is a CNN implementation suitable for the Fashion MNIST dataset. It follows the common pattern:\n",
        "- Convolution + ReLU\n",
        "- Pooling\n",
        "- Repeat\n",
        "- Fully connected layers at the top\n"
      ],
      "metadata": {
        "id": "JvDoSg4z-zHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DefaultConv2d = partial(nn.Conv2d, kernel_size=3, padding=\"same\")\n",
        "\n",
        "model = nn.Sequential(\n",
        "    DefaultConv2d(in_channels=1, out_channels=64, kernel_size=7), nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "    DefaultConv2d(in_channels=64, out_channels=128), nn.ReLU(),\n",
        "    DefaultConv2d(in_channels=128, out_channels=128), nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "    DefaultConv2d(in_channels=128, out_channels=256), nn.ReLU(),\n",
        "    DefaultConv2d(in_channels=256, out_channels=256), nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(in_features=2304, out_features=128), nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(in_features=128, out_features=64), nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(in_features=64, out_features=10),\n",
        ")\n"
      ],
      "metadata": {
        "id": "xB6zL5r8-0Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation\n",
        "\n",
        "- `DefaultConv2d` is defined using `functools.partial` to avoid repeating kernel size and padding.\n",
        "- The number of filters doubles after each pooling layer.\n",
        "- Max pooling reduces spatial dimensions by a factor of 2.\n",
        "- Dropout layers reduce overfitting.\n",
        "- The output layer has 10 units (one per class).\n",
        "- Softmax is omitted because `nn.CrossEntropyLoss` expects logits.\n"
      ],
      "metadata": {
        "id": "YreAKe7p-1ZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why 2,304 Input Features?\n",
        "\n",
        "Fashion MNIST images start at 28 × 28 pixels.\n",
        "Pooling reduces them to:\n",
        "- 14 × 14\n",
        "- 7 × 7\n",
        "- 3 × 3\n",
        "\n",
        "With 256 feature maps:\n",
        "256 × 3 × 3 = 2,304 input features.\n"
      ],
      "metadata": {
        "id": "X4rpmBoZ-2SV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classical CNN Architectures\n",
        "\n",
        "Over time, CNN architectures evolved rapidly, especially through competitions like ImageNet (ILSVRC). The top-5 error rate dropped from over 26% to under 2.3% in just six years.\n",
        "\n",
        "We now review several landmark architectures.\n"
      ],
      "metadata": {
        "id": "KmbVPxtJ-3af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LeNet-5 (1998)\n",
        "\n",
        "LeNet-5 was created by Yann LeCun for handwritten digit recognition (MNIST).\n",
        "\n",
        "It consists of:\n",
        "- Convolutional layers\n",
        "- Average pooling layers\n",
        "- Fully connected layers\n",
        "- tanh activations\n",
        "\n",
        "Today, ReLU and softmax would typically be used instead.\n"
      ],
      "metadata": {
        "id": "QiaIjRSv-8d5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AlexNet (2012)\n",
        "\n",
        "AlexNet won the 2012 ImageNet challenge with a top-5 error rate of 17%.\n",
        "\n",
        "Key contributions:\n",
        "- Much deeper than LeNet-5\n",
        "- Stacked convolutional layers\n",
        "- ReLU activations\n",
        "- Dropout for regularization\n",
        "- Data augmentation\n",
        "\n",
        "It popularized deep CNNs.\n"
      ],
      "metadata": {
        "id": "MPDbZT6--_X0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation\n",
        "\n",
        "Data augmentation increases training data by generating realistic variants:\n",
        "- Shifts\n",
        "- Rotations\n",
        "- Resizing\n",
        "- Color changes\n",
        "- Horizontal flips\n",
        "\n",
        "This reduces overfitting and improves generalization.\n"
      ],
      "metadata": {
        "id": "91NWy8A8_AX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GoogLeNet (Inception, 2014)\n",
        "\n",
        "GoogLeNet introduced **inception modules**, allowing the network to:\n",
        "- Capture patterns at multiple scales\n",
        "- Use far fewer parameters than AlexNet\n",
        "\n",
        "It won the 2014 ImageNet challenge with under 7% top-5 error.\n"
      ],
      "metadata": {
        "id": "wlSOvm_U_BaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inception Modules\n",
        "\n",
        "An inception module runs several layers in parallel:\n",
        "- 1 × 1 convolutions\n",
        "- 3 × 3 convolutions\n",
        "- 5 × 5 convolutions\n",
        "- Max pooling\n",
        "\n",
        "All outputs are concatenated along the depth dimension.\n"
      ],
      "metadata": {
        "id": "9SgqWBbg_CQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet (2015)\n",
        "\n",
        "ResNet introduced **skip connections**, enabling very deep networks (up to 152 layers).\n",
        "\n",
        "Instead of learning h(x), residual units learn:\n",
        "f(x) = h(x) − x\n",
        "\n",
        "This greatly improves gradient flow and training stability.\n"
      ],
      "metadata": {
        "id": "1IEOIRBQ_DP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residual Units\n",
        "\n",
        "Each residual unit:\n",
        "- Contains two or three convolutional layers\n",
        "- Preserves spatial dimensions\n",
        "- Adds the input to the output\n",
        "\n",
        "When dimensions change, a 1 × 1 convolution is used to match shapes.\n"
      ],
      "metadata": {
        "id": "KaB1EO_d_ENN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xception\n",
        "\n",
        "Xception replaces inception modules with **depthwise separable convolutions**.\n",
        "\n",
        "These separate:\n",
        "- Spatial feature extraction\n",
        "- Cross-channel feature extraction\n",
        "\n",
        "This reduces parameters and often improves performance.\n"
      ],
      "metadata": {
        "id": "eO6kMTsh_FEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super().__init__()\n",
        "        self.depthwise_conv = nn.Conv2d(\n",
        "            in_channels, in_channels, kernel_size,\n",
        "            stride=stride, padding=padding, groups=in_channels\n",
        "        )\n",
        "        self.pointwise_conv = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pointwise_conv(self.depthwise_conv(x))\n"
      ],
      "metadata": {
        "id": "K2ghMt0V_GBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SENet (2017)\n",
        "\n",
        "SENet introduces **Squeeze-and-Excitation (SE) blocks**, which:\n",
        "- Analyze feature map importance\n",
        "- Recalibrate channels dynamically\n",
        "- Improve performance with minimal overhead\n"
      ],
      "metadata": {
        "id": "AP33SwhB_Gyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SE Block Structure\n",
        "\n",
        "An SE block consists of:\n",
        "1. Global average pooling\n",
        "2. Dense layer (ReLU)\n",
        "3. Dense layer (sigmoid)\n",
        "4. Channel-wise scaling of feature maps\n"
      ],
      "metadata": {
        "id": "-lZpDljn_HpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Noteworthy Architectures\n",
        "\n",
        "- **VGGNet** – deep but simple, many parameters\n",
        "- **ResNeXt** – grouped convolutions\n",
        "- **DenseNet** – dense connections between layers\n",
        "- **MobileNet** – lightweight, mobile-friendly\n",
        "- **EfficientNet** – compound scaling (depth, width, resolution)\n",
        "- **ConvNeXt** – CNNs inspired by vision transformers\n"
      ],
      "metadata": {
        "id": "tfB75V_z_IiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU Memory: Training vs Inference\n",
        "\n",
        "During inference, activations can be freed layer by layer.\n",
        "During training, all activations must be stored for backpropagation.\n",
        "\n",
        "This makes training far more memory-intensive than inference.\n"
      ],
      "metadata": {
        "id": "5KKC-xF3_KfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memory Optimization Techniques\n",
        "\n",
        "- Reduce batch size\n",
        "- Use mixed precision (FP16)\n",
        "- Gradient accumulation\n",
        "- Activation checkpointing\n",
        "- Model parallelism\n",
        "- Reversible networks (RevNets)\n"
      ],
      "metadata": {
        "id": "9_17TFut_M2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.checkpoint import checkpoint\n"
      ],
      "metadata": {
        "id": "EUrrrwGW_N_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reversible Residual Networks (RevNets)\n",
        "\n",
        "RevNets avoid storing activations entirely by making each layer reversible.\n",
        "Inputs can be recomputed from outputs during backpropagation, saving memory.\n"
      ],
      "metadata": {
        "id": "QyKKN5Ox_O8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You now have a full overview of modern CNN architectures and design principles.\n",
        "\n",
        "Next, we’ll implement a popular CNN architecture from scratch using PyTorch.\n"
      ],
      "metadata": {
        "id": "A_GLROyK_kNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing a ResNet-34 CNN Using PyTorch\n",
        "\n",
        "Most CNN architectures described so far can be implemented pretty naturally using PyTorch (although generally you would load a pretrained network instead, as you will see).\n",
        "\n",
        "To illustrate the process, we will implement a **ResNet-34** from scratch using PyTorch.\n"
      ],
      "metadata": {
        "id": "OVJt7EoH_5-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Residual Unit\n",
        "\n",
        "We start by defining a **ResidualUnit** layer. This corresponds directly to the residual blocks used in ResNet architectures.\n",
        "\n",
        "Each residual unit consists of:\n",
        "- A main path with two convolutional layers\n",
        "- A skip (identity) connection\n",
        "- An elementwise addition followed by a ReLU activation\n"
      ],
      "metadata": {
        "id": "mVP4yfBj_6vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n"
      ],
      "metadata": {
        "id": "fCX90YxG_7mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualUnit(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        DefaultConv2d = partial(\n",
        "            nn.Conv2d, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "\n",
        "        self.main_layers = nn.Sequential(\n",
        "            DefaultConv2d(in_channels, out_channels, stride=stride),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            DefaultConv2d(out_channels, out_channels),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "\n",
        "        if stride > 1:\n",
        "            self.skip_connection = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_channels, out_channels,\n",
        "                    kernel_size=1, stride=stride, padding=0, bias=False\n",
        "                ),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "        else:\n",
        "            self.skip_connection = nn.Identity()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return F.relu(self.main_layers(inputs) + self.skip_connection(inputs))\n"
      ],
      "metadata": {
        "id": "2FwE3IZC_8fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residual Unit Explanation\n",
        "\n",
        "This implementation closely matches the standard ResNet residual block.\n",
        "\n",
        "- The **main layers** correspond to the right-hand path of the residual diagram.\n",
        "- The **skip connection** is either:\n",
        "  - A 1 × 1 convolution with stride > 1 when dimensions must change, or\n",
        "  - An identity mapping when dimensions stay the same.\n",
        "- In the forward pass, the outputs of both paths are added together and passed through a ReLU activation.\n"
      ],
      "metadata": {
        "id": "qxMSNvjA_9jv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the ResNet-34 Architecture\n",
        "\n",
        "Now that we have the ResidualUnit, we can assemble the full **ResNet-34** architecture.\n",
        "\n",
        "The network is essentially a large sequential stack:\n",
        "- Initial convolution and max pooling\n",
        "- Four groups of residual units\n",
        "- Global average pooling\n",
        "- A fully connected output layer\n"
      ],
      "metadata": {
        "id": "8WY7RA4x_-qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet34(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = [\n",
        "            nn.Conv2d(\n",
        "                in_channels=3, out_channels=64,\n",
        "                kernel_size=7, stride=2, padding=3, bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "        ]\n",
        "\n",
        "        prev_filters = 64\n",
        "\n",
        "        for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
        "            stride = 1 if filters == prev_filters else 2\n",
        "            layers.append(ResidualUnit(prev_filters, filters, stride=stride))\n",
        "            prev_filters = filters\n",
        "\n",
        "        layers += [\n",
        "            nn.AdaptiveAvgPool2d(output_size=1),\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(10),\n",
        "        ]\n",
        "\n",
        "        self.resnet = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.resnet(inputs)\n"
      ],
      "metadata": {
        "id": "Ef8-6Xut__qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture Breakdown\n",
        "\n",
        "- The first convolution uses a 7 × 7 kernel with stride 2, followed by batch normalization and max pooling.\n",
        "- Residual units are stacked in the following pattern:\n",
        "  - 3 blocks with 64 filters\n",
        "  - 4 blocks with 128 filters\n",
        "  - 6 blocks with 256 filters\n",
        "  - 3 blocks with 512 filters\n",
        "- When the number of filters increases, the stride is set to 2 to downsample the spatial dimensions.\n"
      ],
      "metadata": {
        "id": "ANHsShPYAA3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Layers\n",
        "\n",
        "- `AdaptiveAvgPool2d(output_size=1)` performs **global average pooling**, producing one value per feature map.\n",
        "- `Flatten()` converts the tensor into a vector.\n",
        "- `LazyLinear(10)` creates the final classification layer with 10 outputs (e.g., for CIFAR-10).\n"
      ],
      "metadata": {
        "id": "ZZeTaM58AB0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "In just about **45 lines of code**, we have implemented ResNet-34 from scratch.\n",
        "\n",
        "This highlights:\n",
        "- The elegance of the ResNet architecture\n",
        "- The expressiveness and flexibility of PyTorch\n",
        "\n",
        "Although implementing other CNN architectures would take more code, it would not be significantly harder. In practice, however, PyTorch’s **TorchVision** library provides many of these models preimplemented and pretrained, making them easy to use in real-world applications.\n"
      ],
      "metadata": {
        "id": "VZ7iJGGRACtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using TorchVision’s Pretrained Models\n",
        "\n",
        "In practice, you rarely need to implement standard CNN architectures like GoogLeNet, ResNet, or ConvNeXt manually. TorchVision provides many **pretrained models** that can be loaded with just a few lines of code.\n"
      ],
      "metadata": {
        "id": "GBw2EJVmAe7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tip: Other Sources of Pretrained Models\n",
        "\n",
        "- **TIMM** is a popular PyTorch-based library that offers a large collection of pretrained image classification models, along with utilities for data loading, augmentation, optimizers, and schedulers.\n",
        "- **Hugging Face Hub** is another excellent resource for pretrained models across many domains.\n",
        "\n",
        "Both libraries integrate well with PyTorch.\n"
      ],
      "metadata": {
        "id": "eolAQ7xeAf5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n"
      ],
      "metadata": {
        "id": "k-CU4TuNAgz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a Pretrained ConvNeXt Model\n",
        "\n",
        "TorchVision provides several ConvNeXt variants: tiny, small, base, and large.  \n",
        "The following example loads the **ConvNeXt Base** model pretrained on ImageNet.\n"
      ],
      "metadata": {
        "id": "BOOBpGVAAh7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torchvision.models.ConvNeXt_Base_Weights.IMAGENET1K_V1\n",
        "model = torchvision.models.convnext_base(weights=weights).to(device)\n"
      ],
      "metadata": {
        "id": "8M6XOyl9Ai6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code automatically downloads the pretrained weights (about 338 MB) from Torch Hub and caches them locally for future use.\n",
        "\n",
        "Some models have multiple versions of pretrained weights (for example, `IMAGENET1K_V2`). To explore available models and weights, you can use:\n",
        "\n",
        "- `torchvision.models.list_models()`\n",
        "- `torchvision.models.get_model_weights(\"convnext_base\")`\n",
        "\n",
        "You can also browse the official documentation on the TorchVision website.\n"
      ],
      "metadata": {
        "id": "JvSQWI1DAmrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the Input Images\n",
        "\n",
        "Before passing images to the model, they must be preprocessed exactly as expected during training.\n",
        "\n",
        "ConvNeXt models expect **224 × 224** pixel images. Instead of manually resizing and normalizing, it is best to use the transforms provided by the pretrained weights object.\n"
      ],
      "metadata": {
        "id": "LrBLB48tAodX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = weights.transforms()\n",
        "preprocessed_images = transforms(sample_images_permuted)\n"
      ],
      "metadata": {
        "id": "8QR4AqKuApTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These transforms:\n",
        "- Resize the images to the correct dimensions\n",
        "- Normalize pixel intensities using ImageNet’s channel-wise means and standard deviations\n",
        "\n",
        "This ensures compatibility with the pretrained model.\n"
      ],
      "metadata": {
        "id": "hZgnGBsCAqQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Inference\n",
        "\n",
        "Before making predictions:\n",
        "- Switch the model to **evaluation mode**\n",
        "- Disable gradient computation to save memory and computation\n"
      ],
      "metadata": {
        "id": "sKyF5leBArGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_logits = model(preprocessed_images.to(device))\n"
      ],
      "metadata": {
        "id": "ux_2An7LAr5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is a tensor of shape **[2, 1000]**, since ImageNet contains 1,000 classes.  \n",
        "Each row contains the logits for one image.\n"
      ],
      "metadata": {
        "id": "jklQHa_dAs33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicted Classes\n",
        "\n",
        "To obtain the predicted class for each image, we select the index of the maximum logit.\n"
      ],
      "metadata": {
        "id": "Sp7Z7nHjAuBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = torch.argmax(y_logits, dim=1)\n",
        "y_pred\n"
      ],
      "metadata": {
        "id": "akAg8au-Au0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting tensor contains the ImageNet class IDs predicted for each image.\n"
      ],
      "metadata": {
        "id": "FbsgbZaFAvoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mapping Class IDs to Human-Readable Labels\n",
        "\n",
        "The pretrained weights object contains metadata, including the ImageNet class names.\n"
      ],
      "metadata": {
        "id": "W5vkYQJtAxSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = weights.meta[\"categories\"]\n",
        "[class_names[class_id] for class_id in y_pred]\n"
      ],
      "metadata": {
        "id": "Hoh_NROvBJHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, the predictions may be:\n",
        "- **palace**\n",
        "- **daisy**\n",
        "\n",
        "These labels make sense even if the exact object is not present in ImageNet, as the model selects the closest available class.\n"
      ],
      "metadata": {
        "id": "BDZz4Kt5BLqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top-3 Predictions\n",
        "\n",
        "Instead of only looking at the top prediction, we can inspect the top three most likely classes using `topk()`.\n"
      ],
      "metadata": {
        "id": "j60oS3UpBNRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_top3_logits, y_top3_class_ids = y_logits.topk(k=3, dim=1)\n",
        "[[class_names[class_id] for class_id in top3] for top3 in y_top3_class_ids]\n"
      ],
      "metadata": {
        "id": "YZtnJuYbCSPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimated Probabilities\n",
        "\n",
        "The logits can be converted into probabilities using the softmax function.\n"
      ],
      "metadata": {
        "id": "M9QWQoZOCRlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_top3_logits.softmax(dim=1)\n"
      ],
      "metadata": {
        "id": "CaHoSVHsCUDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These probabilities indicate how confident the model is in each of its top predictions.\n"
      ],
      "metadata": {
        "id": "_p34bFsHCUwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "TorchVision makes it extremely easy to download and use pretrained models, and they perform very well out of the box on ImageNet classes.\n",
        "\n",
        "When your task involves different classes (such as specific flower species), pretrained models are still highly valuable through **transfer learning**, which we will explore next.\n"
      ],
      "metadata": {
        "id": "nGG3KdTfCV8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using TorchVision’s Pretrained Models\n",
        "\n",
        "In general, you won’t have to implement standard models like GoogLeNet, ResNet, or ConvNeXt manually, since pretrained networks are readily available with a couple lines of code using TorchVision.\n"
      ],
      "metadata": {
        "id": "R0Jk0kQjl-6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TIP\n",
        "\n",
        "TIMM is another very popular library built on PyTorch: it provides a collection of pretrained image classification models, as well as many related tools such as data loaders, data augmentation utilities, optimizers, schedulers, and more.\n",
        "\n",
        "Hugging Face’s Hub is also a great place to get all sorts of pretrained models (see Chapter 14).\n"
      ],
      "metadata": {
        "id": "7aMegycmmDlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, you can load a ConvNeXt model pretrained on ImageNet with the following code. There are several variants of the ConvNeXt model—tiny, small, base, and large—and this code loads the base variant:\n"
      ],
      "metadata": {
        "id": "7cRjvyg8mEjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torchvision.models.ConvNeXt_Base_Weights.IMAGENET1K_V1\n",
        "model = torchvision.models.convnext_base(weights=weights).to(device)\n"
      ],
      "metadata": {
        "id": "cyoOnruFmFo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That’s all! This code automatically downloads the weights (338 MB) from the Torch Hub, an online repository of pretrained models. The weights are saved and cached for future use (e.g., in ~/.cache/torch/hub; run torch.hub.get_dir() to find the exact path on your system).\n",
        "\n",
        "Some models have newer weights versions (e.g., IMAGENET1K_V2) or other weight variants. For the full list of available models, run:\n",
        "\n",
        "- torchvision.models.list_models()\n",
        "\n",
        "To find the list of pretrained weights available for a given model, such as convnext_base, run:\n",
        "\n",
        "- list(torchvision.models.get_model_weights(\"convnext_base\"))\n",
        "\n",
        "Alternatively, visit https://pytorch.org/vision/main/models.\n"
      ],
      "metadata": {
        "id": "ePKBzUrcmGgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s use this model to classify the two sample images we loaded earlier.\n",
        "\n",
        "Before we can do this, we must first ensure that the images are preprocessed exactly as the model expects. In particular, they must have the right size. A ConvNeXt model expects 224 × 224 pixel images (other models may expect other sizes, such as 299 × 299).\n",
        "\n",
        "Since our sample images are 427 × 640 pixels, we need to resize them. We could do this using TorchVision’s CenterCrop and/or Resize transform, but it’s much easier and safer to use the transforms returned by weights.transforms(), as they are specifically designed for this particular pretrained model:\n"
      ],
      "metadata": {
        "id": "FJztrrehmJLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = weights.transforms()\n",
        "preprocessed_images = transforms(sample_images_permuted)\n"
      ],
      "metadata": {
        "id": "Zc8FhXGEmKop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importantly, these transforms also normalize the pixel intensities just like during training. In this case, the transforms standardize the pixel intensities separately for each color channel, using ImageNet’s means and standard deviations for each channel (we will see how to do this manually later in this chapter).\n"
      ],
      "metadata": {
        "id": "vrJuJm0EmKF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we can move the images to the GPU and pass them to the model. As always, remember to switch the model to evaluation mode before making predictions—the model is in training mode by default—and also turn off autograd:\n"
      ],
      "metadata": {
        "id": "S3PD5dc8mNbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_logits = model(preprocessed_images.to(device))\n"
      ],
      "metadata": {
        "id": "lgZAnwfFmSqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result is a 2 × 1,000 tensor containing the class logits for each image (recall that ImageNet has 1,000 classes). As we did in Chapter 10, we can use torch.argmax() to get the predicted class for each image (i.e., the class with the maximum logit):\n"
      ],
      "metadata": {
        "id": "AbYvRi-wmUed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = torch.argmax(y_logits, dim=1)\n",
        "y_pred\n"
      ],
      "metadata": {
        "id": "M3STLit6mVPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, so good, but what exactly do these classes represent?\n",
        "\n",
        "Well you could find the ImageNet class names online, but once again it’s simpler and safer to get the class names directly from the weights object. Indeed, its meta attribute is a dictionary containing metadata about the pretrained model, including the class names:\n"
      ],
      "metadata": {
        "id": "X40pIkR4mW7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = weights.meta[\"categories\"]\n",
        "[class_names[class_id] for class_id in y_pred]\n"
      ],
      "metadata": {
        "id": "vdqS0B7mmYBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There you have it: the first image is classified as a palace, and the second as a daisy. Since the ImageNet dataset does not have classes for Chinese towers or dahlia flowers, a palace and a daisy are reasonable substitutes (the tower is part of the Summer Palace in Beijing).\n"
      ],
      "metadata": {
        "id": "iY-3w1xFmagW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s look at the top-three predictions using topk():\n"
      ],
      "metadata": {
        "id": "_BAsFVoYmbU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_top3_logits, y_top3_class_ids = y_logits.topk(k=3, dim=1)\n",
        "[[class_names[class_id] for class_id in top3] for top3 in y_top3_class_ids]\n"
      ],
      "metadata": {
        "id": "T7pIUve9mcVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s look at the estimated probabilities for each of these classes:\n"
      ],
      "metadata": {
        "id": "h_dL95Eimdfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_top3_logits.softmax(dim=1)\n"
      ],
      "metadata": {
        "id": "vG2XlGFHmewO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, TorchVision makes it easy to download and use pretrained models, and it works quite well out of the box for ImageNet classes.\n",
        "\n",
        "But what if you need to classify images into classes that don’t belong to the ImageNet dataset, such as various flower species? In that case, you may still benefit from the pretrained models by using them to perform transfer learning.\n"
      ],
      "metadata": {
        "id": "AR1iD0C4mfq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification and Localization\n",
        "\n",
        "Localizing an object in a picture can be expressed as a regression task, as discussed in Chapter 9: to predict a bounding box around the object, a common approach is to predict the location of the bounding box’s center, as well as its width and height (alternatively, you could predict the horizontal and vertical coordinates of the object’s upper-left and lower-right corners).\n",
        "\n",
        "This means we have four numbers to predict.\n"
      ],
      "metadata": {
        "id": "SHZfzh0rnFDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It does not require much change to the ConvNeXt model; we just need to add a second dense output layer with four units (e.g., on top of the global average pooling layer).\n",
        "\n",
        "Here’s a FlowerLocator model that adds a localization head to a given base model, such as our ConvNeXt model:\n"
      ],
      "metadata": {
        "id": "HYQbblTknF4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FlowerLocator(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.localization_head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(base_model.classifier[2].in_features, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        features = self.base_model.features(X)\n",
        "        pool = self.base_model.avgpool(features)\n",
        "        logits = self.base_model.classifier(pool)\n",
        "        bbox = self.localization_head(pool)\n",
        "        return logits, bbox\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "locator_model = FlowerLocator(model).to(device)\n"
      ],
      "metadata": {
        "id": "TbG5UuSrnG5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This locator model has two heads: the first outputs class logits, while the second outputs the bounding box.\n",
        "\n",
        "The localization head has the same number of inputs as the nn.Linear layer of the classification head, but it outputs just four numbers.\n",
        "\n",
        "The forward() method takes a batch of preprocessed images as input and outputs both the predicted class logits (102 per image) and the predicted bounding boxes (1 per image).\n"
      ],
      "metadata": {
        "id": "yQX89_UBnevR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training this model, you can use it as follows:\n"
      ],
      "metadata": {
        "id": "7QLDhFhnnfj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preproc_images = [...]  # a batch of preprocessed images\n",
        "y_pred_logits, y_pred_bbox = locator_model(preprocessed_images.to(device))\n"
      ],
      "metadata": {
        "id": "deTC4sHnnhwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But how can we train this model?\n",
        "\n",
        "Well, we saw how to train a model with two or more outputs in Chapter 10, and this one is no different: in this case, we can use the nn.CrossEntropyLoss for the classification head, and the nn.MSELoss for the localization head.\n",
        "\n",
        "The final loss can just be a weighted sum of the two. Voilà, that’s all there is to it.\n"
      ],
      "metadata": {
        "id": "m6LskwVnni_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hey, not so fast! We have a problem: the Flowers102 dataset does not include any bounding boxes, so we need to add them ourselves.\n",
        "\n",
        "This is often one of the hardest and most costly parts of a machine learning project: labeling and annotating the data.\n"
      ],
      "metadata": {
        "id": "b4cPDbNmnkGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To annotate images with bounding boxes, you may want to use an open source labeling tool like:\n",
        "\n",
        "- Label Studio\n",
        "- OpenLabeler\n",
        "- ImgLab\n",
        "- Labelme\n",
        "- VoTT\n",
        "- VGG Image Annotator\n",
        "\n",
        "Or perhaps a commercial tool like:\n",
        "\n",
        "- LabelBox\n",
        "- Supervisely\n",
        "- Roboflow\n",
        "- RectLabel\n"
      ],
      "metadata": {
        "id": "eYP24bG8n3Ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many of these are now AI assisted, greatly speeding up the annotation task. You may also want to consider crowdsourcing platforms such as Amazon Mechanical Turk if you have a very large number of images to annotate.\n",
        "\n",
        "However, it is quite a lot of work to set up a crowdsourcing platform, prepare the form to be sent to the workers, supervise them, and ensure that the quality of the bounding boxes they produce is good, so make sure it is worth the effort.\n"
      ],
      "metadata": {
        "id": "qKiNYs5in4q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there are just a few hundred or even a couple of thousand images to label, and you don’t plan to do this frequently, it may be preferable to do it yourself: with the right tools, it will only take a few days, and you’ll also gain a better understanding of your dataset and task.\n"
      ],
      "metadata": {
        "id": "59oe0GJ1n5oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then create a custom dataset (see Chapter 10) where each entry contains an image, a label, and a bounding box.\n",
        "\n",
        "TorchVision conveniently includes a BoundingBoxes class that represents a list of bounding boxes.\n"
      ],
      "metadata": {
        "id": "dnWOLpcwn6hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, the following code creates a bounding box for the largest flower in the first image of the Flowers102 training set (for now we only consider one bounding box per image, but we’ll discuss multiple bounding boxes per image later in this chapter):\n"
      ],
      "metadata": {
        "id": "5kT930Lan70S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.tv_tensors\n",
        "\n",
        "bbox = torchvision.tv_tensors.BoundingBoxes(\n",
        "    [[377, 199, 248, 262]],  # center x=377, center y=199, width=248, height=262\n",
        "    format=\"CXCYWH\",        # other possible formats: \"XYXY\" and \"XYWH\"\n",
        "    canvas_size=(500, 754) # raw image size before preprocessing\n",
        ")\n"
      ],
      "metadata": {
        "id": "0ba1d7sPn81D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import torchvision.tv_tensors\n",
        "\n",
        "bbox = torchvision.tv_tensors.BoundingBoxes(\n",
        "    [[377, 199, 248, 262]],  # center x=377, center y=199, width=248, height=262\n",
        "    format=\"CXCYWH\",        # other possible formats: \"XYXY\" and \"XYWH\"\n",
        "    canvas_size=(500, 754) # raw image size before preprocessing\n",
        ")\n"
      ],
      "metadata": {
        "id": "jE7rgusOoKPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BoundingBoxes class is a subclass of TVTensor, which is a subclass of torch.Tensor, so you can treat bounding boxes exactly like regular tensors, with extra features.\n",
        "\n",
        "Most importantly, you can transform bounding boxes using TorchVision’s transforms API v2.\n"
      ],
      "metadata": {
        "id": "0bDQX3zMoLCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, let’s use the transform we defined earlier to preprocess this bounding box:\n"
      ],
      "metadata": {
        "id": "RTOeJxiVoMIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform(bbox)\n"
      ],
      "metadata": {
        "id": "D-DipdGAoNCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BoundingBoxes([[ 90,  91, 120, 154]], format=BoundingBoxFormat.CXCYWH,\n",
        "              canvas_size=(224, 224), clamping_mode=soft)\n"
      ],
      "metadata": {
        "id": "EG-K3hgCoV6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WARNING\n",
        "\n",
        "Resizing and cropping a bounding box works as expected, but rotation is special: the bounding box can’t be rotated since it doesn’t have any rotation parameter, so instead it is resized to fit the rotated box (not the rotated object).\n",
        "\n",
        "As a result, it may end up being a bit too large for the object.\n"
      ],
      "metadata": {
        "id": "nEAMk5J9oOV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can pass a nested data structure to a transform and the output will have the same structure, except with all the images and bounding boxes transformed.\n"
      ],
      "metadata": {
        "id": "mtoSKoMAoYOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_image = [...]  # load the first training image without any preprocessing\n",
        "\n",
        "preproc_image, preproc_target = transform(\n",
        "    (first_image, {\"label\": 0, \"bbox\": bbox})\n",
        ")\n",
        "\n",
        "preproc_bbox = preproc_target[\"bbox\"]\n"
      ],
      "metadata": {
        "id": "u08z7MVSoY9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TIP\n",
        "\n",
        "When using the MSE, a 10-pixel error for a large bounding box will be penalized just as much as a 10-pixel error for a small bounding box.\n",
        "\n",
        "To avoid this, you can use a custom loss function that computes the square root of the width and height—for both the target and the prediction—before computing the MSE.\n"
      ],
      "metadata": {
        "id": "hjTrXzW0oZ71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MSE is simple and often works fairly well to train the model, but it is not a great metric to evaluate how well the model can predict bounding boxes.\n"
      ],
      "metadata": {
        "id": "AyU4ydeMoa2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most common metric for this is the intersection over union (IoU, also known as the Jaccard index): it is the area of overlap between the target bounding box T and the predicted bounding box P, divided by the area of their union P ∪ T.\n",
        "\n",
        "In short, IoU = |P ∩ T| / |P ∪ T|, where |x| is the area of x.\n"
      ],
      "metadata": {
        "id": "GyxE5KMdobwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The IoU ranges from 0 (no overlap) to 1 (perfect overlap). It is implemented by the torchvision.ops.box_iou() function.\n"
      ],
      "metadata": {
        "id": "Am0YsEKdodxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The IoU is not great for training because it is equal to zero whenever P and T have no overlap, regardless of the distance between them or their shapes: in this case the gradient is also equal to zero and therefore gradient descent cannot make any progress.\n"
      ],
      "metadata": {
        "id": "2ClQ0Iteoe13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luckily, it’s possible to fix this flaw by incorporating extra information.\n",
        "\n",
        "For example, the Generalized IoU (GIoU), introduced in a 2019 paper by H. Rezatofighi et al., considers the smallest box S that contains both P and T, and it subtracts from the IoU the ratio of S that is not covered by P or T.\n"
      ],
      "metadata": {
        "id": "nTfrQJ_iof87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In short:\n",
        "\n",
        "GIoU = IoU – |S – (P ∪ T)| / |S|\n",
        "\n",
        "Since we want to maximize the GIoU, the GIoU loss is equal to 1 – GIoU.\n",
        "\n",
        "This loss quickly became popular, and it is implemented by the torchvision.ops.generalized_box_iou_loss() function.\n"
      ],
      "metadata": {
        "id": "NVg07tsJohMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another important variant of the IoU is the Complete IoU (CIoU), introduced in a 2020 paper by Z. Zheng et al.\n",
        "\n",
        "It considers three geometric factors:\n",
        "- the IoU (the more overlap, the better)\n",
        "- the distance between the centers of P and T (the closer, the better)\n",
        "- the similarity between the aspect ratios of P and T (the closer, the better)\n"
      ],
      "metadata": {
        "id": "T_4pJweiojEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss is 1 – CIoU, and it is implemented by the torchvision.ops.complete_box_iou_loss() function.\n",
        "\n",
        "It generally performs better than the MSE or the GIoU, converging faster and leading to more accurate bounding boxes, so it is becoming the default loss for localization.\n"
      ],
      "metadata": {
        "id": "5NqdvWqQokQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifying and localizing a single object is nice, but what if the images contain multiple objects (as is often the case in the flowers dataset)?\n"
      ],
      "metadata": {
        "id": "wSUgksvpolqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object Detection\n",
        "\n",
        "The task of classifying and localizing multiple objects in an image is called object detection.\n",
        "\n",
        "Until a few years ago, a common approach was to take a CNN that was trained to classify and locate a single object roughly centered in the image, then slide this CNN across the image and make predictions at each step.\n"
      ],
      "metadata": {
        "id": "nCASIauhpHJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CNN was generally trained to predict not only class probabilities and a bounding box, but also an objectness score: this is the estimated probability that the image does indeed contain an object centered near the middle.\n",
        "\n",
        "This is a binary classification output; it can be produced by a dense output layer with a single unit, using the sigmoid activation function and trained using the binary cross-entropy loss.\n"
      ],
      "metadata": {
        "id": "GerPO5iPpMtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NOTE\n",
        "\n",
        "Instead of an objectness score, a “no-object” class was sometimes added, but in general this did not work as well.\n",
        "\n",
        "The questions “Is an object present?” and “What type of object is it?” are best answered separately.\n"
      ],
      "metadata": {
        "id": "AtGaQjsVpNry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This sliding-CNN approach is illustrated in Figure 12-25.\n",
        "\n",
        "In this example, the image was chopped into a 5 × 7 grid, and we see a CNN—the thick black rectangle—sliding across all 3 × 3 regions and making predictions at each step.\n"
      ],
      "metadata": {
        "id": "TX6VVZHdpOnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Figure 12-25.** Detecting multiple objects by sliding a CNN across the image\n"
      ],
      "metadata": {
        "id": "6v2dH3M7pPuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this figure, the CNN has already made predictions for three of these 3 × 3 regions:\n"
      ],
      "metadata": {
        "id": "OyNmEYCfqAxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When looking at the top-left 3 × 3 region (centered on the red-shaded grid cell located in the second row and second column), it detected the leftmost rose.\n",
        "\n",
        "Notice that the predicted bounding box exceeds the boundary of this 3 × 3 region. That’s absolutely fine: even though the CNN could not see the bottom part of the rose, it was able to make a reasonable guess as to where it might be.\n"
      ],
      "metadata": {
        "id": "j5cg62rEqDWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It also predicted class probabilities, giving a high probability to the “rose” class.\n",
        "\n",
        "Lastly, it predicted a fairly high objectness score, since the center of the bounding box lies within the central grid cell (in this figure, the objectness score is represented by the thickness of the bounding box).\n"
      ],
      "metadata": {
        "id": "SGQ7TodcqEoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When looking at the next 3 × 3 region, one grid cell to the right (centered on the shaded blue square), it did not detect any flower centered in that region, so it predicted a very low objectness score.\n",
        "\n",
        "Therefore, the predicted bounding box and class probabilities can safely be ignored.\n"
      ],
      "metadata": {
        "id": "_hIsYAg-qFeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, when looking at the next 3 × 3 region, again one grid cell to the right (centered on the shaded green cell), it detected the rose at the top, although not perfectly.\n",
        "\n",
        "This rose is not well centered within this region, so the predicted objectness score was not very high.\n"
      ],
      "metadata": {
        "id": "prJh6uq4qHhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can imagine how sliding the CNN across the whole image would give you a total of 15 predicted bounding boxes, organized in a 3 × 5 grid, with each bounding box accompanied by its estimated class probabilities and objectness score.\n"
      ],
      "metadata": {
        "id": "dizx7-P2qg0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since objects can have varying sizes, you may then want to slide the CNN again across 2 × 2 and 4 × 4 regions as well, to capture smaller and larger objects.\n"
      ],
      "metadata": {
        "id": "g-DuIeauqiTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This technique is fairly straightforward, but as you can see it will often detect the same object multiple times, at slightly different positions.\n",
        "\n",
        "Some post-processing is needed to get rid of all the unnecessary bounding boxes.\n"
      ],
      "metadata": {
        "id": "RRBGGcgwqjl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A common approach for this is called **non-max suppression (NMS)**.\n",
        "\n",
        "Here’s how it works:\n"
      ],
      "metadata": {
        "id": "ygoxlf7YqlBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. First, get rid of all the bounding boxes for which the objectness score is below some threshold.\n"
      ],
      "metadata": {
        "id": "DWlx-5eVq-Bi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Find the remaining bounding box with the highest objectness score, and get rid of all the other remaining bounding boxes that overlap a lot with it (e.g., with an IoU greater than 60%).\n"
      ],
      "metadata": {
        "id": "VexZ3Xh4q-1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Repeat step 2 until there are no more bounding boxes to get rid of.\n"
      ],
      "metadata": {
        "id": "zakRdlqgrAH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This simple approach to object detection works pretty well, but it requires running the CNN many times, so it is quite slow.\n",
        "\n",
        "Fortunately, there is a much faster way to slide a CNN across an image: using a fully convolutional network (FCN).\n"
      ],
      "metadata": {
        "id": "Oxs4OpkOrCKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fully Convolutional Networks\n",
        "\n",
        "The idea of FCNs was first introduced in a 2015 paper by Jonathan Long et al., for semantic segmentation.\n"
      ],
      "metadata": {
        "id": "9GOHGnp8rDJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The authors pointed out that you could replace the dense layers at the top of a CNN with convolutional layers.\n"
      ],
      "metadata": {
        "id": "sp8JE7IrrED0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose a dense layer with 200 neurons sits on top of a convolutional layer that outputs 100 feature maps, each of size 7 × 7.\n",
        "\n",
        "Each neuron computes a weighted sum of all 100 × 7 × 7 activations.\n"
      ],
      "metadata": {
        "id": "fzFIAAKmrE_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now replace this dense layer with a convolutional layer using 200 filters of size 7 × 7 and \"valid\" padding.\n",
        "\n",
        "The output will be 200 feature maps of size 1 × 1.\n"
      ],
      "metadata": {
        "id": "vownAdh6rGQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In other words, the output values are exactly the same — only the tensor shape changes from:\n",
        "\n",
        "- Dense: [batch size, 200]\n",
        "- Conv: [batch size, 200, 1, 1]\n"
      ],
      "metadata": {
        "id": "mQyrKktCrHUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TIP\n",
        "\n",
        "To convert a dense layer to a convolutional layer:\n",
        "\n",
        "- Number of filters = number of dense units\n",
        "- Kernel size = input feature map size\n",
        "- Padding = \"valid\"\n"
      ],
      "metadata": {
        "id": "oa8ApccwrJ3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is this important?\n",
        "\n",
        "Because convolutional layers can process images of any spatial size, while dense layers cannot.\n",
        "\n",
        "This means FCNs can be trained and run on images of any size.\n"
      ],
      "metadata": {
        "id": "Y8HBqmpXrLHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, if an FCN is trained on 224 × 224 images and later receives a 448 × 448 image, it will naturally output a larger prediction grid — without retraining.\n"
      ],
      "metadata": {
        "id": "nHpA0uM0rbW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This allows the CNN to make many predictions in a single forward pass, instead of sliding a window manually.\n",
        "\n",
        "In fact, **YOLO** — *You Only Look Once* — is based on this idea.\n"
      ],
      "metadata": {
        "id": "Al8qIb61rc6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Figure 12-26.** The same fully convolutional network processing a small image (left) and a large one (right)\n"
      ],
      "metadata": {
        "id": "eWLZiwXird-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You Only Look Once (YOLO)\n",
        "\n",
        "YOLO is a fast and accurate object detection architecture proposed by Joseph Redmon et al. in 2015.\n",
        "\n",
        "It is fast enough to run in real time on video.\n"
      ],
      "metadata": {
        "id": "xaCA079rre8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLO differs from basic FCNs in several important ways:\n"
      ],
      "metadata": {
        "id": "-B_cv_iTrgFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Each grid cell only predicts objects whose bounding box center lies within that cell\n",
        "- Bounding box coordinates are relative to the grid cell\n",
        "- Width and height may extend beyond the cell\n"
      ],
      "metadata": {
        "id": "9oj22GcZrhX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- YOLO predicts multiple bounding boxes per grid cell\n",
        "- Each bounding box has its own objectness score\n"
      ],
      "metadata": {
        "id": "npFO8glErjB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Class probabilities are predicted per grid cell, not per bounding box\n"
      ],
      "metadata": {
        "id": "Qpo5_xLDrkEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLO has evolved through many versions (YOLOv2, YOLOv3, YOLO9000, and beyond), adding improvements such as:\n",
        "\n",
        "- Anchor priors\n",
        "- More bounding boxes per cell\n",
        "- More classes\n",
        "- Skip connections\n",
        "- Tiny versions for real-time inference\n"
      ],
      "metadata": {
        "id": "1LBu03AArtDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Mean Average Precision (mAP)\n",
        "\n",
        "A very common metric used in object detection is the mean average precision (mAP).\n"
      ],
      "metadata": {
        "id": "Oaxj_ghksVDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compute mAP, we first compute the **average precision (AP)** for each class by averaging the maximum precision achievable at recall levels from 0% to 100%.\n"
      ],
      "metadata": {
        "id": "jZkIeM4qsWUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mAP is then computed by averaging the AP values across all classes.\n"
      ],
      "metadata": {
        "id": "PGc3gyD-syWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In object detection, a prediction is only considered correct if:\n",
        "\n",
        "- The predicted class is correct\n",
        "- The IoU with the ground truth box exceeds a threshold (e.g., 0.5)\n"
      ],
      "metadata": {
        "id": "2PMIDnhnszgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This leads to metrics such as:\n",
        "\n",
        "- mAP@0.5\n",
        "- mAP@[.50:.95]\n"
      ],
      "metadata": {
        "id": "SW3d7VLNs08t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TorchMetrics provides a ready-to-use MeanAveragePrecision metric that handles all of this.\n"
      ],
      "metadata": {
        "id": "lwUcR-AIs2yr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TorchVision does not include YOLO models, but you can use the Ultralytics library, which provides pretrained YOLO models based on PyTorch.\n"
      ],
      "metadata": {
        "id": "5vYYICiBs4MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov9m.pt')  # n=nano, s=small, m=medium, x=large\n",
        "images = [\"https://homl.info/soccer.jpg\", \"https://homl.info/traffic.jpg\"]\n",
        "results = model(images)\n"
      ],
      "metadata": {
        "id": "Shj7P5TZs5NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is a list of Results objects.\n",
        "\n",
        "For example, here is the first detected object in the first image:\n"
      ],
      "metadata": {
        "id": "3sdnVN1vs6o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results[0].summary()[0]\n"
      ],
      "metadata": {
        "id": "AZbeMpOys7jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'name': 'sports ball',\n",
        " 'class': 32,\n",
        " 'confidence': 0.96214,\n",
        " 'box': {'x1': 245.35733, 'y1': 286.03003, 'x2': 300.62509, 'y2': 343.57184}}\n"
      ],
      "metadata": {
        "id": "s6PEPMWUs8Y0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TIP\n",
        "\n",
        "The Ultralytics library also provides a simple API to train YOLO models on your own datasets.\n",
        "\n",
        "See https://docs.ultralytics.com/modes/train for more details.\n"
      ],
      "metadata": {
        "id": "jirwLZU1s9aI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several other pretrained object detection models are available via TorchVision:\n"
      ],
      "metadata": {
        "id": "Kbw_GQFis-ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Faster R-CNN  \n",
        "- SSD  \n",
        "- SSDlite  \n",
        "- RetinaNet  \n",
        "- FCOS\n"
      ],
      "metadata": {
        "id": "8tRcZjD4s_uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we’ve only considered detecting objects in single images.\n",
        "\n",
        "But what about videos?\n",
        "\n",
        "Objects must not only be detected in each frame, they must also be tracked over time.\n"
      ],
      "metadata": {
        "id": "YNZRgZEVtAzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object Tracking\n",
        "\n",
        "Object tracking is a challenging task: objects move, they may grow or shrink as they get closer or further away, their appearance may change as they turn around or move to different lighting conditions or backgrounds, they may be temporarily occluded by other objects, and so on.\n"
      ],
      "metadata": {
        "id": "QsYeFG_u0aA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the most popular object tracking systems is **DeepSORT**. It is based on a combination of classical algorithms and deep learning:\n"
      ],
      "metadata": {
        "id": "-FxuDizS0a4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the most popular object tracking systems is **DeepSORT**. It is based on a combination of classical algorithms and deep learning:\n"
      ],
      "metadata": {
        "id": "RpnoHbtT0bpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It uses **Kalman filters** to estimate the most likely current position of an object given prior detections, and assuming that objects tend to move at a constant speed.\n"
      ],
      "metadata": {
        "id": "LvPgGmDj0cZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It uses a **deep learning model** to measure the resemblance between new detections and existing tracked objects.\n"
      ],
      "metadata": {
        "id": "gAnSFYBb0evB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lastly, it uses the **Hungarian algorithm** to map new detections to existing tracked objects (or to new tracked objects). This algorithm efficiently finds the combination of mappings that minimizes the distance between the detections and the predicted positions of tracked objects, while also minimizing the appearance discrepancy.\n"
      ],
      "metadata": {
        "id": "lJGunFfb1PbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, imagine a red ball that just bounced off a blue ball traveling in the opposite direction.\n"
      ],
      "metadata": {
        "id": "Fw-YvrM-1SaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the previous positions of the balls, the Kalman filter will predict that the balls will go through each other; indeed, it assumes that objects move at a constant speed, so it will not expect the bounce.\n"
      ],
      "metadata": {
        "id": "8bNA0RTp1kwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the Hungarian algorithm only considered positions, then it would happily map the new detections to the wrong balls, as if they had just gone through each other and swapped colors.\n"
      ],
      "metadata": {
        "id": "Z1pLPNJc1l_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But thanks to the resemblance measure, the Hungarian algorithm will notice the problem. Assuming the balls are not too similar, the algorithm will map the new detections to the correct balls.\n"
      ],
      "metadata": {
        "id": "ys0dbtzD1nPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Ultralytics library supports object tracking. It uses the **Bot-SORT** algorithm by default: this algorithm is very similar to DeepSORT but it’s faster and more accurate thanks to improvements such as camera-motion compensation and tweaks to the Kalman filter.\n"
      ],
      "metadata": {
        "id": "FXVC4gHJ1oS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we also print the ID of each tracked object at every frame, and we save a copy of the video with annotations (its path is displayed at the end):\n"
      ],
      "metadata": {
        "id": "Qb5YXJPT1pdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_video = \"https://homl.info/cars.mp4\"\n",
        "\n",
        "results = model.track(\n",
        "    source=my_video,\n",
        "    stream=True,\n",
        "    save=True\n",
        ")\n",
        "\n",
        "for frame_results in results:\n",
        "    summary = frame_results.summary()  # similar summary as earlier + track id\n",
        "    track_ids = [obj[\"track_id\"] for obj in summary]\n",
        "    print(\"Track ids:\", track_ids)\n"
      ],
      "metadata": {
        "id": "d7Oe1buw1qVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we have located objects using bounding boxes.\n",
        "\n",
        "This is often sufficient, but sometimes you need to locate objects with much more precision—for example, to remove the background behind a person during a videoconference call.\n",
        "\n",
        "Let’s see how to go down to the pixel level.\n"
      ],
      "metadata": {
        "id": "5BeBLq9X1rha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Segmentation\n"
      ],
      "metadata": {
        "id": "0N1RObUO2-HK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In semantic segmentation, each pixel is classified according to the class of the object it belongs to (e.g., road, car, pedestrian, building, etc.), as shown in Figure 12-27.\n"
      ],
      "metadata": {
        "id": "RcSOXy9M2_vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that different objects of the same class are not distinguished. For example, all the bicycles on the righthand side of the segmented image end up as one big lump of pixels.\n"
      ],
      "metadata": {
        "id": "5lkGcn3m3An4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main difficulty in this task is that when images go through a regular CNN, they gradually lose their spatial resolution (due to the layers with strides greater than 1).\n"
      ],
      "metadata": {
        "id": "pVPO7KQc3Beu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, a regular CNN may end up knowing that there’s a person somewhere in the bottom left of the image, but it might not be much more precise than that.\n"
      ],
      "metadata": {
        "id": "eD8qi9eo3Cjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 12-27. Semantic segmentation\n"
      ],
      "metadata": {
        "id": "16IMo7p53Dbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like for object detection, there are many different approaches to tackle this problem, some quite complex.\n"
      ],
      "metadata": {
        "id": "dpiaPxkK3Ed8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, a fairly simple solution was proposed in the 2015 paper by Jonathan Long et al. on fully convolutional networks (FCNs).\n"
      ],
      "metadata": {
        "id": "Bf1-m0P_3FYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The authors start by taking a pretrained CNN and turning it into an FCN.\n"
      ],
      "metadata": {
        "id": "OriogyXx3G5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CNN applies an overall stride of 32 to the input image (i.e., if you multiply all the strides), meaning the last layer outputs feature maps that are 32 times smaller than the input image.\n"
      ],
      "metadata": {
        "id": "phXHRdUP33CG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is clearly too coarse, so they added a single upsampling layer that multiplies the resolution by 32.\n"
      ],
      "metadata": {
        "id": "Knac__4o34K-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several solutions available for upsampling (increasing the size of an image), such as bilinear interpolation, but that only works reasonably well up to ×4 or ×8.\n"
      ],
      "metadata": {
        "id": "31x9hX9G35N3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead, they use a transposed convolutional layer.\n"
      ],
      "metadata": {
        "id": "zh3xY-IJ7Swx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is equivalent to first stretching the image by inserting empty rows and columns (full of zeros), then performing a regular convolution.\n"
      ],
      "metadata": {
        "id": "Sw0Zvh7-7TmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, some people prefer to think of it as a regular convolutional layer that uses fractional strides.\n"
      ],
      "metadata": {
        "id": "mASwZmTt7VrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transposed convolutional layer can be initialized to perform something close to linear interpolation, but since it is a trainable layer, it will learn to do better during training.\n"
      ],
      "metadata": {
        "id": "HEoY1prx7Woh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, you can use the `nn.ConvTranspose2d` layer.\n"
      ],
      "metadata": {
        "id": "hMbw4rEl7X2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE\n",
        "\n",
        "In a transposed convolutional layer, the stride defines how much the input will be stretched, not the size of the filter steps, so the larger the stride, the larger the output (unlike for convolutional layers or pooling layers).\n"
      ],
      "metadata": {
        "id": "0-2vnQ0u7afg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 12-28. Upsampling using a transposed convolutional layer\n"
      ],
      "metadata": {
        "id": "HAMN28TA7b3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other PyTorch Convolutional Layers\n"
      ],
      "metadata": {
        "id": "78eKVw8V7dAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**nn.Conv1d**\n",
        "\n",
        "A convolutional layer for 1D inputs, such as time series or text (sequences of letters or words).\n"
      ],
      "metadata": {
        "id": "NogSZT767fos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**À-trous convolutional layer**\n",
        "\n",
        "Setting the dilation hyperparameter of any convolutional layer to a value of 2 or more creates an à-trous convolutional layer (à trous is French for “with holes”).\n"
      ],
      "metadata": {
        "id": "yjB4bNt47jC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is equivalent to using a regular convolutional layer with a filter dilated by inserting rows and columns of zeros.\n"
      ],
      "metadata": {
        "id": "oOFTpkNq7kMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This lets the convolutional layer have a larger receptive field at no computational cost and using no extra parameters.\n"
      ],
      "metadata": {
        "id": "I7U48P8p7k99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using transposed convolutional layers for upsampling is OK, but still too imprecise.\n"
      ],
      "metadata": {
        "id": "0Gp28-z27o21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do better, Long et al. added skip connections from lower layers.\n"
      ],
      "metadata": {
        "id": "43NNs3CD7qBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "They upsampled the output image by a factor of 2 and added the output of a lower layer that had this double resolution.\n"
      ],
      "metadata": {
        "id": "jMOPOogq7rBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then they upsampled the result by a factor of 16, leading to a total upsampling factor of 32.\n"
      ],
      "metadata": {
        "id": "sk2_6AZu7scz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This recovered some of the spatial resolution that was lost in earlier pooling layers.\n"
      ],
      "metadata": {
        "id": "q7cGoo7T7tfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In their best architecture, they used a second similar skip connection to recover even finer details from an even lower layer.\n"
      ],
      "metadata": {
        "id": "cpoaLWdJ7uj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In their best architecture, they used a second similar skip connection to recover even finer details from an even lower layer.\n"
      ],
      "metadata": {
        "id": "_ANlFPf_7v3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is even possible to scale up beyond the size of the original image, which can be used for super-resolution.\n"
      ],
      "metadata": {
        "id": "rMvMaOcz7xCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 12-29. Skip layers recover some spatial resolution from lower layers\n"
      ],
      "metadata": {
        "id": "703jSGbg7x9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TIP\n",
        "\n",
        "The FCN model is available in TorchVision, along with a couple of other semantic segmentation models.\n"
      ],
      "metadata": {
        "id": "8a0DKQh37zU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instance segmentation is similar to semantic segmentation, but instead of merging all objects of the same class, each object is distinguished from the others.\n"
      ],
      "metadata": {
        "id": "ik2il3BS70RP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, Mask R-CNN extends Faster R-CNN by additionally producing a pixel mask for each bounding box.\n"
      ],
      "metadata": {
        "id": "QgVBkpve71y6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So you get a bounding box, class probabilities, and a pixel mask for each object.\n"
      ],
      "metadata": {
        "id": "2KR1LsfB726B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model is available in TorchVision, pretrained on the COCO 2017 dataset.\n"
      ],
      "metadata": {
        "id": "gFH1eJrj74P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TIP\n",
        "\n",
        "TorchVision’s transforms API v2 can apply to masks and videos, just like it applies to bounding boxes.\n"
      ],
      "metadata": {
        "id": "dOL5bFng75T9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, deep computer vision is a vast and fast-paced field, with new architectures appearing every year.\n"
      ],
      "metadata": {
        "id": "fJmZAfP576rG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since 2020, Transformers have also entered the computer vision space.\n"
      ],
      "metadata": {
        "id": "7hdjExZF78AG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Researchers are now tackling harder problems such as adversarial learning, explainability, realistic image generation, single-shot learning, video prediction, and multimodal models.\n"
      ],
      "metadata": {
        "id": "n0CsGMqb79bT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we move on to sequential data such as time series, using recurrent neural networks and convolutional neural networks.\n"
      ],
      "metadata": {
        "id": "SHVbsdPB7-ik"
      }
    }
  ]
}