{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdcaOgvN6zBtxfd3Y4O82/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanDCunha/Hands-On-Machine-Learning-with-Scikit-Learn-and-PyTorch/blob/main/Chapter9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From Biological to Artificial Neurons\n",
        "\n",
        "Surprisingly, artificial neural networks (ANNs) have been around for quite a while. They were first introduced in 1943 by neurophysiologist **Warren McCulloch** and mathematician **Walter Pitts**. In their landmark paper *“A Logical Calculus of Ideas Immanent in Nervous Activity”*, they presented a simplified computational model of how biological neurons might work together to perform complex computations using propositional logic. This was the first artificial neural network architecture.\n",
        "\n",
        "Early successes led to the belief that intelligent machines were just around the corner. When this failed to materialize in the 1960s, funding dried up and ANNs entered a long winter. A revival in the 1980s brought new architectures and better training techniques, but progress remained slow. By the 1990s, other machine learning methods such as support vector machines offered better results and stronger theoretical foundations, causing neural networks to fall out of favor again.\n",
        "\n",
        "Today, ANNs are experiencing another resurgence — and this time, it’s different.\n"
      ],
      "metadata": {
        "id": "PCbN9n_LROv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Neural Networks Succeeded This Time\n",
        "\n",
        "Several key factors explain the renewed success of neural networks:\n",
        "\n",
        "- **Massive datasets** are now available, and ANNs often outperform other models on large, complex problems.\n",
        "- **Computing power** has increased dramatically since the 1990s, driven by Moore’s Law and the widespread availability of GPUs originally developed for gaming.\n",
        "- **Cloud platforms** make powerful hardware accessible to nearly everyone.\n",
        "- **Training algorithms** have improved slightly but significantly in effectiveness.\n",
        "- **Theoretical concerns**, such as getting stuck in local optima, turned out to be far less problematic in practice.\n",
        "- **The Transformer architecture (2017)** revolutionized the field by enabling models that scale well and work across text, images, audio, robotics, and protein folding.\n",
        "- **Transfer learning, in-context learning, few-shot learning, and chain-of-thought prompting** have unlocked new capabilities.\n",
        "\n",
        "ANNs now benefit from a virtuous cycle of funding, research, and real-world impact. AI is no longer hidden behind the scenes — tools like ChatGPT have brought it directly to the public.\n"
      ],
      "metadata": {
        "id": "gAe33GmdRPvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Biological Neurons\n",
        "\n",
        "A biological neuron consists of:\n",
        "- A **cell body** containing the nucleus\n",
        "- Many branching **dendrites**\n",
        "- One long **axon**\n",
        "- Branching **telodendria** ending in **synapses**\n",
        "\n",
        "Neurons communicate via **electrical impulses** called action potentials. When enough neurotransmitters are received in a short time window, the neuron fires — unless inhibitory signals prevent it.\n",
        "\n",
        "Despite their simplicity, biological neurons form massive networks with billions of nodes, each connected to thousands of others. These networks often organize into **layers**, especially in the cerebral cortex.\n"
      ],
      "metadata": {
        "id": "8qxgup8KR0Px"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logical Computations with Neurons\n",
        "\n",
        "McCulloch and Pitts proposed a simplified neuron model with:\n",
        "- Binary inputs\n",
        "- A binary output\n",
        "- An activation threshold\n",
        "\n",
        "They showed that networks of such neurons can compute **any logical proposition**.\n",
        "\n",
        "Using neurons that activate when at least two inputs are active, simple networks can compute:\n",
        "- Identity\n",
        "- AND\n",
        "- OR\n",
        "- NOT (with inhibitory connections)\n",
        "\n",
        "By combining these networks, arbitrarily complex logical expressions can be constructed.\n"
      ],
      "metadata": {
        "id": "1u8WaCrzR1po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Perceptron\n",
        "\n",
        "Invented in 1957 by **Frank Rosenblatt**, the perceptron is based on a **threshold logic unit (TLU)**.\n",
        "\n",
        "A TLU:\n",
        "1. Computes a weighted sum  \n",
        "   \\[\n",
        "   z = w^\\top x + b\n",
        "   \\]\n",
        "2. Applies a **step function**  \n",
        "   \\[\n",
        "   h_w(x) = \\text{step}(z)\n",
        "   \\]\n",
        "\n",
        "This is similar to logistic regression, except it uses a step function instead of a sigmoid.\n"
      ],
      "metadata": {
        "id": "WMR3GAGWR28m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step Functions\n",
        "\n",
        "Common step functions include:\n",
        "- **Heaviside step function**\n",
        "- **Sign function**\n",
        "\n",
        "These functions output discrete values and are not differentiable.\n"
      ],
      "metadata": {
        "id": "7cGXqkWXSfNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptron Architecture\n",
        "\n",
        "A perceptron consists of:\n",
        "- An **input layer**\n",
        "- A single **fully connected output layer**\n",
        "\n",
        "A perceptron with multiple outputs can perform **multilabel classification** or **multiclass classification**.\n",
        "\n",
        "The outputs of a fully connected layer can be computed efficiently using matrix operations:\n",
        "\\[\n",
        "Y = \\phi(XW + b)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\(X\\): input matrix\n",
        "- \\(W\\): weight matrix\n",
        "- \\(b\\): bias vector\n",
        "- \\(\\phi\\): activation function\n"
      ],
      "metadata": {
        "id": "vpXEYKv2SgCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptron Training (Hebbian Learning)\n",
        "\n",
        "Perceptron training is inspired by **Hebb’s rule**:\n",
        "\n",
        "> “Cells that fire together, wire together.”\n",
        "\n",
        "For each training instance:\n",
        "- The model predicts outputs\n",
        "- Weights connected to incorrect outputs are updated to reduce error\n",
        "\n",
        "The update rule is:\n",
        "\\[\n",
        "w_{i,j} \\leftarrow w_{i,j} + \\eta (y_j - \\hat{y}_j)x_i\n",
        "\\]\n",
        "\n",
        "Perceptrons converge **only if the data is linearly separable**.\n"
      ],
      "metadata": {
        "id": "XtkHMNruTVrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations of Perceptrons\n",
        "\n",
        "Perceptrons:\n",
        "- Have **linear decision boundaries**\n",
        "- Cannot solve problems like **XOR**\n",
        "- Do not output class probabilities\n",
        "- Use no regularization by default\n",
        "\n",
        "These limitations led to a major loss of interest in neural networks in the late 1960s.\n"
      ],
      "metadata": {
        "id": "e4n42nKbTWu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multilayer Perceptrons (MLPs)\n",
        "\n",
        "Stacking perceptrons produces a **multilayer perceptron (MLP)**.\n",
        "\n",
        "An MLP includes:\n",
        "- One input layer\n",
        "- One or more **hidden layers**\n",
        "- One output layer\n",
        "\n",
        "MLPs can solve **nonlinear problems**, including XOR.\n"
      ],
      "metadata": {
        "id": "0DbXTBFmTXvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Neural Networks\n",
        "\n",
        "When an ANN contains many hidden layers, it is called a **deep neural network (DNN)**.\n",
        "\n",
        "The field of **deep learning** studies such models, though the term is often used loosely for all neural networks.\n",
        "\n",
        "MLPs are **feedforward neural networks**, meaning signals flow in one direction only.\n"
      ],
      "metadata": {
        "id": "1tJ16UxRTYvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation\n",
        "\n",
        "Training MLPs was difficult until **reverse-mode automatic differentiation** was introduced by **Seppo Linnainmaa** in 1970.\n",
        "\n",
        "Backpropagation combines:\n",
        "- Reverse-mode autodiff\n",
        "- Gradient descent\n",
        "\n",
        "It computes gradients efficiently in two passes:\n",
        "- Forward pass\n",
        "- Backward pass\n"
      ],
      "metadata": {
        "id": "_6_0hWwITZjM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Backpropagation Works\n",
        "\n",
        "1. Process one **mini-batch** at a time\n",
        "2. Perform a **forward pass**\n",
        "3. Compute the **loss**\n",
        "4. Compute gradients using the **chain rule**\n",
        "5. Propagate gradients backward through layers\n",
        "6. Update weights using **gradient descent**\n",
        "\n",
        "Random weight initialization is essential to break symmetry.\n"
      ],
      "metadata": {
        "id": "-ienkl47Ta30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Functions\n",
        "\n",
        "Step functions were replaced because they have zero gradients.\n",
        "\n",
        "Common activation functions:\n",
        "- **Sigmoid**\n",
        "- **Tanh**\n",
        "- **ReLU**\n",
        "\n",
        "ReLU is now the default for most architectures due to speed and effectiveness.\n",
        "\n",
        "Without nonlinear activation functions, deep networks collapse into linear models.\n"
      ],
      "metadata": {
        "id": "kJXpBWqsT2I3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Neural Networks Work\n",
        "\n",
        "A deep neural network with nonlinear activations can theoretically approximate **any continuous function**.\n",
        "\n",
        "This makes modern neural networks incredibly powerful — and explains why they now dominate machine learning.\n"
      ],
      "metadata": {
        "id": "WfY7c8yVT3np"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building and Training MLPs with Scikit-Learn\n",
        "\n",
        "Multilayer perceptrons (MLPs) can tackle a wide range of tasks, but the most common are **regression** and **classification**. Scikit-Learn provides tools for both. Let’s start with regression.\n"
      ],
      "metadata": {
        "id": "jJxOmeZIUdPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression MLPs\n",
        "\n",
        "For a regression task where you want to predict a **single numeric value** (for example, the price of a house), you only need **one output neuron**. Its output is the predicted value.\n",
        "\n",
        "For **multivariate regression**, where you predict multiple values at once, you need **one output neuron per output dimension**. For example:\n",
        "- Predicting an object’s center in an image requires **2 output neurons** (x and y).\n",
        "- Adding a bounding box requires **2 more neurons** (width and height).\n",
        "- Total: **4 output neurons**.\n"
      ],
      "metadata": {
        "id": "learZLm6UecC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "metadata": {
        "id": "tzoThj_3ZaMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing = fetch_california_housing()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    housing.data, housing.target, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "vYt3i8XCZwBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using `MLPRegressor`\n",
        "\n",
        "Scikit-Learn provides the `MLPRegressor` class. As an example, we can build an MLP with:\n",
        "- **3 hidden layers**\n",
        "- **50 neurons per hidden layer**\n",
        "- **ReLU activation** in hidden layers\n",
        "- **No activation function** in the output layer\n",
        "\n",
        "The model automatically adapts the input and output sizes when training starts.\n"
      ],
      "metadata": {
        "id": "dIo_qDH0Ukgw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8e_L91bQtSy"
      },
      "outputs": [],
      "source": [
        "mlp_reg = MLPRegressor(\n",
        "    hidden_layer_sizes=[50, 50, 50],\n",
        "    early_stopping=True,\n",
        "    verbose=True,\n",
        "    random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks are sensitive to feature scaling.  \n",
        "We therefore standardize the input features using `StandardScaler` and train the model using a pipeline.\n"
      ],
      "metadata": {
        "id": "iHcvhJlYbXe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = make_pipeline(StandardScaler(), mlp_reg)\n",
        "pipeline.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "MQS1VT7IbYR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`MLPRegressor` uses **R² score** for validation.  \n",
        "The best validation score achieved during training is:\n"
      ],
      "metadata": {
        "id": "5FOGoHXcbZTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_reg.best_validation_score_\n"
      ],
      "metadata": {
        "id": "HMJOnH2zbaKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we evaluate the model on the test set using Root Mean Squared Error (RMSE).\n"
      ],
      "metadata": {
        "id": "3CJPKCzibbCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = pipeline.predict(X_test)\n",
        "rmse = root_mean_squared_error(y_test, y_pred)\n",
        "rmse\n"
      ],
      "metadata": {
        "id": "lTHRsfmwbbl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This MLP does not use an activation function in the output layer, so it can predict any real value.\n",
        "\n",
        "To constrain outputs:\n",
        "- Positive-only → ReLU or Softplus\n",
        "- Bounded range → Sigmoid (0–1) or Tanh (–1 to 1)\n",
        "\n",
        "Unfortunately, `MLPRegressor` does not support output-layer activations.\n"
      ],
      "metadata": {
        "id": "TNPCLv2Ubcq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification MLPs\n",
        "\n",
        "MLPs can also handle classification tasks:\n",
        "\n",
        "### Binary classification\n",
        "- 1 output neuron\n",
        "- Sigmoid activation\n",
        "- Cross-entropy loss\n",
        "\n",
        "### Multilabel classification\n",
        "- 1 output neuron per label\n",
        "- Sigmoid activation\n",
        "- Probabilities do not sum to 1\n",
        "\n",
        "### Multiclass classification\n",
        "- 1 output neuron per class\n",
        "- Softmax activation\n",
        "- Probabilities sum to 1\n"
      ],
      "metadata": {
        "id": "tmCfPDfgbd5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fashion MNIST Classification Example\n",
        "\n",
        "Fashion MNIST contains:\n",
        "- 70,000 grayscale images\n",
        "- 28×28 pixels\n",
        "- 10 classes of clothing items\n",
        "\n",
        "We load it using `fetch_openml`.\n"
      ],
      "metadata": {
        "id": "wR4UbfeAbin6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "fashion_mnist = fetch_openml(name=\"Fashion-MNIST\", as_frame=False)\n",
        "targets = fashion_mnist.target.astype(int)\n"
      ],
      "metadata": {
        "id": "_mTJeS5abjmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = fashion_mnist.data[:60_000], targets[:60_000]\n",
        "X_test, y_test = fashion_mnist.data[60_000:], targets[60_000:]\n"
      ],
      "metadata": {
        "id": "s6_ABh8nbkfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_sample = X_train[0].reshape(28, 28)\n",
        "plt.imshow(X_sample, cmap=\"binary\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wJfpew7lbljo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fashion MNIST class labels:\n"
      ],
      "metadata": {
        "id": "opZc8hLYbm04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = [\n",
        "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "rGHGJelzbnoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names[y_train[0]]\n"
      ],
      "metadata": {
        "id": "92o8-WtVbofp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now build an MLP classifier with two hidden layers and train it using pixel values scaled to the 0–1 range.\n"
      ],
      "metadata": {
        "id": "UR5KLjpfbpX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "mlp_clf = MLPClassifier(\n",
        "    hidden_layer_sizes=[300, 100],\n",
        "    verbose=True,\n",
        "    early_stopping=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "pipeline = make_pipeline(MinMaxScaler(), mlp_clf)\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "pipeline.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "IUNA3tVvbqGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MinMaxScaler works better than StandardScaler for images because:\n",
        "- Pixel values naturally lie in [0, 255]\n",
        "- Some pixels have very low variance\n",
        "- Standard scaling would overemphasize unimportant pixels\n"
      ],
      "metadata": {
        "id": "TW4NmF9pbrIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = X_test[:15]\n",
        "mlp_clf.predict(X_new)\n"
      ],
      "metadata": {
        "id": "wSn71PnIbsIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = mlp_clf.predict_proba(X_new)\n",
        "y_proba[12]\n"
      ],
      "metadata": {
        "id": "WmiKK-UHbs_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks are often **overconfident** in their predictions.\n",
        "\n",
        "One mitigation technique is **label smoothing**, where the true class probability is reduced slightly and the remaining probability mass is distributed across other classes.\n"
      ],
      "metadata": {
        "id": "nyU0wLJAcEm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning Guidelines\n",
        "\n",
        "Neural networks are extremely flexible, but that flexibility comes with a cost: **many hyperparameters to tune**.  \n",
        "Even a simple MLP has dozens of choices, including:\n",
        "\n",
        "- Number of layers\n",
        "- Number of neurons per layer\n",
        "- Activation functions\n",
        "- Optimizer and learning rate\n",
        "- Batch size\n",
        "- Regularization methods\n",
        "\n",
        "This section gives **practical rules of thumb** to narrow down good choices.\n"
      ],
      "metadata": {
        "id": "7zyRx1nmfqMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Number of Hidden Layers\n",
        "\n",
        "For many problems, you can start with **one hidden layer** and already get reasonable results.  \n",
        "In theory, a single hidden layer with enough neurons can approximate any function.\n",
        "\n",
        "However, **deep networks are more parameter-efficient**:\n",
        "- They reuse features across layers\n",
        "- They need fewer neurons overall\n",
        "- They often generalize better\n",
        "\n",
        "### Why depth helps\n",
        "Lower layers learn simple features (edges, curves),  \n",
        "higher layers combine them into complex concepts (faces, objects).\n",
        "\n",
        "This layered structure:\n",
        "- Speeds up convergence\n",
        "- Improves generalization\n",
        "- Enables **transfer learning**\n"
      ],
      "metadata": {
        "id": "GZ5GV3l7frDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer Learning\n",
        "\n",
        "If you already trained a model on a similar task, you can reuse its lower layers.\n",
        "\n",
        "Example:\n",
        "- A face recognition model → hairstyle detection\n",
        "- Reuse early layers that detect edges and shapes\n",
        "- Train only the higher layers\n",
        "\n",
        "This dramatically reduces:\n",
        "- Training time\n",
        "- Required data\n"
      ],
      "metadata": {
        "id": "NlTtSRayfr4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practical Rule\n",
        "\n",
        "- Start with **1–2 hidden layers**\n",
        "- Increase depth **only if underfitting**\n",
        "- For very complex tasks (vision, speech), deep architectures are required\n",
        "- In practice, large models are rarely trained from scratch\n"
      ],
      "metadata": {
        "id": "KgPc2gQSfs3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Number of Neurons per Hidden Layer\n",
        "\n",
        "The input and output sizes are fixed by the task:\n",
        "- MNIST: 784 inputs, 10 outputs\n",
        "\n",
        "Hidden layers are more flexible.\n"
      ],
      "metadata": {
        "id": "ol7qiWZffto5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The “Stretch Pants” Strategy\n",
        "\n",
        "Use **slightly larger networks than needed**, then rely on:\n",
        "- Early stopping\n",
        "- ℓ2 regularization\n",
        "\n",
        "This avoids bottleneck layers that permanently lose information.\n",
        "\n",
        "Example:\n",
        "- PCA shows Fashion MNIST needs 187 dimensions\n",
        "- First hidden layer with 200 neurons avoids information loss\n"
      ],
      "metadata": {
        "id": "aCQtO5Y5fvaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rule of Thumb\n",
        "\n",
        "- Increase neurons **until overfitting starts**\n",
        "- Prefer **more layers over more neurons**\n",
        "- Bottlenecks can help denoising—but don’t overdo it\n"
      ],
      "metadata": {
        "id": "7PDXj8TPfwMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Rate\n",
        "\n",
        "The learning rate is one of the **most important hyperparameters**.\n",
        "\n",
        "A good rule:\n",
        "> Optimal learning rate ≈ **½ of the maximum stable learning rate**\n"
      ],
      "metadata": {
        "id": "96Yex4EyfxHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Rate Finder Strategy\n",
        "\n",
        "1. Start with a very small learning rate (e.g., 1e-5)\n",
        "2. Gradually increase it each iteration\n",
        "3. Plot loss vs learning rate (log scale)\n",
        "4. Pick a learning rate **slightly before loss explodes**\n"
      ],
      "metadata": {
        "id": "2qClXgJYfyAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: manually updating learning rate with Scikit-Learn\n",
        "\n",
        "mlp = MLPRegressor(\n",
        "    hidden_layer_sizes=(100,),\n",
        "    warm_start=True,\n",
        "    max_iter=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "learning_rate = 1e-5\n",
        "\n",
        "for _ in range(500):\n",
        "    mlp.learning_rate_init = learning_rate\n",
        "    mlp.partial_fit(X_train, y_train)\n",
        "    learning_rate *= 1.05\n"
      ],
      "metadata": {
        "id": "mx2AVaqefy6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ⚠️ In Scikit-Learn, dynamic learning rates require:\n",
        "- `warm_start=True`\n",
        "- `partial_fit()`\n"
      ],
      "metadata": {
        "id": "3I5KKLWFf0MG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Size\n",
        "\n",
        "Batch size affects:\n",
        "- Training speed\n",
        "- Stability\n",
        "- Generalization\n"
      ],
      "metadata": {
        "id": "1spTxf8Jf058"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Small vs Large Batches\n",
        "\n",
        "**Small batches (2–32):**\n",
        "- Better generalization\n",
        "- More stable training\n",
        "- Slower per-epoch speed\n",
        "\n",
        "**Large batches (1,000+):**\n",
        "- Faster on GPUs\n",
        "- Can be unstable early on\n",
        "- Often require learning-rate warmup\n"
      ],
      "metadata": {
        "id": "lfbLsr2Pf16R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practical Strategy\n",
        "\n",
        "1. Start with a **moderate batch size**\n",
        "2. Increase batch size if training is slow\n",
        "3. Reduce batch size if:\n",
        "   - Training is unstable\n",
        "   - Validation performance degrades\n"
      ],
      "metadata": {
        "id": "0QAbgs2Yf27o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Important Hyperparameters\n"
      ],
      "metadata": {
        "id": "DVMHkvpLgA6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "Advanced optimizers (Adam, RMSProp) often:\n",
        "- Converge faster\n",
        "- Need less tuning\n",
        "\n",
        "But:\n",
        "- They still depend heavily on learning rate\n"
      ],
      "metadata": {
        "id": "4uCeJxB2gBxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "Advanced optimizers (Adam, RMSProp) often:\n",
        "- Converge faster\n",
        "- Need less tuning\n",
        "\n",
        "But:\n",
        "- They still depend heavily on learning rate\n"
      ],
      "metadata": {
        "id": "fn9hpacIhzgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ⚠️ If you change **any hyperparameter**, re-tune the learning rate.\n"
      ],
      "metadata": {
        "id": "qVLiDgkxh02L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recommended Reading\n",
        "\n",
        "- Leslie Smith (2018): *A disciplined approach to neural network hyperparameters*\n",
        "- Google: *Deep Learning Tuning Playbook*\n",
        "- Andrew Ng: *Machine Learning Yearning*\n"
      ],
      "metadata": {
        "id": "JehfFGAih1n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "- Start simple, scale gradually\n",
        "- Prefer depth over width\n",
        "- Tune learning rate early and often\n",
        "- Use early stopping and regularization\n",
        "- Always validate on unseen data\n"
      ],
      "metadata": {
        "id": "L3OhcBdRh2e5"
      }
    }
  ]
}