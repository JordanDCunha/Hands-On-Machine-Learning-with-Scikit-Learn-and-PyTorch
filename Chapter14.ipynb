{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZWU1rfpq+xo/EaGimZMhN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanDCunha/Hands-On-Machine-Learning-with-Scikit-Learn-and-PyTorch/blob/main/Chapter14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Shakespearean Text Using a Character RNN\n",
        "\n",
        "In a famous 2015 blog post titled *‚ÄúThe Unreasonable Effectiveness of Recurrent Neural Networks‚Äù*, Andrej Karpathy showed how to train an RNN to predict the next character in a sentence.\n",
        "\n",
        "This char-RNN can then be used to generate novel text, one character at a time.\n"
      ],
      "metadata": {
        "id": "7OP82u_SOpQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of Generated Text\n",
        "\n",
        "After training on all of Shakespeare‚Äôs works, a char-RNN generated text such as:\n",
        "\n",
        "PANDARUS:\n",
        "Alas, I think he shall be come approached and the day\n",
        "When little srain would be attain‚Äôd into being never fed,\n",
        "And who is but a chain and subjects of his death,\n",
        "I should not sleep.\n"
      ],
      "metadata": {
        "id": "pYMauEz0SvHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not exactly a masterpiece, but still impressive: the model learned words, grammar, punctuation, and structure purely by predicting the next character.\n",
        "\n",
        "This is our first example of a **language model**.\n"
      ],
      "metadata": {
        "id": "97PYFwh8Sv51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Training Dataset\n",
        "\n",
        "First, we download a subset of Shakespeare‚Äôs works (about 25%) from Andrej Karpathy‚Äôs char-RNN project.\n"
      ],
      "metadata": {
        "id": "Y8IwqXy6Swvf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh-UQQ2DGliP"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import urllib.request\n",
        "\n",
        "def download_shakespeare_text():\n",
        "    path = Path(\"datasets/shakespeare/shakespeare.txt\")\n",
        "    if not path.is_file():\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        url = \"https://homl.info/shakespeare\"\n",
        "        urllib.request.urlretrieve(url, path)\n",
        "    return path.read_text()\n",
        "\n",
        "shakespeare_text = download_shakespeare_text()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs inspect the beginning of the text.\n"
      ],
      "metadata": {
        "id": "u-3nmSxKS7H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(shakespeare_text[:80])\n"
      ],
      "metadata": {
        "id": "eqcgDIXRS74z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks work with numbers, not text.  \n",
        "We must encode text into numbers by splitting it into **tokens**.\n",
        "\n",
        "Here, we use **characters** as tokens.\n"
      ],
      "metadata": {
        "id": "rptHmU3oS8np"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks work with numbers, not text.  \n",
        "We must encode text into numbers by splitting it into **tokens**.\n",
        "\n",
        "Here, we use **characters** as tokens.\n"
      ],
      "metadata": {
        "id": "HRtPRYEUS90e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(shakespeare_text.lower()))\n",
        "\"\".join(vocab)\n"
      ],
      "metadata": {
        "id": "jiPuHCpeS_CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we assign an integer ID to each character.\n"
      ],
      "metadata": {
        "id": "lO4LYe7uS_sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_id = {char: index for index, char in enumerate(vocab)}\n",
        "id_to_char = {index: char for index, char in enumerate(vocab)}\n",
        "\n",
        "char_to_id[\"a\"], id_to_char[13]\n"
      ],
      "metadata": {
        "id": "4o_2tB9VTAto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding and Decoding Text\n"
      ],
      "metadata": {
        "id": "iDwUNUK3TBjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def encode_text(text):\n",
        "    return torch.tensor([char_to_id[char] for char in text.lower()])\n",
        "\n",
        "def decode_text(char_ids):\n",
        "    return \"\".join([id_to_char[char_id.item()] for char_id in char_ids])\n"
      ],
      "metadata": {
        "id": "RReDk4fWTCYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs test these helper functions.\n"
      ],
      "metadata": {
        "id": "fkr562_VTDRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = encode_text(\"Hello, world!\")\n",
        "encoded, decode_text(encoded)\n"
      ],
      "metadata": {
        "id": "wQIkwp7sTEEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Dataset\n",
        "\n",
        "We turn the long character sequence into many overlapping windows.\n",
        "\n",
        "Inputs:\n",
        "- A window of characters\n",
        "\n",
        "Targets:\n",
        "- The same window shifted one character into the future\n"
      ],
      "metadata": {
        "id": "Mx07zai3TE6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, text, window_length):\n",
        "        self.encoded_text = encode_text(text)\n",
        "        self.window_length = window_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_text) - self.window_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self):\n",
        "            raise IndexError(\"dataset index out of range\")\n",
        "        end = idx + self.window_length\n",
        "        window = self.encoded_text[idx:end]\n",
        "        target = self.encoded_text[idx + 1:end + 1]\n",
        "        return window, target\n"
      ],
      "metadata": {
        "id": "cd0dJWxNTF6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Data Loaders\n"
      ],
      "metadata": {
        "id": "mwUGYllDTILf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_length = 50\n",
        "batch_size = 512\n",
        "\n",
        "train_set = CharDataset(shakespeare_text[:1_000_000], window_length)\n",
        "valid_set = CharDataset(shakespeare_text[1_000_000:1_060_000], window_length)\n",
        "test_set  = CharDataset(shakespeare_text[1_060_000:], window_length)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size)\n",
        "test_loader  = DataLoader(test_set, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "EAL8viSbTJV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each batch contains shuffled windows and their shifted targets.\n",
        "\n",
        "The window length limits how far back the model can learn dependencies.\n"
      ],
      "metadata": {
        "id": "d0EsByovTKd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why We Need Embeddings\n",
        "\n",
        "Token IDs are arbitrary numbers ‚Äî nearby IDs are not necessarily similar.\n",
        "\n",
        "One-hot encoding fixes this but scales poorly for large vocabularies.\n",
        "\n",
        "**Embeddings** solve this by learning dense vector representations.\n"
      ],
      "metadata": {
        "id": "ae9O5X1kTMz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What Is an Embedding?\n",
        "\n",
        "An embedding maps each category to a trainable dense vector.\n",
        "\n",
        "Embeddings are learned during training and capture semantic relationships.\n"
      ],
      "metadata": {
        "id": "jlMNqesTTO7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Using nn.Embedding\n"
      ],
      "metadata": {
        "id": "oQSe6O-ITTlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "embed = nn.Embedding(5, 3)\n",
        "embed(torch.tensor([[3, 2], [0, 2]]))\n"
      ],
      "metadata": {
        "id": "J0uKoHh9TUUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding layers are equivalent to one-hot encoding followed by a linear layer,\n",
        "but far more efficient.\n"
      ],
      "metadata": {
        "id": "NflD9GRjTbny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Char-RNN Model\n",
        "\n",
        "We use:\n",
        "- An embedding layer\n",
        "- A two-layer GRU\n",
        "- A linear output layer\n"
      ],
      "metadata": {
        "id": "6MxlDAdcTcqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShakespeareModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_layers=2, embed_dim=10, hidden_dim=128,\n",
        "                 dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n",
        "                          batch_first=True, dropout=dropout)\n",
        "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        embeddings = self.embed(X)\n",
        "        outputs, _states = self.gru(embeddings)\n",
        "        return self.output(outputs).permute(0, 2, 1)\n"
      ],
      "metadata": {
        "id": "Wgx6JzdWTd07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model = ShakespeareModel(len(vocab)).to(device)\n"
      ],
      "metadata": {
        "id": "VZoeeToaTe-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is permuted because nn.CrossEntropyLoss expects\n",
        "the class dimension to be second.\n"
      ],
      "metadata": {
        "id": "GwdQ_1n9UEfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting the Next Character\n"
      ],
      "metadata": {
        "id": "gQSISJjvUFeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "text = \"To be or not to b\"\n",
        "encoded_text = encode_text(text).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    Y_logits = model(encoded_text)\n",
        "    predicted_char_id = Y_logits[0, :, -1].argmax().item()\n",
        "    predicted_char = id_to_char[predicted_char_id]\n",
        "\n",
        "predicted_char\n"
      ],
      "metadata": {
        "id": "NHtV1KdAUGPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating New Text\n",
        "\n",
        "Greedy decoding often leads to repetition.\n",
        "\n",
        "Instead, we **sample** characters using predicted probabilities.\n"
      ],
      "metadata": {
        "id": "JDkpiaWkUUIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def next_char(model, text, temperature=1):\n",
        "    encoded_text = encode_text(text).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(encoded_text)[0, :, -1]\n",
        "        probs = F.softmax(logits / temperature, dim=-1)\n",
        "        char_id = torch.multinomial(probs, num_samples=1).item()\n",
        "    return id_to_char[char_id]\n"
      ],
      "metadata": {
        "id": "NA2DwrwsUVP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_text(model, text, n_chars=80, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(model, text, temperature)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "JpTFcrP4UWSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Effect of Temperature\n"
      ],
      "metadata": {
        "id": "osClbbfFUYUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(extend_text(model, \"To be or not to b\", temperature=0.01))\n"
      ],
      "metadata": {
        "id": "UANqKOnoUa5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(extend_text(model, \"To be or not to b\", temperature=0.4))\n"
      ],
      "metadata": {
        "id": "Usnm5nNkUc0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(extend_text(model, \"To be or not to b\", temperature=100))\n"
      ],
      "metadata": {
        "id": "ag3N3c8bUd0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low temperature ‚Üí repetitive but coherent  \n",
        "Medium temperature ‚Üí best balance  \n",
        "High temperature ‚Üí chaos\n"
      ],
      "metadata": {
        "id": "-bQgKBMJUgVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Notes\n",
        "\n",
        "- Char-RNNs learn surprisingly rich representations\n",
        "- They are limited by window length\n",
        "- Sampling strategy strongly affects output quality\n",
        "\n",
        "Next up: **sentiment analysis**.\n"
      ],
      "metadata": {
        "id": "YyNfhs0aUhUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis Using Hugging Face Libraries\n",
        "\n",
        "One of the most common applications of NLP is text classification‚Äîespecially sentiment analysis.\n",
        "\n",
        "If image classification on the MNIST dataset is the ‚ÄúHello, world!‚Äù of computer vision, then sentiment analysis on the IMDb reviews dataset is the ‚ÄúHello, world!‚Äù of natural language processing.\n",
        "\n",
        "The IMDb dataset consists of 50,000 movie reviews in English (25,000 for training, 25,000 for testing), each labeled as negative (0) or positive (1).\n",
        "\n",
        "It is simple enough to run on a laptop but challenging enough to be interesting.\n"
      ],
      "metadata": {
        "id": "zBx3QfyAVNSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the IMDb Dataset\n",
        "\n",
        "We will use the Hugging Face Datasets library to download IMDb.\n"
      ],
      "metadata": {
        "id": "DSGKIL6jVY8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "split = imdb_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
        "imdb_train_set, imdb_valid_set = split[\"train\"], split[\"test\"]\n",
        "imdb_test_set = imdb_dataset[\"test\"]\n"
      ],
      "metadata": {
        "id": "xstbI5UsVYY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting the Dataset\n"
      ],
      "metadata": {
        "id": "P71zo_zAVb4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_train_set[1][\"text\"], imdb_train_set[1][\"label\"]\n"
      ],
      "metadata": {
        "id": "C1f8JD9JVdmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_train_set[16][\"text\"], imdb_train_set[16][\"label\"]\n"
      ],
      "metadata": {
        "id": "nJa_e8cXVeY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first review is clearly positive.  \n",
        "The second contains mixed sentiment but ends negatively.\n",
        "\n",
        "This is a nontrivial classification task.\n"
      ],
      "metadata": {
        "id": "X5sLUPcoVc8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization with Subword Models\n",
        "\n",
        "A simple character RNN would struggle here.\n",
        "\n",
        "We need a better tokenization strategy that can handle rare words and morphology.\n"
      ],
      "metadata": {
        "id": "hiy-H-NYVgR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Byte Pair Encoding (BPE)\n",
        "\n",
        "BPE starts with characters and repeatedly merges the most frequent adjacent pairs until a target vocabulary size is reached.\n"
      ],
      "metadata": {
        "id": "7rwy-YNEVhxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tokenizers\n",
        "\n",
        "bpe_model = tokenizers.models.BPE(unk_token=\"<unk>\")\n",
        "bpe_tokenizer = tokenizers.Tokenizer(bpe_model)\n",
        "bpe_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
        "\n",
        "special_tokens = [\"<pad>\", \"<unk>\"]\n",
        "bpe_trainer = tokenizers.trainers.BpeTrainer(\n",
        "    vocab_size=1000,\n",
        "    special_tokens=special_tokens\n",
        ")\n",
        "\n",
        "train_reviews = [review[\"text\"].lower() for review in imdb_train_set]\n",
        "bpe_tokenizer.train_from_iterator(train_reviews, bpe_trainer)\n"
      ],
      "metadata": {
        "id": "aqskdgB9Vjw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the BPE Tokenizer\n"
      ],
      "metadata": {
        "id": "QgrxvTFtVk30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "some_review = \"what an awesome movie!\"\n",
        "bpe_encoding = bpe_tokenizer.encode(some_review)\n",
        "bpe_encoding.tokens, bpe_encoding.ids\n"
      ],
      "metadata": {
        "id": "Um2tuxDsVl_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequent words become single tokens, rare words are split.\n",
        "\n",
        "Unknown characters (like emojis) are replaced by `<unk>`.\n"
      ],
      "metadata": {
        "id": "aH93Nnd5VorC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer.decode(bpe_encoding.ids)\n"
      ],
      "metadata": {
        "id": "3koXt7WoVrNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding and Truncation\n"
      ],
      "metadata": {
        "id": "5H3I_zbwVtDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "bpe_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\n",
        "bpe_tokenizer.enable_truncation(max_length=500)\n",
        "\n",
        "bpe_encodings = bpe_tokenizer.encode_batch(train_reviews[:3])\n",
        "bpe_batch_ids = torch.tensor([e.ids for e in bpe_encodings])\n",
        "bpe_batch_ids\n"
      ],
      "metadata": {
        "id": "gmPto3Y3Vs2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding ensures equal-length sequences.\n",
        "\n",
        "Attention masks indicate which tokens are padding.\n"
      ],
      "metadata": {
        "id": "9bCpMJpAVvaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask = torch.tensor([e.attention_mask for e in bpe_encodings])\n",
        "lengths = attention_mask.sum(dim=-1)\n",
        "attention_mask, lengths\n"
      ],
      "metadata": {
        "id": "b5E-kP-XVwhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Byte-Level BPE (BBPE)\n",
        "\n",
        "Whitespace tokenization loses space information.\n",
        "\n",
        "ByteLevel pre-tokenization preserves spaces and supports all Unicode bytes.\n"
      ],
      "metadata": {
        "id": "VJotmBuxVxVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordPiece Tokenization\n",
        "\n",
        "WordPiece is similar to BPE but uses a likelihood-based scoring function.\n",
        "\n",
        "It often produces shorter sequences and more meaningful splits.\n"
      ],
      "metadata": {
        "id": "8vXaYFbAVybA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unigram Language Model Tokenization\n",
        "\n",
        "Unigram LM starts with a large vocabulary and removes tokens that reduce likelihood the least.\n",
        "\n",
        "It works well for languages without spaces.\n"
      ],
      "metadata": {
        "id": "4LwlDU4bV-Jh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reusing Pretrained Tokenizers\n"
      ],
      "metadata": {
        "id": "psoVjf7PWBRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "gpt2_tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_encoding = gpt2_tokenizer(\n",
        "    train_reviews[:3],\n",
        "    truncation=True,\n",
        "    max_length=500\n",
        ")\n"
      ],
      "metadata": {
        "id": "uBj1v479WCEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_token_ids = gpt2_encoding[\"input_ids\"][0][:10]\n",
        "gpt2_tokenizer.decode(gpt2_token_ids)\n"
      ],
      "metadata": {
        "id": "KMijzqxEWDxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gpt2_token_ids = gpt2_encoding[\"input_ids\"][0][:10]\n",
        "gpt2_tokenizer.decode(gpt2_token_ids)\n"
      ],
      "metadata": {
        "id": "byUzTrLvWEnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    \"bert-base-uncased\"\n",
        ")\n",
        "\n",
        "bert_encoding = bert_tokenizer(\n",
        "    train_reviews[:3],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=500,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "bert_encoding[\"input_ids\"], bert_encoding[\"attention_mask\"]\n"
      ],
      "metadata": {
        "id": "hFNHTJcEWFmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader Tokenization with collate_fn\n"
      ],
      "metadata": {
        "id": "w8rPZZnWWGTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch, tokenizer=bert_tokenizer):\n",
        "    reviews = [r[\"text\"] for r in batch]\n",
        "    labels = [[r[\"label\"]] for r in batch]\n",
        "    encodings = tokenizer(\n",
        "        reviews,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=200,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)\n",
        "    return encodings, labels\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "imdb_train_loader = DataLoader(\n",
        "    imdb_train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n"
      ],
      "metadata": {
        "id": "RuhRWlYQWHLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis Model (GRU)\n"
      ],
      "metadata": {
        "id": "K2N9ZrdiWZ_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentAnalysisModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_layers=2, embed_dim=128,\n",
        "                 hidden_dim=64, pad_id=0, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
        "        self.gru = nn.GRU(\n",
        "            embed_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.output = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, encodings):\n",
        "        embeddings = self.embed(encodings[\"input_ids\"])\n",
        "        _, hidden_states = self.gru(embeddings)\n",
        "        return self.output(hidden_states[-1])\n"
      ],
      "metadata": {
        "id": "J6KEyRiaWazX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packed Sequences to Ignore Padding\n"
      ],
      "metadata": {
        "id": "ANMUQd2QWb3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "def forward(self, encodings):\n",
        "    embeddings = self.embed(encodings[\"input_ids\"])\n",
        "    lengths = encodings[\"attention_mask\"].sum(dim=1)\n",
        "    packed = pack_padded_sequence(\n",
        "        embeddings,\n",
        "        lengths.cpu(),\n",
        "        batch_first=True,\n",
        "        enforce_sorted=False\n",
        "    )\n",
        "    _, hidden_states = self.gru(packed)\n",
        "    return self.output(hidden_states[-1])\n"
      ],
      "metadata": {
        "id": "GW5DgjEMWcmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bidirectional GRU\n",
        "\n",
        "Bidirectional RNNs read sequences forward and backward.\n",
        "\n",
        "Hidden sizes must be doubled.\n"
      ],
      "metadata": {
        "id": "c5c6P30WWdfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "self.gru = nn.GRU(\n",
        "    embed_dim,\n",
        "    hidden_dim,\n",
        "    num_layers=n_layers,\n",
        "    batch_first=True,\n",
        "    dropout=dropout,\n",
        "    bidirectional=True\n",
        ")\n",
        "\n",
        "self.output = nn.Linear(2 * hidden_dim, 1)\n"
      ],
      "metadata": {
        "id": "6fzYvSTnWedf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reusing Pretrained BERT Embeddings\n"
      ],
      "metadata": {
        "id": "uY1UDDjLWhcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "class SentimentAnalysisModelPreEmbeds(nn.Module):\n",
        "    def __init__(self, pretrained_embeddings, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        weights = pretrained_embeddings.weight.data\n",
        "        self.embed = nn.Embedding.from_pretrained(weights, freeze=True)\n",
        "        embed_dim = weights.shape[-1]\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.output = nn.Linear(hidden_dim, 1)\n"
      ],
      "metadata": {
        "id": "dBtIlRQqWmhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Full BERT for Classification\n"
      ],
      "metadata": {
        "id": "QvIu79nNWn32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentAnalysisModelBert(nn.Module):\n",
        "    def __init__(self, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.bert = transformers.AutoModel.from_pretrained(\n",
        "            \"bert-base-uncased\"\n",
        "        )\n",
        "        embed_dim = self.bert.config.hidden_size\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.output = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, encodings):\n",
        "        x = self.bert(**encodings).last_hidden_state\n",
        "        lengths = encodings[\"attention_mask\"].sum(dim=1)\n",
        "        packed = pack_padded_sequence(\n",
        "            x, lengths.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "        _, hidden_states = self.gru(packed)\n",
        "        return self.output(hidden_states[-1])\n"
      ],
      "metadata": {
        "id": "Xtt4AOaPWpj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BertForSequenceClassification\n"
      ],
      "metadata": {
        "id": "pRDONVIUW7C8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "bert_for_binary_clf = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2,\n",
        "    dtype=torch.float16\n",
        ")\n"
      ],
      "metadata": {
        "id": "wWz9ALIvW-bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer API\n"
      ],
      "metadata": {
        "id": "Y55eteymW_1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_batch(batch):\n",
        "    return bert_tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=200\n",
        "    )\n",
        "\n",
        "tok_imdb_train_set = imdb_train_set.map(tokenize_batch, batched=True)\n",
        "tok_imdb_valid_set = imdb_valid_set.map(tokenize_batch, batched=True)\n"
      ],
      "metadata": {
        "id": "EZecOGaqXAyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(pred):\n",
        "    return {\n",
        "        \"accuracy\": (pred.label_ids == pred.predictions.argmax(-1)).mean()\n",
        "    }\n"
      ],
      "metadata": {
        "id": "dwYlIX5XXD19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    output_dir=\"my_imdb_model\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "1rjsdBmgXBhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, DataCollatorWithPadding\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=bert_for_binary_clf,\n",
        "    args=train_args,\n",
        "    train_dataset=tok_imdb_train_set,\n",
        "    eval_dataset=tok_imdb_valid_set,\n",
        "    compute_metrics=compute_accuracy,\n",
        "    data_collator=DataCollatorWithPadding(bert_tokenizer)\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "vdPvOGI5XFSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipelines API\n"
      ],
      "metadata": {
        "id": "2hlWn6MgXH5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    truncation=True,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "classifier(train_reviews[:10])\n"
      ],
      "metadata": {
        "id": "CUSMvBFzXJWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Notes\n",
        "\n",
        "- Pretrained transformers dominate modern NLP\n",
        "- Tokenization matters deeply\n",
        "- Pipelines give instant results\n",
        "- Fine-tuning gives best performance\n",
        "- Bias must always be evaluated\n",
        "\n",
        "Next: **Neural Machine Translation** üöÄ\n"
      ],
      "metadata": {
        "id": "7Wn-1E2iXKnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An Encoder-Decoder Network for Neural Machine Translation\n",
        "\n",
        "Let‚Äôs begin with a relatively simple sequence-to-sequence NMT model that will translate English text to Spanish (see Figure 14-5).\n"
      ],
      "metadata": {
        "id": "XKqEGqKEYMax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture\n",
        "\n",
        "English texts are fed as inputs to the encoder, and the decoder outputs the Spanish translations.\n",
        "\n",
        "The Spanish translations are also used as inputs to the decoder during training, but shifted back by one step. In other words, during training the decoder is given as input the token that it should have output at the previous step, regardless of what it actually output.\n",
        "\n",
        "This is called **teacher forcing**, a technique that significantly speeds up training and improves the model‚Äôs performance.\n",
        "\n",
        "For the very first token, the decoder is given the start-of-sequence (SoS) token (`\"<s>\"`), and the decoder is expected to end the text with an end-of-sequence (EoS) token (`\"</s>\"`).\n"
      ],
      "metadata": {
        "id": "mrPfBCG0YNo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each token is initially represented by its ID (e.g., 4553 for the token ‚Äúsoccer‚Äù). An `nn.Embedding` layer returns the token embedding, which is fed to the encoder and decoder.\n",
        "\n",
        "At each step, the decoder‚Äôs dense output layer (`nn.Linear`) outputs a logit score for each token in the output vocabulary (Spanish).\n",
        "\n",
        "Passing these logits through softmax gives a probability distribution over all tokens. This is a standard classification task, and the model is trained using `nn.CrossEntropyLoss`.\n"
      ],
      "metadata": {
        "id": "Nn7dVzVkYPrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each token is initially represented by its ID (e.g., 4553 for the token ‚Äúsoccer‚Äù). An `nn.Embedding` layer returns the token embedding, which is fed to the encoder and decoder.\n",
        "\n",
        "At each step, the decoder‚Äôs dense output layer (`nn.Linear`) outputs a logit score for each token in the output vocabulary (Spanish).\n",
        "\n",
        "Passing these logits through softmax gives a probability distribution over all tokens. This is a standard classification task, and the model is trained using `nn.CrossEntropyLoss`.\n"
      ],
      "metadata": {
        "id": "Typkx_4QYQ6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TIP\n",
        "\n",
        "In a 2015 paper, Samy Bengio et al. proposed gradually switching from feeding the decoder the previous target token to feeding it the previous output token during training.\n"
      ],
      "metadata": {
        "id": "D7lblnq6YSEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "We will use the Tatoeba Challenge dataset via the Hugging Face Datasets library.\n",
        "\n",
        "The training set is large, so we use the validation set for training and split it into training and validation subsets.\n"
      ],
      "metadata": {
        "id": "omlCW0leYTLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "nmt_original_valid_set, nmt_test_set = load_dataset(\n",
        "    path=\"ageron/tatoeba_mt_train\",\n",
        "    name=\"eng-spa\",\n",
        "    split=[\"validation\", \"test\"]\n",
        ")\n",
        "\n",
        "split = nmt_original_valid_set.train_test_split(train_size=0.8, seed=42)\n",
        "nmt_train_set, nmt_valid_set = split[\"train\"], split[\"test\"]\n"
      ],
      "metadata": {
        "id": "of8v2QSeYUCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each sample contains an English sentence and its Spanish translation.\n"
      ],
      "metadata": {
        "id": "XkBoqvO8YYzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nmt_train_set[0]\n"
      ],
      "metadata": {
        "id": "raWbhhSiYaTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Shared BPE Tokenizer\n",
        "\n",
        "Since English and Spanish share many words and subwords, we use a single tokenizer.\n",
        "\n",
        "We train a BPE tokenizer on both English and Spanish text.\n"
      ],
      "metadata": {
        "id": "BBDDHnopYbRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tokenizers\n",
        "\n",
        "def train_eng_spa():\n",
        "    for pair in nmt_train_set:\n",
        "        yield pair[\"source_text\"]\n",
        "        yield pair[\"target_text\"]\n",
        "\n",
        "max_length = 256\n",
        "vocab_size = 10_000\n",
        "\n",
        "nmt_tokenizer_model = tokenizers.models.BPE(unk_token=\"<unk>\")\n",
        "nmt_tokenizer = tokenizers.Tokenizer(nmt_tokenizer_model)\n",
        "\n",
        "nmt_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\n",
        "nmt_tokenizer.enable_truncation(max_length=max_length)\n",
        "nmt_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
        "\n",
        "nmt_tokenizer_trainer = tokenizers.trainers.BpeTrainer(\n",
        "    vocab_size=vocab_size,\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]\n",
        ")\n",
        "\n",
        "nmt_tokenizer.train_from_iterator(train_eng_spa(), nmt_tokenizer_trainer)\n"
      ],
      "metadata": {
        "id": "5Jjan9TuYcR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Tokenizer\n"
      ],
      "metadata": {
        "id": "HwYwjUV2Ydon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nmt_tokenizer.encode(\"I like soccer\").ids\n"
      ],
      "metadata": {
        "id": "_LpgeV5RYegX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nmt_tokenizer.encode(\"<s> Me gusta el f√∫tbol\").ids\n"
      ],
      "metadata": {
        "id": "GImxhlPPYfjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Class for Tokenized Pairs\n",
        "\n",
        "We store token IDs and attention masks for both source and target sequences.\n"
      ],
      "metadata": {
        "id": "QKBuAsBjYgkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from collections import namedtuple\n",
        "\n",
        "fields = [\"src_token_ids\", \"src_mask\", \"tgt_token_ids\", \"tgt_mask\"]\n",
        "\n",
        "class NmtPair(namedtuple(\"NmtPairBase\", fields)):\n",
        "    def to(self, device):\n",
        "        return NmtPair(\n",
        "            self.src_token_ids.to(device),\n",
        "            self.src_mask.to(device),\n",
        "            self.tgt_token_ids.to(device),\n",
        "            self.tgt_mask.to(device)\n",
        "        )\n"
      ],
      "metadata": {
        "id": "TaP6oHeGYhdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def nmt_collate_fn(batch):\n",
        "    src_texts = [pair[\"source_text\"] for pair in batch]\n",
        "    tgt_texts = [f\"<s> {pair['target_text']} </s>\" for pair in batch]\n",
        "\n",
        "    src_encodings = nmt_tokenizer.encode_batch(src_texts)\n",
        "    tgt_encodings = nmt_tokenizer.encode_batch(tgt_texts)\n",
        "\n",
        "    src_token_ids = torch.tensor([enc.ids for enc in src_encodings])\n",
        "    tgt_token_ids = torch.tensor([enc.ids for enc in tgt_encodings])\n",
        "\n",
        "    src_mask = torch.tensor([enc.attention_mask for enc in src_encodings])\n",
        "    tgt_mask = torch.tensor([enc.attention_mask for enc in tgt_encodings])\n",
        "\n",
        "    inputs = NmtPair(\n",
        "        src_token_ids,\n",
        "        src_mask,\n",
        "        tgt_token_ids[:, :-1],\n",
        "        tgt_mask[:, :-1]\n",
        "    )\n",
        "\n",
        "    labels = tgt_token_ids[:, 1:]\n",
        "    return inputs, labels\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "nmt_train_loader = DataLoader(\n",
        "    nmt_train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=nmt_collate_fn\n",
        ")\n",
        "\n",
        "nmt_valid_loader = DataLoader(\n",
        "    nmt_valid_set,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=nmt_collate_fn\n",
        ")\n",
        "\n",
        "nmt_test_loader = DataLoader(\n",
        "    nmt_test_set,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=nmt_collate_fn\n",
        ")\n"
      ],
      "metadata": {
        "id": "3xAINTIYZEtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder-Decoder GRU Model\n"
      ],
      "metadata": {
        "id": "bX_eMbCtZHD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "class NmtModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=512, pad_id=0,\n",
        "                 hidden_dim=512, n_layers=2):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
        "        self.encoder = nn.GRU(\n",
        "            embed_dim, hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.GRU(\n",
        "            embed_dim, hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, pair):\n",
        "        src_embeddings = self.embed(pair.src_token_ids)\n",
        "        tgt_embeddings = self.embed(pair.tgt_token_ids)\n",
        "\n",
        "        src_lengths = pair.src_mask.sum(dim=1)\n",
        "        src_packed = pack_padded_sequence(\n",
        "            src_embeddings,\n",
        "            src_lengths.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        _, hidden_states = self.encoder(src_packed)\n",
        "        outputs, _ = self.decoder(tgt_embeddings, hidden_states)\n",
        "\n",
        "        return self.output(outputs).permute(0, 2, 1)\n"
      ],
      "metadata": {
        "id": "5wYHq0FVZII7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "vocab_size = nmt_tokenizer.get_vocab_size()\n",
        "nmt_model = NmtModel(vocab_size).to(device)\n"
      ],
      "metadata": {
        "id": "vVyYs5mLZJWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function\n",
        "\n",
        "Padding tokens should be ignored.\n"
      ],
      "metadata": {
        "id": "YgXRj1j5ZKBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xentropy = nn.CrossEntropyLoss(ignore_index=0)\n"
      ],
      "metadata": {
        "id": "gntR-05kZK3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation Helper Function\n"
      ],
      "metadata": {
        "id": "16l_LY7gZMhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, src_text, max_length=20, eos_id=3):\n",
        "    tgt_text = \"\"\n",
        "    for index in range(max_length):\n",
        "        batch, _ = nmt_collate_fn([{\n",
        "            \"source_text\": src_text,\n",
        "            \"target_text\": tgt_text\n",
        "        }])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(batch.to(device))\n",
        "            token_ids = logits.argmax(dim=1)\n",
        "            next_token_id = token_ids[0, index]\n",
        "\n",
        "        next_token = nmt_tokenizer.id_to_token(next_token_id)\n",
        "        tgt_text += \" \" + next_token\n",
        "\n",
        "        if next_token_id == eos_id:\n",
        "            break\n",
        "\n",
        "    return tgt_text\n"
      ],
      "metadata": {
        "id": "e30-tVaaZNhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Translator\n"
      ],
      "metadata": {
        "id": "enIzzU_YZRvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nmt_model.eval()\n",
        "translate(nmt_model, \"I like soccer.\")\n"
      ],
      "metadata": {
        "id": "rE9QPhp9ZTi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Optimizations\n",
        "\n",
        "- Sampled softmax\n",
        "- Adaptive softmax (`nn.AdaptiveLogSoftmaxWithLoss`)\n",
        "- Weight tying between embedding and output layers\n"
      ],
      "metadata": {
        "id": "IoSlHbuTZUqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations\n",
        "\n",
        "The model struggles with long sentences and loses details.\n"
      ],
      "metadata": {
        "id": "vBQcBeNxZVnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "longer_text = \"I like to play soccer with my friends.\"\n",
        "translate(nmt_model, longer_text)\n"
      ],
      "metadata": {
        "id": "Oy8YtgJIZWhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Possible Improvements\n",
        "\n",
        "- Larger dataset\n",
        "- More GRU layers\n",
        "- Bidirectional encoder\n",
        "- Beam search\n",
        "- Attention mechanisms (next section!)\n"
      ],
      "metadata": {
        "id": "dz5Vo-hyZXY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam Search\n",
        "\n",
        "To translate an English text to Spanish, we call our model several times, producing one word at a time. Unfortunately, this means that when the model makes one mistake, it is stuck with it for the rest of the translation, which can cause more errors, making the translation worse and worse.\n"
      ],
      "metadata": {
        "id": "8DsDpuDPZ1wu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, suppose we want to translate ‚ÄúI like soccer‚Äù, and the model correctly starts with ‚ÄúMe‚Äù, but then predicts ‚Äúgustan‚Äù (plural) instead of ‚Äúgusta‚Äù (singular). This mistake is understandable, since ‚ÄúMe gustan‚Äù is the correct way to start translating ‚ÄúI like‚Äù in many cases.\n",
        "\n",
        "Once the model has made this mistake, it is stuck with ‚Äúgustan‚Äù. It then reasonably adds ‚Äúlos‚Äù, which is the plural for ‚Äúthe‚Äù. But since the model never saw ‚Äúlos f√∫tbol‚Äù in the training data (soccer is singular, not plural), the model tries to find something reasonable to add, and given the context it adds ‚Äújugadores‚Äù, which means ‚Äúthe players‚Äù.\n",
        "\n",
        "So ‚ÄúI like soccer‚Äù gets translated to ‚ÄúI like the players‚Äù. One error caused a chain of errors.\n"
      ],
      "metadata": {
        "id": "_6GOb0KkZ22i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Beam Search?\n",
        "\n",
        "How can we give the model a chance to go back and fix mistakes it made earlier?\n",
        "\n",
        "One of the most common solutions is **beam search**: it keeps track of a short list of the *k* most promising output sequences (say, the top three), and at each decoder step it tries to extend each of them by one word, keeping only the *k* most likely sequences.\n",
        "\n",
        "The parameter *k* is called the **beam width**.\n"
      ],
      "metadata": {
        "id": "WKkaOAEQZ33b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Beam Width = 3\n",
        "\n",
        "Suppose you translate the sentence ‚ÄúI like soccer‚Äù using beam search with a beam width of three (see Figure 14-7).\n",
        "\n",
        "At the first decoder step, the model outputs an estimated probability for each possible first word. Suppose the top three words are:\n",
        "\n",
        "- ‚ÄúMe‚Äù (75%)\n",
        "- ‚Äúa‚Äù (3%)\n",
        "- ‚Äúcomo‚Äù (1%)\n",
        "\n",
        "These become our initial beam.\n"
      ],
      "metadata": {
        "id": "Wy-Ma0eiZ4u2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, the model predicts the next word for each of these sentences.\n",
        "\n",
        "For ‚ÄúMe‚Äù, it might output:\n",
        "- ‚Äúgustan‚Äù (36%)\n",
        "- ‚Äúgusta‚Äù (32%)\n",
        "- ‚Äúencanta‚Äù (16%)\n",
        "\n",
        "These are **conditional probabilities**, given that the sentence starts with ‚ÄúMe‚Äù.\n",
        "\n",
        "Since the vocabulary may contain 10,000 tokens, this results in 10,000 candidate continuations per beam entry.\n"
      ],
      "metadata": {
        "id": "yhN2dcLCZ7RF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now compute the probability of each two-word sentence by multiplying probabilities.\n",
        "\n",
        "For example:\n",
        "- P(\"Me\") = 75%\n",
        "- P(\"gustan\" | \"Me\") = 36%\n",
        "\n",
        "So:\n",
        "- P(\"Me gustan\") = 0.75 √ó 0.36 = 27%\n"
      ],
      "metadata": {
        "id": "g61gRRHZZ81W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now compute the probability of each two-word sentence by multiplying probabilities.\n",
        "\n",
        "For example:\n",
        "- P(\"Me\") = 75%\n",
        "- P(\"gustan\" | \"Me\") = 36%\n",
        "\n",
        "So:\n",
        "- P(\"Me gustan\") = 0.75 √ó 0.36 = 27%\n"
      ],
      "metadata": {
        "id": "H8WqVqkEaMqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After computing probabilities for all 30,000 two-word sequences (3 √ó 10,000), we keep only the top three:\n",
        "\n",
        "- ‚ÄúMe gustan‚Äù (27%)\n",
        "- ‚ÄúMe gusta‚Äù (24%)\n",
        "- ‚ÄúMe encanta‚Äù (12%)\n",
        "\n",
        "Even though ‚ÄúMe gustan‚Äù is currently the best, ‚ÄúMe gusta‚Äù is still alive.\n"
      ],
      "metadata": {
        "id": "xjS-8Y3aaNw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuing the Search\n",
        "\n",
        "Repeating the process, the top candidates may become:\n",
        "\n",
        "- ‚ÄúMe gustan los‚Äù (10%)\n",
        "- ‚ÄúMe gusta el‚Äù (8%)\n",
        "- ‚ÄúMe gusta mucho‚Äù (2%)\n",
        "\n",
        "At the next step:\n",
        "- ‚ÄúMe gusta el f√∫tbol‚Äù (6%)\n",
        "- ‚ÄúMe gusta mucho el‚Äù (1%)\n",
        "- ‚ÄúMe gusta el deporte‚Äù (0.2%)\n",
        "\n",
        "Notice that ‚ÄúMe gustan‚Äù has now been eliminated, and the correct translation is winning.\n"
      ],
      "metadata": {
        "id": "p8f7WGDBaOqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Takeaway\n",
        "\n",
        "Beam search improves translation quality **without any extra training**, simply by using the model more intelligently at inference time.\n"
      ],
      "metadata": {
        "id": "9sFpM2M-aPjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practical Implementations\n",
        "\n",
        "The notebook for this chapter contains a very simple `beam_search()` function.\n",
        "\n",
        "In practice, you will usually want to use the implementation provided by the `GenerationMixin` class in the Transformers library.\n"
      ],
      "metadata": {
        "id": "moyNwwElaQjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is where the text generation models from the Transformers library get their `generate()` method.\n",
        "\n",
        "It supports:\n",
        "- `num_beams` for beam width\n",
        "- `do_sample` for probabilistic sampling\n",
        "- combinations of multiple decoding strategies\n"
      ],
      "metadata": {
        "id": "q1wkxR9zaSlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more details, see:\n",
        "https://homl.info/hfgen\n"
      ],
      "metadata": {
        "id": "5gujbmXMaifQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Output with Beam Search\n"
      ],
      "metadata": {
        "id": "PUBSb2V0ajdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beam_search(nmt_model, longer_text, beam_width=3)\n"
      ],
      "metadata": {
        "id": "adkq2-PJak4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This produces a correct translation:\n"
      ],
      "metadata": {
        "id": "BR8_dY3Aap7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "' Me gusta jugar al f√∫tbol con mis amigos . </s>'\n"
      ],
      "metadata": {
        "id": "vLIkIB_karZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations on Long Sentences\n",
        "\n",
        "Unfortunately, the model still struggles with long sentences.\n"
      ],
      "metadata": {
        "id": "HaDLpjCgauqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "longest_text = \"I like to play soccer with my friends at the beach.\"\n",
        "beam_search(nmt_model, longest_text, beam_width=3)\n"
      ],
      "metadata": {
        "id": "LsA6B2kAav_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This produces:\n"
      ],
      "metadata": {
        "id": "b5sX0Qo6aw2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "' Me gusta jugar con jugar con los jug adores de la playa . </s>'\n"
      ],
      "metadata": {
        "id": "IETeLNCFaxuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which translates to:\n",
        "\n",
        "‚ÄúI like to play with play with the players of the beach‚Äù.\n",
        "\n",
        "The core issue remains the limited short-term memory of RNNs.\n"
      ],
      "metadata": {
        "id": "CXNQ8od4ayx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What‚Äôs Next?\n",
        "\n",
        "Attention mechanisms are the game-changing innovation that addressed this problem.\n"
      ],
      "metadata": {
        "id": "1Hw91i8Qazke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Mechanisms\n",
        "\n",
        "Consider the path from the word ‚Äúsoccer‚Äù to its translation ‚Äúf√∫tbol‚Äù back in Figure 14-5: it is quite long! This means that a representation of this word (along with all the other words) needs to be carried over many steps before it is actually used. Can‚Äôt we make this path shorter?\n"
      ],
      "metadata": {
        "id": "hgyUdYSOmvdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was the core idea in a landmark 2014 paper by Dzmitry Bahdanau et al., where the authors introduced a technique that allowed the decoder to focus on the appropriate words (as encoded by the encoder) at each time step.\n"
      ],
      "metadata": {
        "id": "bJNcAowTmwis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, at the time step where the decoder needs to output the word ‚Äúf√∫tbol‚Äù, it will focus its attention on the word ‚Äúsoccer‚Äù. This means that the path from an input word to its translation is now much shorter, so the short-term memory limitations of RNNs have much less impact.\n"
      ],
      "metadata": {
        "id": "Xo7Up3ZUmxem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, at the time step where the decoder needs to output the word ‚Äúf√∫tbol‚Äù, it will focus its attention on the word ‚Äúsoccer‚Äù. This means that the path from an input word to its translation is now much shorter, so the short-term memory limitations of RNNs have much less impact.\n"
      ],
      "metadata": {
        "id": "2vT5XpJcmydG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder-Decoder with Attention\n",
        "\n",
        "Figure 14-8 shows our encoder-decoder model with an added attention mechanism.\n"
      ],
      "metadata": {
        "id": "yegNYku5m0D9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- On the left, you have the encoder and the decoder (the encoder is bidirectional).\n",
        "- Instead of sending only the encoder‚Äôs final hidden state to the decoder, we now send **all encoder outputs**.\n"
      ],
      "metadata": {
        "id": "OO3eAWWxm06X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the decoder cannot deal with all encoder outputs at once, they are aggregated.\n",
        "\n",
        "At each time step, the decoder computes a **weighted sum** of all encoder outputs. This determines which words the decoder focuses on.\n"
      ],
      "metadata": {
        "id": "aOejKlPFm1ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The weight Œ±(t,i) is the weight of the *i*th encoder output at the *t*th decoder time step.\n",
        "\n",
        "If Œ±(3,2) is much larger than Œ±(3,0) and Œ±(3,1), then the decoder focuses mostly on encoder output #2 (e.g., the word ‚Äúsoccer‚Äù).\n"
      ],
      "metadata": {
        "id": "pru8b_hpm2yM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rest of the decoder works as before: at each time step, it receives:\n",
        "- the current inputs\n",
        "- the hidden state from the previous step\n",
        "- the previous target word (or previous output at inference time)\n"
      ],
      "metadata": {
        "id": "RUrKr40vm3sP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alignment Model (Attention Layer)\n",
        "\n",
        "The attention weights Œ±(t,i) are generated by a small neural network called an **alignment model**.\n"
      ],
      "metadata": {
        "id": "NG8jR42pu_U5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model:\n",
        "- Takes each encoder output\n",
        "- Takes the decoder‚Äôs previous hidden state\n",
        "- Outputs a score (energy) measuring how well they align\n"
      ],
      "metadata": {
        "id": "dnBJcgAhvAN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, after outputting ‚Äúme gusta el‚Äù, the decoder expects a noun. The encoder output corresponding to ‚Äúsoccer‚Äù aligns best, so it gets a high score.\n"
      ],
      "metadata": {
        "id": "Ubu_dYbovBNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All scores go through a softmax layer to produce attention weights that sum to 1.\n"
      ],
      "metadata": {
        "id": "kwtKdvA_vCC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This attention mechanism is called **Bahdanau attention** (or additive / concatenative attention).\n"
      ],
      "metadata": {
        "id": "gtNYJPPnvCzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention as Differentiable Memory Retrieval\n",
        "\n",
        "Attention can be viewed as a differentiable memory lookup mechanism.\n"
      ],
      "metadata": {
        "id": "rc81WIz1vDsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose the encoder learned:\n",
        "{\"subject\": \"I\", \"verb\": \"like\", \"noun\": \"soccer\"}\n",
        "\n",
        "The decoder wants to retrieve the noun.\n"
      ],
      "metadata": {
        "id": "hsucYf8OvErm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder does not use symbolic keys like ‚Äúnoun‚Äù. Instead, it uses learned vector representations.\n",
        "\n",
        "It computes similarity scores between the query and each key, applies softmax, and retrieves a weighted sum of values.\n"
      ],
      "metadata": {
        "id": "I_QBRqU3vGeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the ‚Äúnoun‚Äù representation matches best, its weight will be near 1, and the retrieved vector will be close to ‚Äúsoccer‚Äù.\n"
      ],
      "metadata": {
        "id": "giyiI8lXvI2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In modern terminology:\n",
        "- **Query** ‚Üí decoder hidden states\n",
        "- **Key** ‚Üí encoder outputs (for scoring)\n",
        "- **Value** ‚Üí encoder outputs (for weighted sum)\n"
      ],
      "metadata": {
        "id": "CeawLSibvJkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE\n",
        "\n",
        "If the input sentence is n words long, attention requires computing about n¬≤ weights, which becomes expensive for very long sequences.\n"
      ],
      "metadata": {
        "id": "OJSZcs-uvKh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Luong (Multiplicative) Attention\n",
        "\n",
        "In 2015, Minh-Thang Luong et al. proposed **dot-product attention**, which computes similarity using a dot product.\n"
      ],
      "metadata": {
        "id": "70TOPQfhvLbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is faster and often more effective. It requires the encoder and decoder vectors to have the same dimensionality.\n"
      ],
      "metadata": {
        "id": "0--7dNKEvMLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luong attention uses the decoder‚Äôs **current hidden state**, concatenates the attention vector with it, and uses this to predict the next token.\n"
      ],
      "metadata": {
        "id": "M9aHZ1LkvNFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dot-product variants outperformed additive attention, so Bahdanau attention is less common today.\n"
      ],
      "metadata": {
        "id": "JP_N3h7avOEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Luong Attention\n"
      ],
      "metadata": {
        "id": "q9ykBj2HvO_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query, key, value):  # note: dq == dk and Lk == Lv\n",
        "    scores = query @ key.transpose(1, 2)  # [B,Lq,dq] @ [B,dk,Lk] = [B, Lq, Lk]\n",
        "    weights = torch.softmax(scores, dim=-1)  # [B, Lq, Lk]\n",
        "    return weights @ value  # [B, Lq, Lk] @ [B, Lv, dv] = [B, Lq, dv]\n"
      ],
      "metadata": {
        "id": "0AG-UqXYvP9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This follows Equation 14-2:\n",
        "1. Compute attention scores\n",
        "2. Apply softmax\n",
        "3. Compute weighted sum of values\n",
        "ya byay"
      ],
      "metadata": {
        "id": "CnERkNRvvQ9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TIP\n",
        "\n",
        "You can replace the `@` operator with `torch.bmm()` for faster batch matrix multiplication.\n"
      ],
      "metadata": {
        "id": "CrS_Y_N9vhnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Updating the NMT Model\n"
      ],
      "metadata": {
        "id": "XXYe9Lq5vjIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output layer must accept concatenated vectors:\n"
      ],
      "metadata": {
        "id": "heHivHDivj4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "self.output = nn.Linear(2 * hidden_dim, vocab_size)\n"
      ],
      "metadata": {
        "id": "0cx5sDSmyNd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updated `forward()` Method\n"
      ],
      "metadata": {
        "id": "de81_yQ1yO7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, pair):\n",
        "    src_embeddings = self.embed(pair.src_token_ids)\n",
        "    tgt_embeddings = self.embed(pair.tgt_token_ids)\n",
        "    src_lengths = pair.src_mask.sum(dim=1)\n",
        "    src_packed = pack_padded_sequence(\n",
        "        src_embeddings, lengths=src_lengths.cpu(),\n",
        "        batch_first=True, enforce_sorted=False)\n",
        "\n",
        "    encoder_outputs_packed, hidden_states = self.encoder(src_packed)\n",
        "    decoder_outputs, _ = self.decoder(tgt_embeddings, hidden_states)\n",
        "\n",
        "    encoder_outputs, _ = pad_packed_sequence(\n",
        "        encoder_outputs_packed, batch_first=True)\n",
        "\n",
        "    attn_output = attention(\n",
        "        query=decoder_outputs,\n",
        "        key=encoder_outputs,\n",
        "        value=encoder_outputs)\n",
        "\n",
        "    combined_output = torch.cat(\n",
        "        (attn_output, decoder_outputs), dim=-1)\n",
        "\n",
        "    return self.output(combined_output).permute(0, 2, 1)\n"
      ],
      "metadata": {
        "id": "uQYbyoviyP73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation\n",
        "\n",
        "- Encoder outputs are no longer discarded\n",
        "- Packed sequences are unpacked before attention\n",
        "- Decoder outputs act as queries\n",
        "- Attention output is concatenated with decoder output\n"
      ],
      "metadata": {
        "id": "HFfIl3N0yRJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WARNING\n",
        "\n",
        "Padding tokens are not masked. The model learns to ignore them, but masking is preferable.\n"
      ],
      "metadata": {
        "id": "RrxgaiegyR7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WARNING\n",
        "\n",
        "Padding tokens are not masked. The model learns to ignore them, but masking is preferable.\n"
      ],
      "metadata": {
        "id": "M3SW78SGyStN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WARNING\n",
        "\n",
        "Padding tokens are not masked. The model learns to ignore them, but masking is preferable.\n"
      ],
      "metadata": {
        "id": "3xCVeMZyyTUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:\n"
      ],
      "metadata": {
        "id": "RYMayLRpyUjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "' Me gusta jugar fu tbol con mis amigos en la playa . </s>'\n"
      ],
      "metadata": {
        "id": "FQj_vw5hyVi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Notes\n",
        "\n",
        "Attention mechanisms were so powerful that researchers removed recurrent layers entirely.\n",
        "\n",
        "This led to the Transformer architecture and the paper:\n",
        "**‚ÄúAttention Is All You Need.‚Äù**\n"
      ],
      "metadata": {
        "id": "Rb5OYp9HyWZt"
      }
    }
  ]
}