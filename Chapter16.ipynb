{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9/BSHUPDcj7kbXSbeFNBL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanDCunha/Hands-On-Machine-Learning-with-Scikit-Learn-and-PyTorch/blob/main/Chapter16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformers\n",
        "\n",
        "Vision transformers didn’t pop out of a vacuum: before they were invented, there were RNNs with visual attention, and hybrid CNN–Transformer models. Let’s take a look at these ViT ancestors before we dive into some of the most influential ViTs.\n"
      ],
      "metadata": {
        "id": "W25NSxmvznDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNNs with Visual Attention\n",
        "\n",
        "One of the first applications of attention mechanisms beyond NLP was to generate image captions using visual attention. Here, a convolutional neural network first processes the image and outputs feature maps, then a decoder RNN equipped with an attention mechanism generates the caption one token at a time.\n",
        "\n",
        "The decoder uses an attention layer at each decoding step to focus on the most relevant part of the image. For example, when generating the word *“Frisbee”*, the model’s attention is concentrated on the Frisbee in the image.\n"
      ],
      "metadata": {
        "id": "LkpEAIg1zobw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explainability\n",
        "\n",
        "A key benefit of attention mechanisms is **explainability**—the ability to understand what parts of the input led to a particular output.\n",
        "\n",
        "For example, if a model labels a dog in the snow as a *wolf*, attention maps might reveal that the model focused heavily on the snow, suggesting it learned an incorrect correlation. This insight allows practitioners to fix the issue by rebalancing the training data.\n",
        "\n",
        "In some domains, explainability is not optional but a legal requirement, such as systems that decide whether to grant loans.\n"
      ],
      "metadata": {
        "id": "g4WTeND4zphy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once transformers were introduced, they were quickly applied to visual tasks, often replacing RNNs while still relying on CNNs for feature extraction. Although transformers were involved, these models are usually not considered true vision transformers.\n",
        "\n",
        "A notable example is DETR.\n"
      ],
      "metadata": {
        "id": "T7ON6QBVzqYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DETR: A CNN–Transformer Hybrid for Object Detection\n",
        "\n",
        "The Detection Transformer (DETR), introduced in 2020, combines a CNN backbone with an encoder–decoder transformer.\n",
        "\n",
        "1. A CNN extracts feature maps from the image.\n",
        "2. The feature maps are converted into a sequence of visual tokens.\n",
        "3. An encoder–decoder transformer processes these tokens.\n",
        "4. The output is a set of bounding box predictions.\n",
        "\n",
        "This raised a natural question: can we remove the CNN entirely?\n"
      ],
      "metadata": {
        "id": "6qGwR0zO2FuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Original Vision Transformer (ViT)\n",
        "\n",
        "In October 2020, Google introduced the first CNN-free vision transformer: **ViT**.\n",
        "\n",
        "The idea is simple:\n",
        "- Split the image into fixed-size patches (e.g., 16 × 16).\n",
        "- Flatten each patch and project it into an embedding vector.\n",
        "- Treat the sequence of patch embeddings like word embeddings.\n",
        "- Add positional embeddings.\n",
        "- Feed everything into a standard encoder-only transformer.\n"
      ],
      "metadata": {
        "id": "KBTyXxLd2HTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a 224 × 224 image with 3 color channels:\n",
        "- The image is split into 14 × 14 = 196 patches\n",
        "- Each patch is flattened into a 768-dimensional vector\n",
        "- A learnable class token is prepended\n",
        "- The class token output is used for classification (BERT-style)\n",
        "\n",
        "ViT achieved state-of-the-art results but required massive datasets because it lacks CNN inductive biases such as locality and translation invariance.\n"
      ],
      "metadata": {
        "id": "6B6A-_CJ2Iwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inductive Bias\n",
        "\n",
        "An inductive bias is an assumption built into a model’s architecture.\n",
        "\n",
        "Examples:\n",
        "- CNNs assume locality and translation invariance\n",
        "- RNNs assume sequential order\n",
        "- Transformers assume relationships are learned via attention\n",
        "\n",
        "More inductive bias → less data needed  \n",
        "Less inductive bias → more data required\n"
      ],
      "metadata": {
        "id": "exqmJbKG2JrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s implement a Vision Transformer from scratch using PyTorch.\n"
      ],
      "metadata": {
        "id": "HjKlsZZm2Knm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csCM8a5kzDtZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels, embed_dim, patch_size=16):\n",
        "        super().__init__()\n",
        "        self.conv2d = nn.Conv2d(\n",
        "            in_channels, embed_dim,\n",
        "            kernel_size=patch_size, stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv2d(x)            # [B, E, H', W']\n",
        "        x = x.flatten(2)              # [B, E, H' * W']\n",
        "        return x.transpose(1, 2)      # [B, L, E]\n"
      ],
      "metadata": {
        "id": "6jWX6Cz82MLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        in_channels=3,\n",
        "        num_classes=1000,\n",
        "        embed_dim=768,\n",
        "        depth=12,\n",
        "        num_heads=12,\n",
        "        ff_dim=3072,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(\n",
        "            in_channels, embed_dim, patch_size\n",
        "        )\n",
        "\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)\n",
        "        self.pos_embed = nn.Parameter(\n",
        "            torch.randn(1, num_patches + 1, embed_dim) * 0.02\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=ff_dim,\n",
        "            dropout=dropout,\n",
        "            activation=\"gelu\",\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, depth)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        cls = self.cls_token.expand(x.size(0), -1, -1)\n",
        "        x = torch.cat((cls, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "        x = self.encoder(x)\n",
        "        x = self.norm(x[:, 0])\n",
        "        return self.head(x)\n"
      ],
      "metadata": {
        "id": "YRDRa85J3Tx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = ViT()\n",
        "batch = torch.randn(4, 3, 224, 224)\n",
        "logits = vit(batch)\n",
        "logits.shape\n"
      ],
      "metadata": {
        "id": "DxaFDrFs3VFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model can now be trained using cross-entropy loss. However, training ViTs from scratch is expensive, so pretrained models are typically fine-tuned instead.\n"
      ],
      "metadata": {
        "id": "0VSjvscg3V-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "pets = load_dataset(\"timm/oxford-iiit-pet\")\n"
      ],
      "metadata": {
        "id": "Ct_N6Nxd3W2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification, AutoImageProcessor\n",
        "\n",
        "model_id = \"google/vit-base-patch16-224-in21k\"\n",
        "\n",
        "vit_model = ViTForImageClassification.from_pretrained(\n",
        "    model_id, num_labels=37\n",
        ")\n",
        "vit_processor = AutoImageProcessor.from_pretrained(\n",
        "    model_id, use_fast=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "o7yiMRmq3Xn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vit_collate_fn(batch):\n",
        "    images = [x[\"image\"] for x in batch]\n",
        "    labels = [x[\"label\"] for x in batch]\n",
        "    inputs = vit_processor(\n",
        "        images, return_tensors=\"pt\", do_convert_rgb=True\n",
        "    )\n",
        "    inputs[\"labels\"] = torch.tensor(labels)\n",
        "    return inputs\n"
      ],
      "metadata": {
        "id": "U0jk1P2R3YlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"my_pets_vit\",\n",
        "    per_device_train_batch_size=16,\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=3,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=vit_model,\n",
        "    args=args,\n",
        "    data_collator=vit_collate_fn,\n",
        "    train_dataset=pets[\"train\"],\n",
        "    eval_dataset=pets[\"test\"]\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "8RTIoHVG3ZZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodal Transformers\n",
        "\n",
        "Humans are multimodal creatures: we perceive the world through multiple senses—sight, hearing, smell, taste, touch, sense of balance, proprioception (i.e., sense of body position), and several others—and we act upon the world through movement, speech, writing, etc.\n",
        "\n",
        "Each of these modalities can be considered at a very low level (e.g., sound waves) or at a higher level (e.g., words, intonations, melody). Importantly, modalities are heterogeneous: one modality may be continuous while another is discrete, one may be temporal while the other is spatial, one may be high-resolution (e.g., 48 kHz audio) while the other is not (e.g., text), one may be noisy while the other is clean, and so on.\n"
      ],
      "metadata": {
        "id": "6VdwxBBj3vHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fusion and Alignment\n",
        "\n",
        "Multimodal machine learning requires designing models that can handle heterogeneous data and capture interactions between modalities.\n",
        "\n",
        "There are two main challenges:\n",
        "\n",
        "**Fusion**  \n",
        "Combining different modalities, often by encoding them into the same representation space.\n",
        "\n",
        "**Alignment**  \n",
        "Discovering relationships between modalities (e.g., aligning spoken words with timestamps, grounding text queries in images).\n",
        "\n",
        "Common multimodal tasks include image captioning, image retrieval, VQA, STT, TTS, embodied AI, and more.\n"
      ],
      "metadata": {
        "id": "HcxLRQ3A4GiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Transformers Work Well for Multimodality\n",
        "\n",
        "Transformers can ingest almost any modality once it is tokenized into sequences:\n",
        "- Text → words/subwords\n",
        "- Images → patches\n",
        "- Audio/video → short clips\n",
        "\n",
        "Once embedded, modalities can be fused via concatenation, summation, or fusion encoders.  \n",
        "Multi-head attention enables both intra-modal and cross-modal reasoning, solving alignment.\n"
      ],
      "metadata": {
        "id": "tia6eEbr4HZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VideoBERT (2019)\n",
        "\n",
        "VideoBERT is a BERT-style multimodal transformer handling text + video.\n",
        "\n",
        "Key ideas:\n",
        "- Video clips encoded using a pretrained 3D CNN (S3D)\n",
        "- Visual features clustered via hierarchical k-means into a discrete vocabulary\n",
        "- Visual tokens treated like word tokens\n",
        "- Pretraining tasks:\n",
        "  - Masked token prediction\n",
        "  - Linguistic–visual alignment (binary classification)\n"
      ],
      "metadata": {
        "id": "3zI2CjMx4IQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video Tokenization\n",
        "\n",
        "- Videos split into 1.5s clips (30 frames)\n",
        "- Each clip → 1024D vector\n",
        "- Hierarchical k-means → 20,736 visual tokens\n",
        "- Significant compression, but key semantics preserved\n"
      ],
      "metadata": {
        "id": "z8I-gSfG4JF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViLBERT (2019)\n",
        "\n",
        "ViLBERT introduced a **dual-stream** architecture:\n",
        "- Separate text encoder and visual encoder\n",
        "- Cross-modal interaction via **co-attention**\n",
        "\n",
        "Motivation:\n",
        "- Modalities require different processing depths\n",
        "- Visual features often already high-level\n",
        "- Avoid damaging pretrained BERT weights\n"
      ],
      "metadata": {
        "id": "7VLlTkIf4KKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Co-Attention\n",
        "\n",
        "In co-attention layers:\n",
        "- Queries from one modality attend to keys/values of the other\n",
        "- Enables bidirectional information flow\n"
      ],
      "metadata": {
        "id": "rQAQFlvS4K2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLIP (2021)\n",
        "\n",
        "CLIP (Contrastive Language–Image Pretraining) uses:\n",
        "- A text encoder\n",
        "- A vision encoder\n",
        "- Contrastive loss over large image–caption datasets\n",
        "\n",
        "Goal:\n",
        "Matching image–text pairs have similar embeddings; mismatched pairs are pushed apart.\n"
      ],
      "metadata": {
        "id": "UQai4J5R4L8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_id = \"openai/clip-vit-base-patch32\"\n",
        "clip_pipeline = pipeline(\n",
        "    task=\"zero-shot-image-classification\",\n",
        "    model=model_id,\n",
        "    device_map=\"auto\",\n",
        "    dtype=\"auto\"\n",
        ")\n",
        "\n",
        "candidate_labels = [\"cricket\", \"ladybug\", \"spider\"]\n",
        "image_url = \"https://homl.info/ladybug\"\n",
        "\n",
        "results = clip_pipeline(\n",
        "    image_url,\n",
        "    candidate_labels=candidate_labels,\n",
        "    hypothesis_template=\"This is a photo of a {}.\"\n",
        ")\n",
        "\n",
        "results\n"
      ],
      "metadata": {
        "id": "SFCbYxcR4Oui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP enables powerful zero-shot classification by comparing image embeddings with text embeddings generated from class names or prompts.\n"
      ],
      "metadata": {
        "id": "xfgvGc1T4M3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import PIL.Image\n",
        "import urllib.request\n",
        "import torch\n",
        "\n",
        "processor = CLIPProcessor.from_pretrained(model_id)\n",
        "model = CLIPModel.from_pretrained(model_id)\n",
        "\n",
        "image = PIL.Image.open(\n",
        "    urllib.request.urlopen(image_url)\n",
        ").convert(\"RGB\")\n",
        "\n",
        "captions = [f\"This is a photo of a {label}.\" for label in candidate_labels]\n",
        "\n",
        "inputs = processor(\n",
        "    text=captions,\n",
        "    images=[image],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "image_features = outputs.image_embeds\n",
        "text_features = outputs.text_embeds\n",
        "\n",
        "similarities = image_features @ text_features.T\n",
        "similarities\n"
      ],
      "metadata": {
        "id": "3epSSlGU4RjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DALL·E\n",
        "\n",
        "- **DALL·E (2021)**: GPT-like autoregressive image token generator\n",
        "- **DALL·E 2 (2022)**: CLIP + diffusion\n",
        "- **DALL·E 3 (2023)**: Diffusion + LLM prompt rewriting\n",
        "\n",
        "Each version improved compositional accuracy and image fidelity.\n"
      ],
      "metadata": {
        "id": "qNhWtTNV4T2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceiver (2021)\n",
        "\n",
        "Perceiver processes raw inputs directly (pixels, audio frames, characters) using:\n",
        "\n",
        "- Cross-attention from inputs → latent tokens\n",
        "- Latent bottleneck to avoid quadratic scaling\n",
        "- Modality-agnostic design\n"
      ],
      "metadata": {
        "id": "shA3gaDg4V4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latent Bottleneck\n",
        "\n",
        "Instead of attending over millions of inputs:\n",
        "- Inputs attend to a fixed number of latent tokens\n",
        "- Complexity scales linearly with input size\n"
      ],
      "metadata": {
        "id": "fsIXOkcQ4WwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latent Bottleneck\n",
        "\n",
        "Instead of attending over millions of inputs:\n",
        "- Inputs attend to a fixed number of latent tokens\n",
        "- Complexity scales linearly with input size\n"
      ],
      "metadata": {
        "id": "A7e2gXU74Xv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flamingo (2022)\n",
        "\n",
        "Flamingo is a few-shot vision-language model built from:\n",
        "- Frozen vision encoder\n",
        "- Frozen decoder-only LLM\n",
        "- Perceiver-based visual resampler\n",
        "- Gated cross-attention modules\n",
        "\n",
        "Enables open-ended visual dialogue and reasoning.\n"
      ],
      "metadata": {
        "id": "-BTm-8Kg4Yvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLIP and BLIP-2\n",
        "\n",
        "BLIP:\n",
        "- Unified vision–language pretraining\n",
        "- Image–text contrastive, matching, and LM objectives\n",
        "\n",
        "BLIP-2:\n",
        "- Reuses frozen vision model + frozen LLM\n",
        "- Introduces the Q-Former\n",
        "- Stronger performance with fewer trainable parameters\n"
      ],
      "metadata": {
        "id": "tjNsOny-4b_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## BLIP and BLIP-2\n",
        "\n",
        "BLIP:\n",
        "- Unified vision–language pretraining\n",
        "- Image–text contrastive, matching, and LM objectives\n",
        "\n",
        "BLIP-2:\n",
        "- Reuses frozen vision model + frozen LLM\n",
        "- Introduces the Q-Former\n",
        "- Stronger performance with fewer trainable parameters\n"
      ],
      "metadata": {
        "id": "6F55C62I4VC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Multimodal Models\n",
        "\n",
        "We’ve covered quite a few multimodal models, with very different architectures and pretraining techniques, but of course there are many others. Below is a quick overview of some of the most notable multimodal models released in recent years.\n"
      ],
      "metadata": {
        "id": "YwrcBCvQ5XIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notable Multimodal Models\n",
        "\n",
        "**LayoutLM (Microsoft, Dec. 2019)**  \n",
        "Document understanding based on text, vision, and document layout.  \n",
        "Widely used for forms, invoices, and scanned documents.  \n",
        "LayoutLMv3 was released in April 2022.\n",
        "\n",
        "**GLIP (Microsoft, Dec. 2021)**  \n",
        "A vision-language model for visual grounding and object detection.  \n",
        "Unifies object detection and phrase grounding.  \n",
        "GLIP-2 was released in 2022.\n",
        "\n",
        "**Stable Diffusion (Stability AI, Dec. 2021)**  \n",
        "A powerful text-to-image diffusion model.  \n",
        "Open-weight and highly influential in generative image modeling.\n",
        "\n",
        "**OFA (Microsoft, Feb. 2022)**  \n",
        "“One For All”: a unified vision-language pretraining framework.  \n",
        "Handles captioning, VQA, classification, and more with a single architecture.\n",
        "\n",
        "**CoCa (Google, May 2022)**  \n",
        "A vision-language model pretrained with both contrastive and captioning objectives.  \n",
        "Strong at zero-shot and generative tasks.  \n",
        "Influenced later models such as PaLI-X and Flamingo-2.\n"
      ],
      "metadata": {
        "id": "iAhjRSii5X7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Large-Scale Multimodal Foundation Models\n",
        "\n",
        "**PaLI (Google, Sep. 2022)**  \n",
        "Multilingual multimodal models for VQA, captioning, and reasoning.  \n",
        "Strong zero-shot and cross-lingual performance.  \n",
        "Follow-ups include PaLI-X and PaLI-3 (2023), and PaliGemma (May 2024).\n",
        "\n",
        "**Kosmos-1 (Microsoft, Feb. 2023)**  \n",
        "Vision-language model with strong visual grounding capabilities.  \n",
        "Extended by Kosmos-2 and Kosmos-2.5 later in 2023.\n",
        "\n",
        "**PaLM-E (Google, Mar. 2023)**  \n",
        "Extends PaLM with visual inputs and embodied sensor data.  \n",
        "A decoder-only LLM outputs textual action commands that are executed by a robot via a downstream system.\n",
        "\n",
        "**LLaVA (H. Liu et al., Apr. 2023)**  \n",
        "One of the strongest open-source vision-language chat models.  \n",
        "Combines a vision encoder with a large language model.\n",
        "\n",
        "**ImageBind (Meta, May 2023)**  \n",
        "Extends CLIP-style contrastive learning to six modalities:  \n",
        "image, text, audio, IMU, depth, and thermal data.\n",
        "\n",
        "**RT-2 (DeepMind, Jul. 2023)**  \n",
        "A vision-language-action model trained on large-scale robotic instruction data.  \n",
        "Capable of reasoning and robotic control.\n"
      ],
      "metadata": {
        "id": "hp37qcGg5Y33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speech, Video, and Real-Time Multimodal Models\n",
        "\n",
        "**SeamlessM4T (Meta, Aug. 2023)**  \n",
        "A single unified model for:\n",
        "- speech-to-text\n",
        "- speech-to-speech\n",
        "- text-to-speech\n",
        "- text-to-text translation  \n",
        "Supports nearly 100 languages.\n",
        "\n",
        "**Qwen-VL (Alibaba, Sep. 2023)**  \n",
        "An open vision-language model family (7B–72B).  \n",
        "Became one of the strongest open multimodal baselines.  \n",
        "Followed by:\n",
        "- Qwen2-VL (Aug. 2024)\n",
        "- Qwen3-Omni (Sep. 2025), expanding to video and audio and reaching trillion-parameter scale.\n",
        "\n",
        "**Fuyu (Adept AI, Oct. 2023)**  \n",
        "Processes interleaved image and text inputs in real time using a unified transformer.\n"
      ],
      "metadata": {
        "id": "KaL_s3fX5Z6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generative and Interactive Multimodal Systems\n",
        "\n",
        "**EMO (Alibaba, Feb. 2024)**  \n",
        "Given:\n",
        "- an image of a person\n",
        "- an audio clip (speech or singing)\n",
        "\n",
        "The model generates a video of the person synchronized with the audio.  \n",
        "EMO-2 was released in January 2025.\n",
        "\n",
        "**GLaMM (H. Rasheed et al., Jun. 2024)**  \n",
        "A visual dialogue model that outputs:\n",
        "- natural language responses\n",
        "- object segmentation masks\n",
        "\n",
        "**LaViDa (UCLA, Panasonic, Adobe, Salesforce, May 2025)**  \n",
        "A family of open diffusion-based vision-language models.\n"
      ],
      "metadata": {
        "id": "hep-CdQi5a14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tip\n",
        "\n",
        "Short links for all models discussed in this chapter are available at:\n",
        "\n",
        "https://homl.info/<modelname>\n",
        "\n",
        "Use lowercase names without hyphens, for example:\n",
        "https://homl.info/qwen2vl\n"
      ],
      "metadata": {
        "id": "yCjUGvL75brS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Commercial Multimodal Models\n",
        "\n",
        "Several commercial multimodal models do not publicly disclose their full architectures, including:\n",
        "\n",
        "- GPT-4.1 and Sora (OpenAI)\n",
        "- Gemini 2.5 Pro (Google)\n",
        "- Veo-3 (DeepMind)\n",
        "- Claude 4 Opus (Anthropic)\n",
        "\n",
        "Access usually requires an account, subscription, or API key.\n"
      ],
      "metadata": {
        "id": "NdXac2zN5ce3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Querying Gemini 2.5 via API\n",
        "\n",
        "The following example shows how to query Gemini 2.5 using the `google-genai` library.\n",
        "You must first obtain an API key from Google AI Studio.\n"
      ],
      "metadata": {
        "id": "fclbpwEu5ddm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "gemini_api_key = [...]  # load from secrets, file, or environment variable\n",
        "gemini_client = genai.Client(api_key=gemini_api_key)\n",
        "\n",
        "cats_photo = gemini_client.files.upload(file=\"my_cats_photo.jpg\")\n",
        "\n",
        "question = \"What animal and how many? Format: [animal, number]\"\n",
        "\n",
        "response = gemini_client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",  # or \"gemini-2.5-pro\"\n",
        "    contents=[cats_photo, question]\n",
        ")\n",
        "\n",
        "print(response.text)  # Example output: \"[cat, 2]\"\n"
      ],
      "metadata": {
        "id": "LiCPUSHb5ebm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example assumes that:\n",
        "- the `google-genai` library is installed (it is preinstalled on Colab)\n",
        "- the file `my_cats_photo.jpg` exists in the working directory\n"
      ],
      "metadata": {
        "id": "qQwB4tTl5fSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter Wrap-Up\n",
        "\n",
        "This wraps up the chapter on multimodal transformers.\n",
        "\n",
        "Transformers can now:\n",
        "- read and write\n",
        "- see and hear\n",
        "- reason across text, images, audio, video, and sensor data\n",
        "\n",
        "In the next chapter, we will explore advanced techniques for speeding up and scaling transformers.\n",
        "\n",
        "As Daft Punk put it: *harder, better, faster, stronger.*\n"
      ],
      "metadata": {
        "id": "-D4S0U7T5gVe"
      }
    }
  ]
}