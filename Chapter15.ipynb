{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRIXoZje3r7saF8h6sXOo1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanDCunha/Hands-On-Machine-Learning-with-Scikit-Learn-and-PyTorch/blob/main/Chapter15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Is All You Need: The Original Transformer Architecture\n",
        "\n",
        "The original 2017 Transformer architecture is represented in Figure 15-3.  \n",
        "The left part of the figure represents the **encoder**, the right part represents the **decoder**.\n"
      ],
      "metadata": {
        "id": "WYUIrDJ324gd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Overview\n",
        "\n",
        "The encoder‚Äôs role is to gradually transform the inputs (e.g., sequences of English tokens) until each token‚Äôs representation perfectly captures the meaning of that token in the context of the sentence.\n",
        "\n",
        "The encoder‚Äôs output is a sequence of **contextualized token embeddings**.\n"
      ],
      "metadata": {
        "id": "m86VwrF225k9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apart from the embedding layer, every layer in the encoder:\n",
        "\n",
        "- Takes input of shape  \n",
        "  **[batch size, max English sequence length, embedding size]**\n",
        "- Returns output of the **same shape**\n",
        "\n",
        "Token representations are gradually transformed ‚Äî hence the name *Transformer*.\n"
      ],
      "metadata": {
        "id": "LHcSZ-JD26pO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example\n",
        "\n",
        "If you feed the sentence **‚ÄúI like soccer‚Äù** into the encoder:\n",
        "\n",
        "- The word **‚Äúlike‚Äù** starts with a vague meaning\n",
        "- After encoding, it captures:\n",
        "  - Correct meaning (verb: *to enjoy*)\n",
        "  - Grammatical role\n",
        "  - Contextual information needed for translation\n"
      ],
      "metadata": {
        "id": "3PFe1KXy27dx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Overview\n",
        "\n",
        "The decoder takes:\n",
        "\n",
        "- The encoder‚Äôs outputs\n",
        "- The translated sentence so far\n",
        "\n",
        "Its goal is to predict the **next token** in the translation.\n"
      ],
      "metadata": {
        "id": "jPLyEDrW28cU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoding Example\n",
        "\n",
        "Source sentence: **‚ÄúI like soccer‚Äù**\n",
        "\n",
        "Decoder outputs step-by-step:\n",
        "1. `me`\n",
        "2. `me gusta`\n",
        "3. `me gusta el`\n",
        "4. `me gusta el f√∫tbol`\n",
        "\n",
        "Since there is no end-of-sentence token (`</s>`), the decoder is called once more to predict it.\n"
      ],
      "metadata": {
        "id": "dN8keFkq29WG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder input sequence becomes:\n",
        "\n",
        "`<s> me gusta el f√∫tbol`\n",
        "\n",
        "Each token representation is transformed so that:\n",
        "- `<s>` ‚Üí `me`\n",
        "- `me` ‚Üí `gusta`\n",
        "- `gusta` ‚Üí `el`\n",
        "- `el` ‚Üí `f√∫tbol`\n",
        "- `f√∫tbol` ‚Üí `</s>`\n"
      ],
      "metadata": {
        "id": "l9KkNEGm2-Yi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Stack Structure\n",
        "\n",
        "- Encoder blocks are stacked **N times** (N = 6 in the paper)\n",
        "- Decoder blocks are also stacked **N times**\n",
        "- The **final encoder output** is fed into **every decoder block**\n"
      ],
      "metadata": {
        "id": "mOH3QtEX2_TX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Components You Already Know\n",
        "\n",
        "- Embedding layers\n",
        "- Skip connections + LayerNorm\n",
        "- Feedforward networks (2 Linear layers with ReLU)\n",
        "- Final Linear output layer\n",
        "\n",
        "‚ùó All of these treat tokens **independently**\n"
      ],
      "metadata": {
        "id": "KuKFZEWH3EjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Components\n",
        "\n",
        "### 1. Encoder Self-Attention\n",
        "\n",
        "Each token attends to **all tokens in the sentence**, including itself.\n",
        "\n",
        "This allows:\n",
        "- Contextual understanding\n",
        "- Disambiguation of words like ‚Äúlike‚Äù\n"
      ],
      "metadata": {
        "id": "hsPuaS_r3t2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Decoder Masked Self-Attention\n",
        "\n",
        "Each token can only attend to:\n",
        "- Previous tokens\n",
        "- Itself\n",
        "\n",
        "This prevents the model from **cheating during training**.\n"
      ],
      "metadata": {
        "id": "WwOiB41u3u4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Encoder‚ÄìDecoder Cross-Attention\n",
        "\n",
        "The decoder attends to the encoder‚Äôs outputs.\n",
        "\n",
        "Example:\n",
        "- While generating **‚Äúf√∫tbol‚Äù**, the decoder attends strongly to **‚Äúsoccer‚Äù**\n"
      ],
      "metadata": {
        "id": "OUS7h0GF3vmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encodings\n",
        "\n",
        "Transformers are **position-agnostic**.\n",
        "\n",
        "To inject order information:\n",
        "- Add a positional encoding to each token embedding\n"
      ],
      "metadata": {
        "id": "_BpcR51H3xRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainable Positional Embeddings (Common Approach)\n",
        "\n",
        "- Use a trainable matrix\n",
        "- Add it to token embeddings\n",
        "- Apply dropout\n"
      ],
      "metadata": {
        "id": "cNOZ-Uuw3yR0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enzytIt5223t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_length, embed_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.pos_embed = nn.Parameter(\n",
        "            torch.randn(max_length, embed_dim) * 0.02\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.dropout(X + self.pos_embed[:X.size(1)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**  \n",
        "- Input shape: `[batch, sequence length, embedding size]`\n",
        "- Positional embedding shape: `[sequence length, embedding size]`\n",
        "- Broadcasting handles the addition\n"
      ],
      "metadata": {
        "id": "lT5C5aHM30Hy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention (MHA)\n",
        "\n",
        "Based on **scaled dot-product attention**.\n",
        "\n",
        "Each head learns to focus on **different aspects** of token representations.\n"
      ],
      "metadata": {
        "id": "Y7A_aO8B31Dt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Multiple Heads?\n",
        "\n",
        "Different heads can specialize:\n",
        "- Grammar\n",
        "- Semantics\n",
        "- Tense\n",
        "- Long-range dependencies\n"
      ],
      "metadata": {
        "id": "R_-fYnDS32AG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaled Dot-Product Attention\n",
        "\n",
        "Given:\n",
        "- Queries (Q)\n",
        "- Keys (K)\n",
        "- Values (V)\n",
        "\n",
        "We compute:\n",
        "\n",
        "softmax(QK·µÄ / ‚àöd) V\n"
      ],
      "metadata": {
        "id": "G-G_WNvn33Em"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaled Dot-Product Attention\n",
        "\n",
        "Given:\n",
        "- Queries (Q)\n",
        "- Keys (K)\n",
        "- Values (V)\n",
        "\n",
        "We compute:\n",
        "\n",
        "softmax(QK·µÄ / ‚àöd) V\n"
      ],
      "metadata": {
        "id": "UF1IvSVO3-PM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention Implementation\n"
      ],
      "metadata": {
        "id": "eRKIKfb83_Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.h = num_heads\n",
        "        self.d = embed_dim // num_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def split_heads(self, X):\n",
        "        return X.view(X.size(0), X.size(1), self.h, self.d).transpose(1, 2)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        q = self.split_heads(self.q_proj(query))\n",
        "        k = self.split_heads(self.k_proj(key))\n",
        "        v = self.split_heads(self.v_proj(value))\n",
        "\n",
        "        scores = q @ k.transpose(2, 3) / self.d ** 0.5\n",
        "        weights = scores.softmax(dim=-1)\n",
        "        Z = self.dropout(weights) @ v\n",
        "\n",
        "        Z = Z.transpose(1, 2).reshape(Z.size(0), Z.size(1), -1)\n",
        "        return self.out_proj(Z), weights\n"
      ],
      "metadata": {
        "id": "FIGkCWgj3_8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masking Support\n"
      ],
      "metadata": {
        "id": "3RI3SDYf4DT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
        "    ...\n",
        "    if attn_mask is not None:\n",
        "        scores = scores.masked_fill(attn_mask, -torch.inf)\n",
        "\n",
        "    if key_padding_mask is not None:\n",
        "        mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
        "        scores = scores.masked_fill(mask, -torch.inf)\n",
        "    ...\n"
      ],
      "metadata": {
        "id": "lvcsU3yM4ENv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Block\n"
      ],
      "metadata": {
        "id": "KxB3I6SB4E9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        attn, _ = self.self_attn(src, src, src,\n",
        "                                 attn_mask=src_mask,\n",
        "                                 key_padding_mask=src_key_padding_mask)\n",
        "        Z = self.norm1(src + self.dropout(attn))\n",
        "        ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n",
        "        return self.norm2(Z + ff)\n"
      ],
      "metadata": {
        "id": "NrYvvUPZ4F-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Block\n"
      ],
      "metadata": {
        "id": "TviPpFXX4HcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
        "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        attn1, _ = self.self_attn(tgt, tgt, tgt,\n",
        "                                  attn_mask=tgt_mask,\n",
        "                                  key_padding_mask=tgt_key_padding_mask)\n",
        "        Z = self.norm1(tgt + self.dropout(attn1))\n",
        "\n",
        "        attn2, _ = self.multihead_attn(Z, memory, memory,\n",
        "                                       attn_mask=memory_mask,\n",
        "                                       key_padding_mask=memory_key_padding_mask)\n",
        "        Z = self.norm2(Z + self.dropout(attn2))\n",
        "\n",
        "        ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n",
        "        return self.norm3(Z + ff)\n"
      ],
      "metadata": {
        "id": "6UnYBf8V4IJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch Built-in Transformer Modules\n",
        "\n",
        "- `nn.TransformerEncoderLayer`\n",
        "- `nn.TransformerDecoderLayer`\n",
        "- `nn.TransformerEncoder`\n",
        "- `nn.TransformerDecoder`\n",
        "- `nn.Transformer`\n"
      ],
      "metadata": {
        "id": "MWP7C1gq4eqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are highly optimized and support:\n",
        "- `batch_first=True`\n",
        "- Causal masking\n",
        "- GELU activation\n",
        "- Performance optimizations\n"
      ],
      "metadata": {
        "id": "beor93kY4flC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Notes\n",
        "\n",
        "You now know how to build a full Transformer **from scratch**.\n",
        "\n",
        "Remaining steps:\n",
        "- Add final Linear layer\n",
        "- Train with `nn.CrossEntropyLoss`\n",
        "- Use autoregressive decoding\n",
        "\n",
        "Next up: **Using Transformers for machine translation** üöÄ\n"
      ],
      "metadata": {
        "id": "d15EkGTW4gUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building an English-to-Spanish Transformer\n",
        "\n",
        "It‚Äôs time to build our Neural Machine Translation (NMT) Transformer model.  \n",
        "We will reuse the `PositionalEmbedding` module and rely on PyTorch‚Äôs built-in\n",
        "`nn.Transformer`, which is well-optimized and faster than a custom implementation.\n",
        "\n",
        "The model will:\n",
        "- Embed source (English) and target (Spanish) tokens\n",
        "- Add positional embeddings\n",
        "- Use an encoder‚Äìdecoder Transformer\n",
        "- Apply causal masking in the decoder\n",
        "- Output logits suitable for `nn.CrossEntropyLoss`\n"
      ],
      "metadata": {
        "id": "eKvHgSPC5bQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "metadata": {
        "id": "4ilmk0lD5cQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "metadata": {
        "id": "XcrFnE6V5c6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NmtTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, max_length, embed_dim=512, pad_id=0,\n",
        "                 num_heads=8, num_layers=6, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
        "        self.pos_embed = PositionalEmbedding(max_length, embed_dim, dropout)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.output = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, pair):\n",
        "        # Embed tokens and add positional encodings\n",
        "        src_embeds = self.pos_embed(self.embed(pair.src_token_ids))\n",
        "        tgt_embeds = self.pos_embed(self.embed(pair.tgt_token_ids))\n",
        "\n",
        "        # Invert masks: True means \"ignore\"\n",
        "        src_pad_mask = ~pair.src_mask.bool()\n",
        "        tgt_pad_mask = ~pair.tgt_mask.bool()\n",
        "\n",
        "        # Create causal mask for decoder self-attention\n",
        "        seq_len = pair.tgt_token_ids.size(1)\n",
        "        full_mask = torch.full((seq_len, seq_len), True, device=tgt_pad_mask.device)\n",
        "        causal_mask = torch.triu(full_mask, diagonal=1)\n",
        "\n",
        "        # Transformer forward pass\n",
        "        out = self.transformer(\n",
        "            src=src_embeds,\n",
        "            tgt=tgt_embeds,\n",
        "            src_key_padding_mask=src_pad_mask,\n",
        "            memory_key_padding_mask=src_pad_mask,\n",
        "            tgt_key_padding_mask=tgt_pad_mask,\n",
        "            tgt_mask=causal_mask,\n",
        "            tgt_is_causal=True\n",
        "        )\n",
        "\n",
        "        # Project to vocabulary size\n",
        "        logits = self.output(out)\n",
        "\n",
        "        # Rearrange for CrossEntropyLoss: (B, vocab_size, seq_len)\n",
        "        return logits.permute(0, 2, 1)\n"
      ],
      "metadata": {
        "id": "fN3QE3EQ5d1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Forward Pass\n",
        "\n",
        "1. Tokens are embedded and enriched with positional encodings.\n",
        "2. Padding masks are inverted because PyTorch expects `True` for tokens to ignore.\n",
        "3. A causal (upper-triangular) mask prevents the decoder from seeing future tokens.\n",
        "4. The Transformer processes source and target sequences.\n",
        "5. A final linear layer produces logits over the vocabulary.\n",
        "6. Output dimensions are rearranged for `nn.CrossEntropyLoss`.\n"
      ],
      "metadata": {
        "id": "NUxAs4C15e-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Smaller Transformer\n",
        "\n",
        "To speed up training and reduce overfitting, we can shrink the model:\n",
        "- Embedding size: 128\n",
        "- Attention heads: 4\n",
        "- Encoder layers: 2\n",
        "- Decoder layers: 2\n"
      ],
      "metadata": {
        "id": "AjlSd6jv5gF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "nmt_tr_model = NmtTransformer(\n",
        "    vocab_size=vocab_size,\n",
        "    max_length=max_length,\n",
        "    embed_dim=128,\n",
        "    pad_id=0,\n",
        "    num_heads=4,\n",
        "    num_layers=2,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "gintg"
      ],
      "metadata": {
        "id": "0TRCBssE5hK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model can now be trained exactly like the RNN encoder‚Äìdecoder\n",
        "from Chapter 14, using `nn.CrossEntropyLoss` and teacher forcing.\n"
      ],
      "metadata": {
        "id": "bXFk-PRh6IA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Example\n",
        "\n",
        "After training for around 20 epochs, even this small Transformer\n",
        "can produce high-quality translations.\n"
      ],
      "metadata": {
        "id": "j6xw0Uw-6IvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluation Example\n",
        "\n",
        "After training for around 20 epochs, even this small Transformer\n",
        "can produce high-quality translations.\n"
      ],
      "metadata": {
        "id": "tyWing0x6Jm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output\n",
        "\n"
      ],
      "metadata": {
        "id": "tudDlaGG6Ki2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output\n",
        "\n"
      ],
      "metadata": {
        "id": "Cfl4WLPO6LZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Up GPU Memory\n",
        "\n",
        "Before moving on to other models, free GPU memory:\n",
        "\n",
        "- Delete unused variables\n",
        "- Run Python garbage collection\n",
        "- Clear CUDA cache if using a GPU\n"
      ],
      "metadata": {
        "id": "iHKGvyG_6MXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "del nmt_tr_model\n",
        "gc.collect()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "X8yWvRwe6NWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder-Only Transformers for Natural Language Understanding\n",
        "\n",
        "When Google released the BERT model in 2018, it proved that an encoder-only\n",
        "Transformer can handle a wide variety of natural language understanding (NLU)\n",
        "tasks, including:\n",
        "\n",
        "- Sentence classification\n",
        "- Token classification\n",
        "- Multiple-choice question answering\n",
        "- Extractive question answering\n",
        "\n",
        "BERT also demonstrated the power of self-supervised pretraining on large corpora\n",
        "followed by fine-tuning on relatively small task-specific datasets.\n",
        "\n",
        "In this section, we will:\n",
        "- Examine BERT‚Äôs architecture\n",
        "- Understand its pretraining objectives\n",
        "- See how to pretrain and fine-tune encoder-only models\n"
      ],
      "metadata": {
        "id": "LUr565AX7euc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WARNING: Encoder-Only vs Decoder Models\n",
        "\n",
        "Encoder-only models are generally **not used for text generation** tasks such as:\n",
        "- Autocompletion\n",
        "- Translation\n",
        "- Summarization\n",
        "- Chatbots\n",
        "\n",
        "This is because encoders are **bidirectional** and must recompute attention over\n",
        "the entire sequence whenever a new token is added.\n",
        "\n",
        "Decoders, by contrast, are **causal** and can cache previous states, making them\n",
        "much faster for generation.\n",
        "\n",
        "The ‚ÄúB‚Äù in BERT stands for **Bidirectional Encoder Representations from\n",
        "Transformers**.\n"
      ],
      "metadata": {
        "id": "W_l676_y7f0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT‚Äôs Architecture\n",
        "\n",
        "BERT is almost identical to the original Transformer encoder, with three key\n",
        "differences:\n",
        "\n",
        "### 1. Scale\n",
        "- BERT-base: 12 layers, 12 heads, 768 hidden units\n",
        "- BERT-large: 24 layers, 16 heads, 1,024 hidden units\n",
        "- Maximum input length: 512 tokens\n",
        "- Uses trainable positional embeddings\n",
        "\n",
        "### 2. Pre-Layer Normalization (Pre-LN)\n",
        "Layer normalization is applied **before** each sublayer instead of after.\n",
        "This stabilizes training and reduces sensitivity to initialization.\n",
        "\n",
        "PyTorch supports this via `norm_first=True`.\n",
        "\n",
        "### 3. Segment Embeddings\n",
        "BERT supports **two input segments**, useful for sentence-pair tasks.\n",
        "- `[SEP]` token separates segments\n",
        "- Segment embeddings (0 or 1) are added to token embeddings\n"
      ],
      "metadata": {
        "id": "J4N0CsSx7gxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Pretraining Objectives\n",
        "\n",
        "BERT uses two self-supervised objectives during pretraining.\n"
      ],
      "metadata": {
        "id": "GOa57Zad7h3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masked Language Modeling (MLM)\n",
        "\n",
        "- Each token has a **15% probability** of being selected\n",
        "- Of selected tokens:\n",
        "  - 80% replaced with `[MASK]`\n",
        "  - 10% replaced with a random token\n",
        "  - 10% left unchanged\n",
        "- Loss is computed **only on selected tokens**\n",
        "\n",
        "This forces the model to learn deep bidirectional context.\n",
        "ya"
      ],
      "metadata": {
        "id": "8vwcC0LT7iuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Sentence Prediction (NSP)\n",
        "\n",
        "The model predicts whether two sentences are consecutive.\n",
        "\n",
        "Implementation:\n",
        "- Add a `[CLS]` token at position 0\n",
        "- Use the `[CLS]` embedding for binary classification\n",
        "\n",
        "Later research showed NSP adds little benefit, so it was dropped in many models\n",
        "(e.g., RoBERTa).\n"
      ],
      "metadata": {
        "id": "4c1qDPkw768e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a BERT Model from Scratch (Using Hugging Face)\n",
        "\n",
        "Instead of manually building BERT with `nn.TransformerEncoder`, we can use the\n",
        "Transformers library to quickly define and train a model.\n"
      ],
      "metadata": {
        "id": "0BNmTXnu78Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, BertForMaskedLM, BertTokenizerFast\n"
      ],
      "metadata": {
        "id": "gZGhYSV279Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "config = BertConfig(\n",
        "    vocab_size=bert_tokenizer.vocab_size,\n",
        "    hidden_size=128,\n",
        "    num_hidden_layers=2,\n",
        "    num_attention_heads=4,\n",
        "    intermediate_size=512,\n",
        "    max_position_embeddings=128\n",
        ")\n",
        "\n",
        "bert = BertForMaskedLM(config)\n"
      ],
      "metadata": {
        "id": "1q1awHHe7-CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Tokenizing a Dataset\n",
        "\n",
        "We use the WikiText dataset here for demonstration purposes.\n"
      ],
      "metadata": {
        "id": "t5eaE86E7_GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "fSMyGDDb7_80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example, tokenizer=bert_tokenizer):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "mlm_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "mlm_dataset = mlm_dataset.map(tokenize, batched=True)\n"
      ],
      "metadata": {
        "id": "BsDK9s858AoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collator for MLM\n",
        "\n",
        "The data collator dynamically applies masking during training.\n"
      ],
      "metadata": {
        "id": "cV5v6MH88BiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collator for MLM\n",
        "\n",
        "The data collator dynamically applies masking during training.\n"
      ],
      "metadata": {
        "id": "_mWj3zuX8nM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling\n"
      ],
      "metadata": {
        "id": "roxjr4UV8orf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"./my_bert\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=16\n",
        ")\n",
        "\n",
        "mlm_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=bert_tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=bert,\n",
        "    args=args,\n",
        "    train_dataset=mlm_dataset,\n",
        "    data_collator=mlm_collator\n",
        ")\n",
        "\n",
        "trainer_output = trainer.train()\n"
      ],
      "metadata": {
        "id": "WY17XFti8pbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Pretrained Model\n",
        "\n",
        "After pretraining, we can test the model using the fill-mask pipeline.\n"
      ],
      "metadata": {
        "id": "e8dfUuM88qfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "fill_mask = pipeline(\"fill-mask\", model=bert, tokenizer=bert_tokenizer)\n",
        "predictions = fill_mask(\"The capital of [MASK] is Rome.\")\n",
        "predictions[0]\n",
        "why is"
      ],
      "metadata": {
        "id": "jKQmF-Dq8rVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performs poorly because it was trained for too few epochs.\n",
        "In practice, BERT was trained for **days on TPUs**.\n",
        "\n",
        "This is why most users fine-tune pretrained checkpoints instead of training\n",
        "from scratch.\n"
      ],
      "metadata": {
        "id": "XD_v_yrw9slS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performs poorly because it was trained for too few epochs.\n",
        "In practice, BERT was trained for **days on TPUs**.\n",
        "\n",
        "This is why most users fine-tune pretrained checkpoints instead of training\n",
        "from scratch."
      ],
      "metadata": {
        "id": "gXgtfjPd9uIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning BERT\n",
        "\n",
        "BERT can be fine-tuned for many tasks with minimal architectural changes:\n"
      ],
      "metadata": {
        "id": "e3lvebuW-LYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Classification\n",
        "- Use `[CLS]` embedding\n",
        "- Add a classification head\n",
        "- Optimize cross-entropy loss\n"
      ],
      "metadata": {
        "id": "r_ubHl9M-QZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token Classification (e.g., NER)\n",
        "- Apply classification head to each token\n",
        "- Common in legal, medical, and financial NLP\n"
      ],
      "metadata": {
        "id": "EnD97IK8-RUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Pair Tasks\n",
        "- Natural Language Inference (NLI)\n",
        "- Paraphrase detection\n",
        "- Question‚Äìanswer matching\n"
      ],
      "metadata": {
        "id": "L3Z7xLEb-YTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple-Choice Question Answering\n",
        "- Run BERT once per candidate answer\n",
        "- Use softmax over answer scores\n"
      ],
      "metadata": {
        "id": "0Fqf7jfc-ZCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extractive Question Answering\n",
        "- Predict start and end token indices\n",
        "- Use two logits per token\n"
      ],
      "metadata": {
        "id": "6N8THjUo-aDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Embeddings and SBERT\n",
        "\n",
        "Standard BERT is inefficient for semantic similarity at scale.\n",
        "Sentence-BERT (SBERT) solves this by producing fixed sentence embeddings.\n"
      ],
      "metadata": {
        "id": "LTGut_8D-a3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n"
      ],
      "metadata": {
        "id": "sQ1ZBiln-iXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "sentences = [\n",
        "    \"She's shopping\",\n",
        "    \"She bought some shoes\",\n",
        "    \"She's working\"\n",
        "]\n",
        "\n",
        "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "similarities = model.similarity(embeddings, embeddings)\n",
        "similarities\n"
      ],
      "metadata": {
        "id": "JZwQSDK2-b56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence embeddings enable:\n",
        "- Semantic search\n",
        "- Document clustering\n",
        "- Reranking search results\n",
        "- Fast similarity comparisons\n"
      ],
      "metadata": {
        "id": "J093CPES-mhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Encoder-Only Models\n",
        "\n",
        "- **RoBERTa**: Better training, dynamic masking, no NSP\n",
        "- **DistilBERT**: Smaller, faster, distilled from BERT\n",
        "- **ALBERT**: Parameter sharing and factorized embeddings\n",
        "- **ELECTRA**: Replaced token detection (RTD)\n",
        "- **DeBERTa**: Disentangled attention with relative positions\n"
      ],
      "metadata": {
        "id": "18NMWGFb-ngX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Thoughts\n",
        "\n",
        "Encoder-only models remain extremely valuable:\n",
        "- Smaller and faster than decoders\n",
        "- Easy to fine-tune\n",
        "- Ideal for NLU tasks\n",
        "\n",
        "Next up: **Decoder-only models like GPT** üöÄ\n"
      ],
      "metadata": {
        "id": "e8M7ZpDf-ouv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder-Only Transformers\n",
        "\n",
        "While Google was working on the first encoder-only model (BERT), OpenAI researchers\n",
        "took a different route and built the first decoder-only model: **GPT**.\n",
        "\n",
        "GPT stands for *Generative Pre-Training*. These models predict the **next token**\n",
        "given all previous tokens, making them ideal for text generation.\n",
        "\n",
        "Decoder-only models are the foundation of modern systems such as ChatGPT and\n",
        "Claude.\n"
      ],
      "metadata": {
        "id": "Z4Cd5HtN_n8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-1 Overview\n",
        "\n",
        "GPT-1 was released in June 2018 and pretrained on approximately 7,000 books.\n",
        "\n",
        "Training objective:\n",
        "- Predict the next token for every position in the sequence\n",
        "\n",
        "This allows the model to generate text one token at a time.\n"
      ],
      "metadata": {
        "id": "CTYZGfNqAHRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation with Decoder-Only Models\n",
        "\n",
        "Given an input such as:\n",
        "\n",
        "\"Happy birthday\"\n",
        "\n",
        "The model predicts the next token:\n",
        "\"to\"\n",
        "\n",
        "This token is appended to the input, and the process repeats.\n"
      ],
      "metadata": {
        "id": "MllmBNgxAICf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strengths and Weaknesses of Decoder-Only Models\n",
        "\n",
        "Decoder-only models excel at:\n",
        "- Text generation\n",
        "- Code generation\n",
        "- Question answering\n",
        "- Chatbots\n",
        "- Reasoning (to some extent)\n",
        "\n",
        "They are less efficient for:\n",
        "- Classification\n",
        "- Tasks requiring full bidirectional context\n",
        "\n",
        "Encoder-only models are often faster and smaller for NLU tasks.\n"
      ],
      "metadata": {
        "id": "KNbTuEJhEx3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WARNING: Inference Cost\n",
        "\n",
        "Decoder-only models are **autoregressive**:\n",
        "- One forward pass per generated token\n",
        "\n",
        "Encoder-only models process the input once.\n",
        "\n",
        "Caching past key/value states significantly improves decoder performance.\n"
      ],
      "metadata": {
        "id": "5q8cffFSEzGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-1 Architecture and Pretraining\n",
        "\n",
        "Pretraining details:\n",
        "- Sequences of exactly 512 tokens\n",
        "- No padding tokens\n",
        "- No start-of-sequence or end-of-sequence tokens\n",
        "- Next-token prediction for all positions\n",
        "\n",
        "This ensures uniform training data across token positions.\n"
      ],
      "metadata": {
        "id": "F2OUaVJWE0Bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-1 Architecture Differences from the Original Transformer\n",
        "\n",
        "1. No cross-attention layers\n",
        "   - Only masked self-attention + feedforward layers\n",
        "\n",
        "2. Larger scale\n",
        "   - 12 decoder layers\n",
        "   - 12 attention heads\n",
        "   - 768-dimensional embeddings\n",
        "   - 117 million parameters\n"
      ],
      "metadata": {
        "id": "ETYvt8y4E1AV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-1 Architecture Differences from the Original Transformer\n",
        "\n",
        "1. No cross-attention layers\n",
        "   - Only masked self-attention + feedforward layers\n",
        "\n",
        "2. Larger scale\n",
        "   - 12 decoder layers\n",
        "   - 12 attention heads\n",
        "   - 768-dimensional embeddings\n",
        "   - 117 million parameters\n"
      ],
      "metadata": {
        "id": "V7kUZgxYE1_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-1 Fine-Tuning Tasks\n",
        "\n",
        "GPT-1 was fine-tuned for multiple tasks with minimal architectural changes.\n"
      ],
      "metadata": {
        "id": "NdiTjSwPE3Pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Classification\n",
        "\n",
        "- Add a classification head on top of the **last token**\n",
        "- Use cross-entropy loss\n"
      ],
      "metadata": {
        "id": "3adph-oBE4EJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Pair Tasks\n",
        "\n",
        "- Concatenate sentences using a delimiter token (`$`)\n",
        "- Add a classification head on the last token\n"
      ],
      "metadata": {
        "id": "v2NJEvhGE47X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic Similarity\n",
        "\n",
        "- Run the model twice:\n",
        "  - Sentence A $ Sentence B\n",
        "  - Sentence B $ Sentence A\n",
        "- Sum final token embeddings\n",
        "- Feed to a regression head\n"
      ],
      "metadata": {
        "id": "3lFtLkFLE6G4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Choice Question Answering\n",
        "\n",
        "- Run model once per answer choice\n",
        "- Score each using last token\n",
        "- Apply softmax across answers\n"
      ],
      "metadata": {
        "id": "qdk9TcSDE7UN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-2 and Zero-Shot Learning\n",
        "\n",
        "GPT-2 was released in February 2019 and scaled up dramatically.\n",
        "\n",
        "Largest version:\n",
        "- 48 layers\n",
        "- 20 attention heads\n",
        "- 1,600-dimensional embeddings\n",
        "- 1.5B parameters\n",
        "- Context length: 1,024 tokens\n"
      ],
      "metadata": {
        "id": "w2rPxGfzE8Hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WebText Dataset\n",
        "\n",
        "GPT-2 was trained on **WebText**, a curated dataset of ~8M high-quality web pages\n",
        "linked from popular Reddit posts.\n",
        "\n",
        "This improved data quality compared to Common Crawl.\n"
      ],
      "metadata": {
        "id": "lv3s33D0E9Dr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-Shot Learning (ZSL)\n",
        "\n",
        "GPT-2 performs many tasks **without fine-tuning**, simply by prompt formatting.\n",
        "\n",
        "Examples:\n",
        "- Question answering using \"Q: ... A:\"\n",
        "- Summarization using \"TL;DR:\"\n",
        "- Translation using examples in the prompt\n"
      ],
      "metadata": {
        "id": "BlE-sI6wE-GG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling Laws\n",
        "\n",
        "Zero-shot performance improves roughly log-linearly with model size.\n",
        "\n",
        "Bigger models ‚Üí better generalization\n"
      ],
      "metadata": {
        "id": "rDIGyBRAE_Td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-3 and In-Context Learning\n",
        "\n",
        "GPT-3 (2020):\n",
        "- ~40B parameters\n",
        "- ~570GB of training data\n",
        "\n",
        "Key insight:\n",
        "- Models can learn tasks from **examples in the prompt**\n"
      ],
      "metadata": {
        "id": "h31vBXYxFAeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In-Context Learning (ICL)\n",
        "\n",
        "ICL means:\n",
        "- No gradient updates\n",
        "- No fine-tuning\n",
        "- Task examples provided directly in the prompt\n",
        "\n",
        "Includes:\n",
        "- One-shot learning (OSL)\n",
        "- Few-shot learning (FSL)\n"
      ],
      "metadata": {
        "id": "BINA84f9FBh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using GPT-2 with Hugging Face Transformers\n"
      ],
      "metadata": {
        "id": "E4kKrWR0FCmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
      ],
      "metadata": {
        "id": "l_6hp9pk_nnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"gpt2\"\n",
        "\n",
        "gpt2_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "gpt2 = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    dtype=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZjgepHII-dD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation Helper Function\n"
      ],
      "metadata": {
        "id": "UxZ6TsfsFFiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, tokenizer, prompt, max_new_tokens=50, **generate_kwargs):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        **generate_kwargs\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "JDm_Sb6DFGYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Greedy Decoding Example\n"
      ],
      "metadata": {
        "id": "xQhtGdS-t2N9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Scientists found a talking unicorn today. Here's the full story:\"\n",
        "generate(gpt2, gpt2_tokenizer, prompt)\n"
      ],
      "metadata": {
        "id": "fw7n7ruVt27S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Greedy decoding often causes repetition.\n",
        "To fix this, we enable sampling.\n"
      ],
      "metadata": {
        "id": "zJuwY3jXt3xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(42)\n",
        "generate(gpt2, gpt2_tokenizer, prompt, do_sample=True)\n"
      ],
      "metadata": {
        "id": "qHXT2oV1t4iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling Controls\n",
        "\n",
        "Common generation parameters:\n",
        "- temperature\n",
        "- top_k\n",
        "- top_p (nucleus sampling)\n",
        "- num_beams\n"
      ],
      "metadata": {
        "id": "FU6TYN54t5Zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "generate(\n",
        "    gpt2,\n",
        "    gpt2_tokenizer,\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    top_p=0.6\n",
        ")\n"
      ],
      "metadata": {
        "id": "tXhqgJLjt6M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering with In-Context Learning\n"
      ],
      "metadata": {
        "id": "Q2UGQNyEt7Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_TEMPLATE = (\n",
        "    \"Capital city of France = Paris\\n\"\n",
        "    \"Capital city of {country} =\"\n",
        ")\n",
        "\n",
        "def get_capital_city(model, tokenizer, country, template=DEFAULT_TEMPLATE):\n",
        "    prompt = template.format(country=country)\n",
        "    extended = generate(model, tokenizer, prompt, max_new_tokens=10)\n",
        "    answer = extended[len(prompt):]\n",
        "    return answer.strip().splitlines()[0].strip()\n"
      ],
      "metadata": {
        "id": "1dmrBAfNt8B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_capital_city(gpt2, gpt2_tokenizer, \"United Kingdom\")\n"
      ],
      "metadata": {
        "id": "okQ91uVjt89Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_capital_city(gpt2, gpt2_tokenizer, \"Mexico\")\n"
      ],
      "metadata": {
        "id": "If4PggwXt9sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations of GPT-2\n",
        "\n",
        "- Common factual errors\n",
        "- Biases from web data\n",
        "- Hallucinations\n",
        "- Performance improves with model size\n"
      ],
      "metadata": {
        "id": "pyxwWlOpt-iA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Larger Decoder Models: Mistral-7B\n",
        "\n",
        "Mistral-7B:\n",
        "- 7 billion parameters\n",
        "- Apache 2.0 license\n",
        "- Advanced attention mechanisms\n",
        "- Runs on Colab GPUs\n"
      ],
      "metadata": {
        "id": "heXkZHAEt_bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Mistral-7B\n"
      ],
      "metadata": {
        "id": "qXnGyGI9uJ1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
      ],
      "metadata": {
        "id": "n-t39PLzuKxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"mistralai/Mistral-7B-v0.3\"\n",
        "\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "mistral = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    dtype=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "RynziIituLpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Notes\n",
        "\n",
        "Decoder-only models:\n",
        "- Scale extremely well\n",
        "- Enable zero-shot and few-shot learning\n",
        "- Power modern LLM systems\n",
        "\n",
        "Next: **Chat fine-tuning and instruction tuning**\n"
      ],
      "metadata": {
        "id": "a-_o76LjuZQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder-Only Transformers\n",
        "\n",
        "While Google was working on the first encoder-only model (i.e., BERT), Alec Radford and other OpenAI researchers were taking a different route: they built the first decoder-only model, named GPT. This model paved the way for today‚Äôs most impressive models, including most of the ones used in famous chatbots like ChatGPT or Claude.\n",
        "\n",
        "The GPT model (now known as GPT-1) was released in June 2018. GPT stands for **Generative Pre-Training**: it was pretrained on a dataset of about 7,000 books and learned to predict the next token, so it can be used to generate text one token at a time, just like the original Transformer‚Äôs decoder.\n",
        "\n",
        "For example, if you feed it *‚ÄúHappy birthday‚Äù*, it will predict *‚Äúbirthday to‚Äù*, so you can append *‚Äúto‚Äù* to the input and repeat the process.\n"
      ],
      "metadata": {
        "id": "2_qKUbTfuyfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What Decoder-Only Models Are Good At\n",
        "\n",
        "Decoder-only models excel at:\n",
        "\n",
        "- Text generation and auto-completion  \n",
        "- Code generation  \n",
        "- Question answering (free-form answers)  \n",
        "- Math and logical reasoning (to some extent)  \n",
        "- Chatbots  \n",
        "\n",
        "They can also be used for summarization or translation, but encoder‚Äìdecoder models are often better at these tasks because the encoder provides a stronger understanding of the source text.\n",
        "\n",
        "Decoder-only models can perform text classification, but encoder-only models are usually faster and more parameter-efficient for this task.\n"
      ],
      "metadata": {
        "id": "7rHXcApDvH08"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Warning: Autoregressive Inference\n",
        "\n",
        "At inference time:\n",
        "\n",
        "- **Encoder-only models** process the input once.\n",
        "- **Decoder-only models** generate one token at a time.\n",
        "\n",
        "This makes generation sequential and slower, but decoder-only models benefit heavily from **key‚Äìvalue caching**, which greatly speeds up inference.\n"
      ],
      "metadata": {
        "id": "3kCi0nWzvPOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-1 Architecture and Generative Pretraining\n",
        "\n",
        "During pretraining, GPT-1:\n",
        "\n",
        "- Used batches of 64 sequences\n",
        "- Each sequence was exactly 512 tokens long\n",
        "- Predicted the next token for *every* input token\n",
        "- Used no padding and no special tokens (no BOS/EOS)\n",
        "\n",
        "Compared to BERT, GPT-1‚Äôs training was simpler and provided equal data exposure to all token positions.\n"
      ],
      "metadata": {
        "id": "01FMfbUYvQTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Architectural Differences vs Transformer Decoder\n",
        "\n",
        "GPT-1 differs from the original Transformer decoder in two major ways:\n",
        "\n",
        "1. **No cross-attention**  \n",
        "   - There is no encoder output to attend to.\n",
        "   - Each block contains:\n",
        "     - Masked multi-head self-attention\n",
        "     - A feedforward network\n",
        "     - Skip connections and layer normalization\n",
        "\n",
        "2. **Much larger model**\n",
        "   - 12 decoder layers (instead of 6)\n",
        "   - Embedding size: 768\n",
        "   - 12 attention heads\n",
        "   - ~117 million parameters\n"
      ],
      "metadata": {
        "id": "87eXRLrivRNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Important PyTorch Warning\n",
        "\n",
        "You **cannot** use `nn.TransformerDecoder` to build a decoder-only model because it always includes cross-attention layers.\n",
        "\n",
        "Instead:\n",
        "- Use `nn.TransformerEncoder`\n",
        "- Always apply a **causal (triangular) mask**\n"
      ],
      "metadata": {
        "id": "SeG2k7HyvSnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-2 and Zero-Shot Learning\n",
        "\n",
        "GPT-2 was released in February 2019 and scaled the GPT-1 architecture dramatically.\n",
        "\n",
        "The largest GPT-2 model:\n",
        "- 48 decoder layers\n",
        "- 20 attention heads\n",
        "- Embedding size of 1600\n",
        "- Context window of 1024 tokens\n",
        "- Over 1.5 billion parameters\n",
        "\n",
        "It was trained on **WebText**, a curated dataset of high-quality web pages.\n"
      ],
      "metadata": {
        "id": "lAv3NCQvvTnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-Shot Learning (ZSL)\n",
        "\n",
        "GPT-2 could perform many tasks *without fine-tuning*:\n",
        "\n",
        "- **Question answering**\n",
        "  - Prompt: `What is the capital of New Zealand? A:`\n",
        "- **Summarization**\n",
        "  - Prompt ends with: `TL;DR:`\n",
        "- **Translation**\n",
        "  - Provide a few examples, then a new sentence\n",
        "\n",
        "Performance improved predictably with model size (log-linear scaling).\n"
      ],
      "metadata": {
        "id": "_VtKCYlHvUnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using GPT-2 to Generate Text\n",
        "\n",
        "We can use Hugging Face‚Äôs Transformers library to load GPT-2.\n"
      ],
      "metadata": {
        "id": "ailK5vQ6vVk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"gpt2\"\n",
        "gpt2_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "gpt2 = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, device_map=\"auto\", dtype=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "8QR2y0E_vWTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why These Arguments Matter\n",
        "\n",
        "- `device_map=\"auto\"` automatically places the model on the best device (GPU if available).\n",
        "- `dtype=\"auto\"` uses mixed precision (float16) when supported, saving memory and improving speed.\n"
      ],
      "metadata": {
        "id": "x6VvKWzDvXNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Why These Arguments Matter\n",
        "\n",
        "- `device_map=\"auto\"` automatically places the model on the best device (GPU if available).\n",
        "- `dtype=\"auto\"` uses mixed precision (float16) when supported, saving memory and improving speed.\n"
      ],
      "metadata": {
        "id": "XdZjMmwjvYLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, `generate()` uses greedy decoding, which often causes repetition.\n",
        "For creative text generation, enable sampling.\n"
      ],
      "metadata": {
        "id": "pjYEI_gDvZJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Scientists found a talking unicorn today. Here's the full story:\"\n",
        "generate(gpt2, gpt2_tokenizer, prompt)\n"
      ],
      "metadata": {
        "id": "i1y6wNhEvsAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(42)\n",
        "generate(gpt2, gpt2_tokenizer, prompt, do_sample=True, top_p=0.6)\n"
      ],
      "metadata": {
        "id": "XyXD00mmvssL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using GPT-2 for Question Answering (In-Context Learning)\n"
      ],
      "metadata": {
        "id": "Bj6kQL7lvt53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_TEMPLATE = \"Capital city of France = Paris\\nCapital city of {country} =\"\n",
        "\n",
        "def get_capital_city(model, tokenizer, country, template=DEFAULT_TEMPLATE):\n",
        "    prompt = template.format(country=country)\n",
        "    extended_text = generate(model, tokenizer, prompt, max_new_tokens=10)\n",
        "    answer = extended_text[len(prompt):]\n",
        "    return answer.strip().splitlines()[0].strip()\n"
      ],
      "metadata": {
        "id": "OFuvqd0Lvu0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_capital_city(gpt2, gpt2_tokenizer, \"United Kingdom\")\n",
        "get_capital_city(gpt2, gpt2_tokenizer, \"Mexico\")\n"
      ],
      "metadata": {
        "id": "tfeLkTovvvyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Turning a Large Language Model into a Chatbot\n",
        "\n",
        "To build a chatbot, you need more than a base model. For example, let‚Äôs try asking Mistral-7B for something:\n"
      ],
      "metadata": {
        "id": "f6voXtvdwR98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"List some places I should visit in Paris.\"\n",
        "generate(mistral7b, mistral7b_tokenizer, prompt)\n"
      ],
      "metadata": {
        "id": "6f-R8CWVwryB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model does not answer the question; it simply *completes* it. This is expected behavior for a base language model.\n",
        "\n",
        "To make the model conversational, we can apply **prompt engineering**, which consists of carefully crafting the prompt so the model behaves like a helpful chatbot.\n"
      ],
      "metadata": {
        "id": "lgsD4jqowtig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Engineering with Role Tags\n",
        "\n",
        "We can introduce the model to a fictional chatbot persona and explicitly mark who is speaking.\n"
      ],
      "metadata": {
        "id": "HYbjQETRwugf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bob_introduction = \"\"\"\n",
        "Bob is an amazing chatbot. It knows everything and it's incredibly helpful.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "cMenFn61wviH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_prompt = f\"{bob_introduction}Me: {prompt}\\nBob:\"\n"
      ],
      "metadata": {
        "id": "p9F6bjtDwyze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extended_text = generate(\n",
        "    mistral7b,\n",
        "    mistral7b_tokenizer,\n",
        "    full_prompt,\n",
        "    max_new_tokens=100\n",
        ")\n",
        "\n",
        "answer = extended_text[len(full_prompt):].strip()\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "TYrdoavRwzwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model now answers correctly, but it continues generating the conversation.\n",
        "\n",
        "To fix this, we can stop generation when the conversation returns to ‚ÄúMe:‚Äù.\n"
      ],
      "metadata": {
        "id": "zyfnT6rUw0yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer.split(\"\\nMe: \")[0]\n"
      ],
      "metadata": {
        "id": "uX1xx845w18R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now suppose we want to continue the same conversation. We must keep the entire conversation context and append new turns to it.\n"
      ],
      "metadata": {
        "id": "GU9OQYDGw21q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BobTheChatbot:\n",
        "    def __init__(self, model, tokenizer, introduction=bob_introduction,\n",
        "                 max_answer_length=10_000):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.context = introduction\n",
        "        self.max_answer_length = max_answer_length\n",
        "\n",
        "    def chat(self, prompt):\n",
        "        self.context += \"\\nMe: \" + prompt + \"\\nBob:\"\n",
        "        context = self.context\n",
        "        start_index = len(context)\n",
        "\n",
        "        while True:\n",
        "            extended = generate(\n",
        "                self.model,\n",
        "                self.tokenizer,\n",
        "                context,\n",
        "                max_new_tokens=100\n",
        "            )\n",
        "            answer = extended[start_index:]\n",
        "\n",
        "            if (\"\\nMe: \" in answer or\n",
        "                extended == context or\n",
        "                len(answer) >= self.max_answer_length):\n",
        "                break\n",
        "\n",
        "            context = extended\n",
        "\n",
        "        answer = answer.split(\"\\nMe: \")[0]\n",
        "        self.context += answer\n",
        "        return answer.strip()\n"
      ],
      "metadata": {
        "id": "a-Zhd6Fkw3yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bob = BobTheChatbot(mistral7b, mistral7b_tokenizer)\n",
        "\n",
        "bob.chat(\"List some places I should visit in Paris.\")\n",
        "bob.chat(\"Tell me more about the first place.\")\n",
        "bob.chat(\"And Rome?\")\n"
      ],
      "metadata": {
        "id": "rxjiWZTmw5RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a working chatbot in about 20 lines of code.\n",
        "\n",
        "However, several problems remain:\n",
        "- The chatbot can repeat itself.\n",
        "- Its answers are often shallow.\n",
        "- It may produce unsafe or illegal advice.\n",
        "\n",
        "Prompt engineering helps, but it is not sufficient.\n"
      ],
      "metadata": {
        "id": "XrEoTxIgw6PR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Engineering Techniques\n",
        "\n",
        "Popular prompt engineering techniques include:\n",
        "\n",
        "- Rephrasing instructions\n",
        "- Adding examples (few-shot prompting)\n",
        "- Assigning a role or personality\n",
        "- Specifying output format and style\n",
        "- Prompt chaining (multi-step prompts)\n",
        "- Chain-of-thought (CoT) prompting\n",
        "- Tree-of-thoughts (ToT)\n",
        "- Self-critique and refinement\n"
      ],
      "metadata": {
        "id": "vsMnSoazw7Xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hallucinations and Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "LLMs can hallucinate facts. To reduce this:\n",
        "- Retrieve relevant information from trusted sources\n",
        "- Inject this information into the prompt\n",
        "- Let the model answer using this context\n",
        "\n",
        "This approach is called **Retrieval-Augmented Generation (RAG)**.\n"
      ],
      "metadata": {
        "id": "H-YS3SFnw8aE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning a Model for Chatting\n",
        "\n",
        "Building a reliable chatbot typically requires two fine-tuning steps:\n",
        "\n",
        "1. **Supervised Fine-Tuning (SFT)**\n",
        "2. **Fine-Tuning with Human Preferences (RLHF or DPO)**\n",
        "\n",
        "This process turns a base model into an instruct and conversational model.\n"
      ],
      "metadata": {
        "id": "QjyJCuhbw9VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised Fine-Tuning (SFT)\n",
        "\n",
        "The model is trained on curated instruction‚Äìresponse pairs.\n",
        "\n",
        "Loss masking is often used so that:\n",
        "- The loss is computed only on answer tokens\n",
        "- The model focuses on improving responses\n"
      ],
      "metadata": {
        "id": "v1oDBGn_w-cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reinforcement Learning from Human Feedback (RLHF)\n",
        "\n",
        "RLHF:\n",
        "- Trains a reward model from human rankings\n",
        "- Uses PPO to optimize the model\n",
        "- Prevents excessive drift from the base model\n",
        "\n",
        "This approach is powerful but complex and unstable.\n"
      ],
      "metadata": {
        "id": "7WtnSA_iw_Hg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Direct Preference Optimization (DPO)\n",
        "\n",
        "DPO is a simpler alternative to RLHF:\n",
        "- No reward model\n",
        "- No reinforcement learning\n",
        "- More stable and data-efficient\n",
        "\n",
        "Each training sample includes:\n",
        "- A prompt\n",
        "- A preferred answer\n",
        "- A rejected answer\n"
      ],
      "metadata": {
        "id": "uDn2ZyAyxAcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The capital of Argentina is \"\n",
        "full_input = [prompt + \"Buenos Aires\", prompt + \"Madrid\"]\n",
        "\n",
        "mistral7b_tokenizer.pad_token = mistral7b_tokenizer.eos_token\n",
        "encodings = mistral7b_tokenizer(\n",
        "    full_input, return_tensors=\"pt\", padding=True\n",
        ").to(device)\n",
        "\n",
        "logits = mistral7b(**encodings).logits\n"
      ],
      "metadata": {
        "id": "tDLCq2nyxBaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "next_token_ids = encodings.input_ids[:, 1:]\n",
        "next_token_log_probas = -F.cross_entropy(\n",
        "    logits[:, :-1].permute(0, 2, 1),\n",
        "    next_token_ids,\n",
        "    reduction=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "kofvnyfwxZHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padding_mask = encodings.attention_mask[:, :-1]\n",
        "log_probas_sum = (next_token_log_probas * padding_mask).sum(dim=1)\n",
        "log_probas_sum\n"
      ],
      "metadata": {
        "id": "7HbVXXthxaFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dpo_loss(model, ref_model, tokenizer, full_input_c, full_input_r, beta=0.1):\n",
        "    p_c = sum_of_log_probas(model, tokenizer, full_input_c)\n",
        "    p_r = sum_of_log_probas(model, tokenizer, full_input_r)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        p_ref_c = sum_of_log_probas(ref_model, tokenizer, full_input_c)\n",
        "        p_ref_r = sum_of_log_probas(ref_model, tokenizer, full_input_r)\n",
        "\n",
        "    return -F.logsigmoid(\n",
        "        beta * ((p_c - p_ref_c) - (p_r - p_ref_r))\n",
        "    ).mean()\n"
      ],
      "metadata": {
        "id": "PNPNrSR4xbMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning with the TRL Library\n",
        "\n",
        "The Hugging Face TRL library supports:\n",
        "- SFT\n",
        "- RLHF\n",
        "- DPO\n",
        "\n",
        "We will use:\n",
        "- Alpaca dataset for SFT\n",
        "- Anthropic HH-RLHF dataset for DPO\n"
      ],
      "metadata": {
        "id": "HfHz0E_yxcHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "sft_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "print(sft_dataset[1][\"text\"])\n"
      ],
      "metadata": {
        "id": "L6HNjZ-nxdBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "sft_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "print(sft_dataset[1][\"text\"])\n"
      ],
      "metadata": {
        "id": "XwZf5wbxxd0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "sft_model_dir = \"./my_gpt2_sft_alpaca\"\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    output_dir=sft_model_dir,\n",
        "    max_length=512,\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=50,\n",
        "    logging_steps=10,\n",
        "    learning_rate=5e-5,\n",
        ")\n",
        "\n",
        "sft_trainer = SFTTrainer(\n",
        "    \"gpt2\",\n",
        "    train_dataset=sft_dataset,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "sft_trainer.train()\n",
        "sft_trainer.model.save_pretrained(sft_model_dir)\n"
      ],
      "metadata": {
        "id": "r3zvuDlgxexD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pref_dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")\n",
        "pref_dataset[2].keys()\n"
      ],
      "metadata": {
        "id": "YKcFD0qKxfwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import DPOTrainer, DPOConfig\n",
        "\n",
        "dpo_model_dir = \"./my_gpt2_sft_alpaca_dpo_hh_rlhf\"\n",
        "\n",
        "training_args = DPOConfig(\n",
        "    output_dir=dpo_model_dir,\n",
        "    max_length=512,\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=50,\n",
        "    logging_steps=10,\n",
        "    learning_rate=2e-5,\n",
        ")\n",
        "\n",
        "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    sft_model_dir,\n",
        "    args=training_args,\n",
        "    train_dataset=pref_dataset,\n",
        "    processing_class=gpt2_tokenizer,\n",
        ")\n",
        "\n",
        "dpo_trainer.train()\n",
        "dpo_trainer.model.save_pretrained(dpo_model_dir)\n"
      ],
      "metadata": {
        "id": "fz_i9nnVxguz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You now understand the full pipeline:\n",
        "\n",
        "- Transformer architecture\n",
        "- Pretraining with next-token prediction\n",
        "- Supervised fine-tuning (SFT)\n",
        "- Preference alignment with DPO\n",
        "- Deployment in a tool-augmented chatbot system\n",
        "\n",
        "This is exactly how modern chatbots are built.\n"
      ],
      "metadata": {
        "id": "xcasTjmCxh74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder‚ÄìDecoder Models\n",
        "\n",
        "In this chapter, other than the original Transformer architecture, we have focused solely on encoder-only and decoder-only models. This might have given you the impression that encoder‚Äìdecoder models are over, but for some problems, they are still very relevant‚Äîespecially for tasks like **translation** or **summarization**.\n",
        "\n",
        "Since the encoder is **bidirectional**, it can encode the source text and output excellent contextual embeddings, which the decoder can then use to produce a better output than a decoder-only model would (at least for models of a similar size).\n",
        "\n",
        "---\n",
        "\n",
        "## T5: Text-to-Text Transfer Transformer\n",
        "\n",
        "The **T5 model** (released by Google in 2019) is a particularly influential encoder‚Äìdecoder model. It was the first to frame *all* NLP tasks as **text-to-text** problems.\n",
        "\n",
        "Examples:\n",
        "\n",
        "- **Translation**  \n",
        "  Input:  \n",
        "  `translate English to Spanish: I like soccer`  \n",
        "  Output:  \n",
        "  `me gusta el f√∫tbol`\n",
        "\n",
        "- **Summarization**  \n",
        "  Input:  \n",
        "  `summarize: <paragraph>`  \n",
        "  Output:  \n",
        "  `<summary>`\n",
        "\n",
        "- **Classification**  \n",
        "  Input:  \n",
        "  `classify: <text>`  \n",
        "  Output:  \n",
        "  `<class name>`\n",
        "\n",
        "For **zero-shot classification**, the possible classes can simply be listed in the prompt.\n",
        "\n",
        "This unified text-to-text approach makes T5:\n",
        "- Very easy to pretrain on diverse tasks\n",
        "- Very easy to use at inference time\n",
        "\n",
        "T5 was pretrained using a **masked span corruption objective**, similar to MLM, but masking one or more **contiguous spans** instead of individual tokens.\n",
        "\n",
        "---\n",
        "\n",
        "## Variants of T5\n",
        "\n",
        "Google released several variants of T5:\n",
        "\n",
        "### mT5 (2020)\n",
        "A multilingual T5 supporting **over 100 languages**.  \n",
        "Well-suited for:\n",
        "- Translation\n",
        "- Cross-lingual tasks (e.g., asking questions in English about Spanish text)\n",
        "\n",
        "### ByT5 (2021)\n",
        "A **byte-level** variant of T5 that removes the need for tokenization entirely (not even BPE).  \n",
        "This approach has not become widely adopted, as tokenizers are generally more efficient.\n",
        "\n",
        "### FLAN-T5 (2022)\n",
        "An **instruction-tuned** version of T5 with excellent:\n",
        "- Zero-shot learning (ZSL)\n",
        "- Few-shot learning (FSL)\n",
        "\n",
        "### UL2 (2022)\n",
        "Pretrained using **multiple objectives**, including:\n",
        "- Masked span denoising (like T5)\n",
        "- Standard next-token prediction\n",
        "- Masked token prediction\n",
        "\n",
        "### FLAN-UL2 (2023)\n",
        "An improved version of UL2 using **instruction tuning**.\n",
        "\n",
        "---\n",
        "\n",
        "## Encoder‚ÄìDecoder Models from Meta\n",
        "\n",
        "Meta released several encoder‚Äìdecoder models, starting with **BART** in 2020.\n",
        "\n",
        "BART was pretrained using a **denoising objective**:\n",
        "- The input text is corrupted (masked, deleted, shuffled, modified, or inserted tokens)\n",
        "- The model must reconstruct the original text\n",
        "\n",
        "BART is particularly effective for:\n",
        "- Text generation\n",
        "- Summarization\n",
        "\n",
        "A multilingual variant called **mBART** is also available.\n",
        "\n",
        "---\n",
        "\n",
        "## Beyond NLP\n",
        "\n",
        "Encoder‚Äìdecoder architectures are also common outside NLP:\n",
        "\n",
        "- **Vision models**, especially for:\n",
        "  - Object detection\n",
        "  - Image segmentation\n",
        "- **Multimodal models**, combining text, vision, audio, or other modalities\n",
        "\n",
        "This brings us to the next chapter, where we will explore **vision transformers and multimodal transformers**.\n",
        "\n",
        "It‚Äôs time for transformers to open their eyes! üëÄ\n"
      ],
      "metadata": {
        "id": "up-zbAbiyL1u"
      }
    }
  ]
}