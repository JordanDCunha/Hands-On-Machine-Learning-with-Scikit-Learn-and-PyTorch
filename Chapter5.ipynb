{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlEpPa0nj06RsV1X46tN6V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanDCunha/Hands-On-Machine-Learning-with-Scikit-Learn-and-PyTorch/blob/main/Chapter5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 5. Decision Trees\n",
        "\n",
        "Decision trees are versatile machine learning algorithms that can perform\n",
        "classification, regression, and even multioutput tasks.\n",
        "\n",
        "They are powerful models capable of fitting very complex datasets.\n"
      ],
      "metadata": {
        "id": "Dn5qpow0oBz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees are also the fundamental building blocks of **random forests**\n",
        "and **gradient boosting**, which are among the most powerful machine learning\n",
        "methods available.\n"
      ],
      "metadata": {
        "id": "RRXlSjucoDlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this chapter, we will learn how to:\n",
        "- Train decision trees\n",
        "- Visualize them\n",
        "- Make predictions\n",
        "- Understand how they split data\n",
        "- Regularize them to reduce overfitting\n"
      ],
      "metadata": {
        "id": "LPAmaUDjoEfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Decision Tree Classifier\n",
        "\n",
        "We will use the Iris dataset and train a decision tree to classify flowers\n",
        "based on petal length and petal width.\n"
      ],
      "metadata": {
        "id": "PCWx5MjqoFgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n"
      ],
      "metadata": {
        "id": "h_cY9rmkoGWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X = iris.data[:, 2:]  # petal length and width\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "Vk-TMWIcoUi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
        "tree_clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "MjEuAUTEoVlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `max_depth` hyperparameter limits how deep the tree can grow,\n",
        "which helps prevent overfitting.\n"
      ],
      "metadata": {
        "id": "95uGjMaIoXaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing a Decision Tree\n",
        "\n",
        "One of the greatest strengths of decision trees is their interpretability.\n",
        "They can be visualized as flowcharts.\n"
      ],
      "metadata": {
        "id": "VUb5f8AsoYQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n"
      ],
      "metadata": {
        "id": "LJyGzB55oY-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_data = export_graphviz(\n",
        "    tree_clf,\n",
        "    out_file=None,\n",
        "    feature_names=iris.feature_names[2:],\n",
        "    class_names=iris.target_names,\n",
        "    rounded=True,\n",
        "    filled=True\n",
        ")\n",
        "\n",
        "graphviz.Source(dot_data)\n"
      ],
      "metadata": {
        "id": "Ho3a3J7yoZs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each node displays:\n",
        "- The feature used for splitting\n",
        "- The threshold value\n",
        "- The impurity (Gini by default)\n",
        "- The number of samples\n",
        "- The predicted class\n"
      ],
      "metadata": {
        "id": "pqjMqQFKoapE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making Predictions\n",
        "\n",
        "To classify a new instance, the decision tree follows a path from the root\n",
        "to a leaf node based on feature thresholds.\n"
      ],
      "metadata": {
        "id": "ZxXgrim_ob4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_clf.predict([[5.0, 1.5]])\n"
      ],
      "metadata": {
        "id": "wih_NTUUodKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prediction corresponds to the class stored in the reached leaf node.\n"
      ],
      "metadata": {
        "id": "V5Ui5vqOod90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CART Algorithm\n",
        "\n",
        "Scikit-Learn uses the CART (Classification and Regression Trees) algorithm.\n",
        "\n",
        "At each node, CART searches for the feature and threshold that produce the\n",
        "purest possible child nodes.\n"
      ],
      "metadata": {
        "id": "8EHNL3SToewW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For classification, purity is typically measured using **Gini impurity**.\n"
      ],
      "metadata": {
        "id": "VoeGAPWjofpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini impurity is defined as:\n",
        "\n",
        "Gini = 1 − Σ pₖ²\n",
        "\n",
        "where pₖ is the proportion of class k in the node.\n"
      ],
      "metadata": {
        "id": "SWNF1Jmaog7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Gini = 0 → perfectly pure node\n",
        "- Higher Gini → more mixed classes\n"
      ],
      "metadata": {
        "id": "bsTKeS3Mohmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CART greedily selects the split that minimizes the **weighted average**\n",
        "of impurity across child nodes.\n"
      ],
      "metadata": {
        "id": "ieuKglOXoirH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularizing Decision Trees\n",
        "\n",
        "Decision trees tend to overfit if left unconstrained.\n",
        "Scikit-Learn provides several hyperparameters for regularization.\n"
      ],
      "metadata": {
        "id": "eVPalaGbojil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common regularization parameters include:\n",
        "\n",
        "- `max_depth`\n",
        "- `min_samples_split`\n",
        "- `min_samples_leaf`\n",
        "- `max_features`\n"
      ],
      "metadata": {
        "id": "G95vTqAzokb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_clf_reg = DecisionTreeClassifier(\n",
        "    max_depth=3,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "tree_clf_reg.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "tlOqexAAouYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smaller, constrained trees usually generalize better than deep,\n",
        "fully grown trees.\n"
      ],
      "metadata": {
        "id": "DDP3hhsRovWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees for Regression\n",
        "\n",
        "Decision trees can also perform regression tasks by predicting\n",
        "continuous values.\n"
      ],
      "metadata": {
        "id": "AqgDtFywowQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n"
      ],
      "metadata": {
        "id": "v1is1NsnoxG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_reg = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
        "tree_reg.fit(X_train, y_train.astype(float))\n"
      ],
      "metadata": {
        "id": "irj6CusHox1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For regression, the tree splits data to minimize **mean squared error (MSE)**.\n",
        "\n",
        "Each leaf predicts the average target value of the samples it contains.\n"
      ],
      "metadata": {
        "id": "1oYBpjfQoyel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations of Decision Trees\n",
        "\n",
        "Despite their strengths, decision trees have important limitations.\n"
      ],
      "metadata": {
        "id": "-M8wM2XQozM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **High variance**  \n",
        "   Small changes in the data can produce very different trees.\n"
      ],
      "metadata": {
        "id": "vVvPyd4noz4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Greedy optimization**  \n",
        "   CART finds locally optimal splits, not globally optimal trees.\n"
      ],
      "metadata": {
        "id": "wO4_V7Cro0mA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Axis-aligned splits**  \n",
        "   Decision boundaries are always perpendicular to feature axes.\n"
      ],
      "metadata": {
        "id": "0J3YwLY_o1OH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Overfitting**  \n",
        "   Without regularization, trees can memorize the training data.\n"
      ],
      "metadata": {
        "id": "LyL_Paz4o14l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter Summary\n",
        "\n",
        "In this chapter, you learned:\n",
        "\n",
        "- How decision trees perform classification and regression\n",
        "- How trees are trained using the CART algorithm\n",
        "- How to visualize and interpret trees\n",
        "- How to regularize trees to reduce overfitting\n",
        "- Why trees are the foundation of ensemble methods\n",
        "\n",
        "In the next chapter, we will combine many trees together to build\n",
        "**Random Forests**.\n"
      ],
      "metadata": {
        "id": "c9uYBnwXo2vP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Visualizing a Decision Tree\n",
        "\n",
        "To understand decision trees, we will train one and examine how it makes\n",
        "predictions using the Iris dataset.\n"
      ],
      "metadata": {
        "id": "V6h7HjpQpNv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train a `DecisionTreeClassifier` using only:\n",
        "- Petal length\n",
        "- Petal width\n"
      ],
      "metadata": {
        "id": "XsYU8ly2pPkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n"
      ],
      "metadata": {
        "id": "JstPdiKGpQSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris(as_frame=True)\n",
        "\n",
        "X_iris = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
        "y_iris = iris.target\n"
      ],
      "metadata": {
        "id": "2FNHHWu3pQ9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We limit the tree depth to 2 levels to keep it simple and interpretable.\n"
      ],
      "metadata": {
        "id": "l_l8EQMHpRsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
        "tree_clf.fit(X_iris, y_iris)\n"
      ],
      "metadata": {
        "id": "Abp-EYCppSW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tree is now trained and ready to be visualized.\n"
      ],
      "metadata": {
        "id": "GmlqDmxJpTDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the Decision Tree\n",
        "\n",
        "Scikit-Learn provides the `export_graphviz()` function to convert a trained\n",
        "decision tree into a Graphviz `.dot` file.\n"
      ],
      "metadata": {
        "id": "aVulkJfLpTuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_graphviz\n"
      ],
      "metadata": {
        "id": "BvicObH2pUZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_graphviz(\n",
        "    tree_clf,\n",
        "    out_file=\"iris_tree.dot\",\n",
        "    feature_names=[\"petal length (cm)\", \"petal width (cm)\"],\n",
        "    class_names=iris.target_names,\n",
        "    rounded=True,\n",
        "    filled=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "hgWyHWU8pVIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `.dot` file describes the structure of the tree and can be rendered\n",
        "using Graphviz.\n"
      ],
      "metadata": {
        "id": "JTp9wpDCpV9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graphviz is an open-source graph visualization tool that can convert `.dot`\n",
        "files into formats such as PNG or PDF.\n"
      ],
      "metadata": {
        "id": "N5ox7fnnpX5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Source\n"
      ],
      "metadata": {
        "id": "6kBBG_qqpZYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Source.from_file(\"iris_tree.dot\")\n"
      ],
      "metadata": {
        "id": "ykrcXQDNpa0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each node in the tree shows:\n",
        "\n",
        "- The feature used for splitting\n",
        "- The threshold value\n",
        "- The impurity (Gini)\n",
        "- The number of samples\n",
        "- The predicted class\n"
      ],
      "metadata": {
        "id": "amUajWlLpbqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This visualization corresponds to **Figure 5-1: Iris decision tree**.\n",
        "\n",
        "You can follow the path from the root to a leaf to see how predictions\n",
        "are made step by step.\n"
      ],
      "metadata": {
        "id": "4QU0CVpFpccS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making Predictions\n",
        "\n",
        "Let’s examine how the trained decision tree makes predictions.\n",
        "\n",
        "We start at the root node (depth 0) and move down the tree based on\n",
        "feature-based questions until we reach a leaf node.\n"
      ],
      "metadata": {
        "id": "WDZPOZyupss9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the root node, the tree asks:\n",
        "\n",
        "**Is petal length < 2.45 cm?**\n",
        "\n",
        "- If yes → move to the left child\n",
        "- If no → move to the right child\n"
      ],
      "metadata": {
        "id": "RQQhW1hJpue_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the petal length is smaller than 2.45 cm, the left child node is a\n",
        "leaf node.\n",
        "\n",
        "This node predicts **Iris setosa**.\n"
      ],
      "metadata": {
        "id": "wuv78MKspvV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the petal length is greater than 2.45 cm, the tree moves to the\n",
        "right child node (depth 1).\n",
        "\n",
        "This node asks a second question:\n",
        "\n",
        "**Is petal width < 1.75 cm?**\n"
      ],
      "metadata": {
        "id": "MZkYu_mepwZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If yes → predict **Iris versicolor**\n",
        "- If no → predict **Iris virginica**\n",
        "\n",
        "The prediction process ends once a leaf node is reached.\n"
      ],
      "metadata": {
        "id": "HAy3t1QupxSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This top-down traversal makes decision trees easy to interpret and\n",
        "even apply manually.\n"
      ],
      "metadata": {
        "id": "JQesicUIpyCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Node Attributes\n",
        "\n",
        "Each node in the decision tree contains useful information:\n"
      ],
      "metadata": {
        "id": "lQKJDCvxpzEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **samples**: number of training instances that reach the node\n",
        "- **value**: number of instances of each class at the node\n",
        "- **gini**: Gini impurity of the node\n"
      ],
      "metadata": {
        "id": "GgWd9WPdpz5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "\n",
        "At depth 1 (right node), 100 training instances reach the node.\n",
        "Of these:\n",
        "- 54 are Iris versicolor\n",
        "- 46 are Iris virginica\n"
      ],
      "metadata": {
        "id": "gOtwCTk5p0xV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A node is **pure** when all instances belong to the same class.\n",
        "\n",
        "Pure nodes have:\n",
        "**Gini impurity = 0**\n"
      ],
      "metadata": {
        "id": "95gfM1xmp1qX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gini Impurity\n",
        "\n",
        "Gini impurity measures how mixed the classes are at a node.\n",
        "\n",
        "The more mixed the classes, the higher the impurity.\n"
      ],
      "metadata": {
        "id": "hfvH0YWsp2mB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gini impurity of node *i* is defined as:\n",
        "\n",
        "Gᵢ = 1 − Σₖ (pᵢ,ₖ)²\n",
        "\n",
        "where pᵢ,ₖ is the proportion of class k at node i.\n"
      ],
      "metadata": {
        "id": "fR7VdCWxp5w3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, if a node contains:\n",
        "\n",
        "- 0 setosa\n",
        "- 49 versicolor\n",
        "- 5 virginica\n",
        "\n",
        "out of 54 samples, the impurity is:\n",
        "\n",
        "1 − (0/54)² − (49/54)² − (5/54)² ≈ 0.168\n"
      ],
      "metadata": {
        "id": "HGXjWy3tp6yf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A perfectly pure node has Gini impurity 0.\n",
        "\n",
        "A maximally impure node has a higher value.\n"
      ],
      "metadata": {
        "id": "RuiEXrBtp71K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CART Algorithm\n",
        "\n",
        "Scikit-Learn uses the CART (Classification and Regression Trees) algorithm.\n"
      ],
      "metadata": {
        "id": "Aafrv-Ojp8rG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CART always produces **binary trees**:\n",
        "\n",
        "Each split node has exactly two children (yes / no questions).\n"
      ],
      "metadata": {
        "id": "cLjS-zuMp9hg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other algorithms (such as ID3) can produce nodes with more than two children,\n",
        "but Scikit-Learn does not use them.\n"
      ],
      "metadata": {
        "id": "MhEd5fqTp-VF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Boundaries\n",
        "\n",
        "Each split in the tree corresponds to a decision boundary in feature space.\n"
      ],
      "metadata": {
        "id": "yGJ73rV8p_A6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The root node creates a vertical boundary at petal length = 2.45 cm\n",
        "- The right child creates a horizontal boundary at petal width = 1.75 cm\n"
      ],
      "metadata": {
        "id": "-4Rlb8dRp__T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since `max_depth=2`, the tree stops splitting after depth 2.\n",
        "\n",
        "Increasing `max_depth` would add additional decision boundaries.\n"
      ],
      "metadata": {
        "id": "9goo7zbkqBl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tree’s full structure is accessible via:\n",
        "\n",
        "tree_clf.tree_\n",
        "\n",
        "You can inspect split thresholds, class counts, and impurities directly.\n"
      ],
      "metadata": {
        "id": "ADW_zusuqDam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## White Box vs Black Box Models\n",
        "\n",
        "Decision trees are considered **white box models**.\n",
        "\n",
        "Their predictions are easy to understand and explain.\n"
      ],
      "metadata": {
        "id": "0y21niGTqEjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In contrast, models like random forests and neural networks are often\n",
        "considered **black box models**.\n",
        "\n",
        "They can make excellent predictions but are harder to interpret.\n"
      ],
      "metadata": {
        "id": "0mIFgLVgqFX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretability is especially important in fields such as:\n",
        "\n",
        "- Healthcare\n",
        "- Finance\n",
        "- Law\n",
        "- Human resources\n"
      ],
      "metadata": {
        "id": "1er43fRPqGLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees provide clear, human-readable rules that support\n",
        "transparent and accountable decision-making.\n"
      ],
      "metadata": {
        "id": "T6mRx369qG3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estimating Class Probabilities\n",
        "\n",
        "A decision tree can also estimate the probability that an instance belongs\n",
        "to a particular class k.\n"
      ],
      "metadata": {
        "id": "rD31T3ISqJrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, it traverses the tree to find the leaf node for this instance.\n"
      ],
      "metadata": {
        "id": "LXLNA56XrPk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then it returns the proportion of instances of class k among the training\n",
        "instances that would also reach this leaf node.\n"
      ],
      "metadata": {
        "id": "6Xq0ED38rRLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, suppose you have found a flower whose petals are 5 cm long\n",
        "and 1.5 cm wide.\n"
      ],
      "metadata": {
        "id": "YzEsgwVtrSGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The corresponding leaf node is the depth-2 left node, so the decision tree\n",
        "outputs the following probabilities:\n"
      ],
      "metadata": {
        "id": "J0X97IO4rSzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 0% for Iris setosa (0 / 54)\n",
        "- 90.7% for Iris versicolor (49 / 54)\n",
        "- 9.3% for Iris virginica (5 / 54)\n"
      ],
      "metadata": {
        "id": "3FmaoEMsrTh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you ask the tree to predict the class, it outputs **Iris versicolor**\n",
        "(class 1) because it has the highest probability.\n"
      ],
      "metadata": {
        "id": "Z3YxPtutrUaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s check this:\n"
      ],
      "metadata": {
        "id": "K2uFMxOfrVFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_clf.predict_proba([[5, 1.5]]).round(3)\n"
      ],
      "metadata": {
        "id": "-Jv9mVKKrWIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_clf.predict([[5, 1.5]])\n"
      ],
      "metadata": {
        "id": "eAg8pjK1rXIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the estimated probabilities would be identical anywhere else\n",
        "in the bottom-right rectangle of Figure 5-2.\n"
      ],
      "metadata": {
        "id": "EfZpcnXZrYvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, the probabilities would be the same if the petals were\n",
        "6 cm long and 1.5 cm wide.\n"
      ],
      "metadata": {
        "id": "bfVYllPXriZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is true even though it seems obvious that such a flower would most\n",
        "likely be an Iris virginica in this case.\n"
      ],
      "metadata": {
        "id": "PjeKJKCTrkeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The CART Training Algorithm\n"
      ],
      "metadata": {
        "id": "4RAjLzS5topv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn uses the **Classification and Regression Tree (CART)** algorithm\n",
        "to train decision trees (also called “growing” trees).\n"
      ],
      "metadata": {
        "id": "sfRQjTAstpON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The algorithm works by first splitting the training set into two subsets\n",
        "using a single feature *k* and a threshold *tₖ*\n",
        "(e.g., “petal length ≤ 2.45 cm”).\n"
      ],
      "metadata": {
        "id": "6CWj3PRZtp2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does it choose *k* and *tₖ*?\n"
      ],
      "metadata": {
        "id": "NYqeQliJtqtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It searches for the pair (*k*, *tₖ*) that produces the **purest subsets**,\n",
        "weighted by their size.\n"
      ],
      "metadata": {
        "id": "n8zM-tYotrV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Equation 5-2 gives the cost function that the algorithm tries to minimize.\n"
      ],
      "metadata": {
        "id": "_Huj9lSVtr_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equation 5-2. CART cost function for classification**\n"
      ],
      "metadata": {
        "id": "rvVAodI5tsxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the CART algorithm has successfully split the training set in two,\n",
        "it splits the subsets using the same logic, then the sub-subsets, and so on,\n",
        "recursively.\n"
      ],
      "metadata": {
        "id": "KOMda0JNttpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It stops recursing once it reaches the maximum depth\n",
        "(defined by the `max_depth` hyperparameter),\n",
        "or if it cannot find a split that will reduce impurity.\n"
      ],
      "metadata": {
        "id": "KgLVye0Xtui0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few other hyperparameters control additional stopping conditions:\n"
      ],
      "metadata": {
        "id": "iLcFU_8utvYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `min_samples_split`\n",
        "- `min_samples_leaf`\n",
        "- `max_leaf_nodes`\n",
        "- and others\n"
      ],
      "metadata": {
        "id": "TD3cjJFgtwMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WARNING\n"
      ],
      "metadata": {
        "id": "5c5ECWBttxES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CART algorithm is a **greedy algorithm**:\n",
        "it greedily searches for an optimal split at the top level,\n",
        "then repeats the process at each subsequent level.\n"
      ],
      "metadata": {
        "id": "5N76p3R9tyr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It does **not** check whether a split will lead to the lowest possible\n",
        "impurity several levels down.\n"
      ],
      "metadata": {
        "id": "FzG5MdsBt0GD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A greedy algorithm often produces a solution that is reasonably good,\n",
        "but it is not guaranteed to be optimal.\n"
      ],
      "metadata": {
        "id": "Le7GNwv3t14r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, finding the optimal decision tree is known to be an\n",
        "**NP-complete problem**.\n"
      ],
      "metadata": {
        "id": "ewBwGd8tt3Bz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It requires exponential time, making the problem intractable\n",
        "even for small training sets.\n"
      ],
      "metadata": {
        "id": "xSW4zGoet33v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is why we must settle for a “reasonably good” solution\n",
        "when training decision trees.\n"
      ],
      "metadata": {
        "id": "f5lWRGovt4mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computational Complexity\n"
      ],
      "metadata": {
        "id": "0Bna9RcRuRh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making predictions requires traversing the decision tree\n",
        "from the root to a leaf.\n"
      ],
      "metadata": {
        "id": "iJJq7yPHuS48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees are generally approximately balanced,\n",
        "so traversing the tree requires going through roughly\n",
        "O(log₂(m)) nodes,\n",
        "where *m* is the number of training instances.\n"
      ],
      "metadata": {
        "id": "8VYNamF_uToT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, log₂(m) is the binary logarithm of *m*,\n",
        "equal to log(m) / log(2).\n"
      ],
      "metadata": {
        "id": "IP4OK5OHuUgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since each node only requires checking the value of one feature,\n",
        "the overall prediction complexity is **O(log₂(m))**,\n",
        "independent of the number of features.\n"
      ],
      "metadata": {
        "id": "RyK2P20PuVYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a result, predictions are very fast,\n",
        "even when dealing with large training sets.\n"
      ],
      "metadata": {
        "id": "Sy1sWYj1uWN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, the training algorithm compares **all features**\n",
        "on **all samples** at each node.\n"
      ],
      "metadata": {
        "id": "KzYuO3D6uW_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This results in a training complexity of:\n"
      ],
      "metadata": {
        "id": "hNeRl6O1uX3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**O(n × m log₂(m))**\n"
      ],
      "metadata": {
        "id": "xQqoLCeXuYgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "where *n* is the number of features\n",
        "and *m* is the number of training instances.\n"
      ],
      "metadata": {
        "id": "DngduwR2uZIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possible to speed up training by limiting\n",
        "the growth of the tree.\n"
      ],
      "metadata": {
        "id": "GazQ5mGuuadi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, you can:\n"
      ],
      "metadata": {
        "id": "rfxE7sSoubMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Set a maximum tree depth using `max_depth`\n",
        "- Set a maximum number of features to consider at each node\n",
        "  (features are then chosen randomly)\n"
      ],
      "metadata": {
        "id": "PSWcREItub4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These techniques help speed up training considerably\n",
        "and can also reduce the risk of overfitting.\n"
      ],
      "metadata": {
        "id": "UMVk-4Zbucum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, as always, going too far may result in underfitting.\n"
      ],
      "metadata": {
        "id": "ESYJJVEyudcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gini Impurity or Entropy?\n"
      ],
      "metadata": {
        "id": "f9kc4PVuvGQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, the `DecisionTreeClassifier` class uses the **Gini impurity**\n",
        "measure.\n"
      ],
      "metadata": {
        "id": "LdRWS0-bvHzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can instead use **entropy** by setting the `criterion`\n",
        "hyperparameter to `\"entropy\"`.\n"
      ],
      "metadata": {
        "id": "aq-DoMRdvIau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The concept of **entropy** originated in thermodynamics\n",
        "as a measure of molecular disorder.\n"
      ],
      "metadata": {
        "id": "DoYby8zDvJOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entropy approaches zero when molecules are still\n",
        "and well ordered.\n"
      ],
      "metadata": {
        "id": "o4IcOcUovKDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entropy later spread to many other fields,\n",
        "including **Shannon’s information theory**,\n",
        "where it measures the average information content of a message.\n"
      ],
      "metadata": {
        "id": "c5wiDCw0vLJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In information theory, entropy is zero\n",
        "when all messages are identical.\n"
      ],
      "metadata": {
        "id": "-dKmBcdevO6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, entropy is frequently used\n",
        "as an **impurity measure**.\n"
      ],
      "metadata": {
        "id": "Oewwi6N7vPoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A set’s entropy is zero when it contains\n",
        "instances of only **one class**.\n"
      ],
      "metadata": {
        "id": "LtM3kk13vQQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Equation 5-3 defines the entropy of the *i*th node.\n"
      ],
      "metadata": {
        "id": "fYPsHueXvQ8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, the depth-2 left node in Figure 5-1\n",
        "has an entropy of approximately **0.445**.\n"
      ],
      "metadata": {
        "id": "wKwUsOEKvRxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This value is computed as:\n",
        "\n",
        "−(49/54) log₂(49/54) − (5/54) log₂(5/54)\n"
      ],
      "metadata": {
        "id": "lAOBC03DvTfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, should you use **Gini impurity** or **entropy**?\n"
      ],
      "metadata": {
        "id": "OYS0cRBdvUUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the time, it does **not** make a big difference:\n",
        "both criteria tend to produce very similar trees.\n"
      ],
      "metadata": {
        "id": "ItEXgTztvVOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini impurity is slightly **faster to compute**,\n",
        "which makes it a good default choice.\n"
      ],
      "metadata": {
        "id": "LM9xTDvEvWD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the two criteria differ, they tend to behave differently:\n"
      ],
      "metadata": {
        "id": "kdtW6k_TvW0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Gini impurity** tends to isolate the most frequent class\n",
        "  in its own branch\n"
      ],
      "metadata": {
        "id": "AsmeZOTZvYCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Entropy** tends to produce slightly more balanced trees\n"
      ],
      "metadata": {
        "id": "6DVQ5-rlvY1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization Hyperparameters\n"
      ],
      "metadata": {
        "id": "pe_3qWLuwoqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees make very few assumptions about the training data.\n",
        "\n",
        "Unlike linear models, they do not assume linear relationships.\n"
      ],
      "metadata": {
        "id": "dG7Gz6Aowp-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If left unconstrained, a decision tree will adapt itself very closely\n",
        "to the training data, often resulting in **overfitting**.\n"
      ],
      "metadata": {
        "id": "oJH1xq8nwqj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Such models are often called **nonparametric models**.\n"
      ],
      "metadata": {
        "id": "-N7uJPEcwrqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This does *not* mean they have no parameters.\n",
        "\n",
        "It means the number of parameters is **not fixed before training**.\n"
      ],
      "metadata": {
        "id": "I17wbKfGwspf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model structure is free to grow and fit the data as closely\n",
        "as possible.\n"
      ],
      "metadata": {
        "id": "C68aus_Nwtau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In contrast, **parametric models** (such as linear regression)\n",
        "have a fixed number of parameters.\n"
      ],
      "metadata": {
        "id": "LWBIQ75IwuDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This limits their flexibility, reducing overfitting\n",
        "but increasing the risk of underfitting.\n"
      ],
      "metadata": {
        "id": "4DulQEavwuzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid overfitting decision trees, we must restrict their freedom.\n",
        "\n",
        "This process is called **regularization**.\n"
      ],
      "metadata": {
        "id": "KA_szM92wvtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most common form of regularization is limiting the **maximum depth**\n",
        "of the tree.\n"
      ],
      "metadata": {
        "id": "VRvDZ7O9wwTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Scikit-Learn, this is controlled by the `max_depth` hyperparameter.\n",
        "\n",
        "- Default: `None` (unlimited depth)\n",
        "- Smaller values → stronger regularization\n"
      ],
      "metadata": {
        "id": "YtLRLN9UwxOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reducing `max_depth` limits how deep the tree can grow,\n",
        "helping prevent overfitting.\n"
      ],
      "metadata": {
        "id": "fS9ZIO8Hwx6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `DecisionTreeClassifier` includes several additional\n",
        "regularization hyperparameters.\n"
      ],
      "metadata": {
        "id": "YHK1d2wiwyvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common Decision Tree Regularization Hyperparameters\n"
      ],
      "metadata": {
        "id": "7atdD0wywzaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **max_features**  \n",
        "  Maximum number of features evaluated at each split\n"
      ],
      "metadata": {
        "id": "tvGwObTAw0GC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **max_leaf_nodes**  \n",
        "  Maximum number of leaf nodes in the tree\n"
      ],
      "metadata": {
        "id": "v1H1-d4pw09P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **min_samples_split**  \n",
        "  Minimum number of samples required to split a node\n"
      ],
      "metadata": {
        "id": "E2-2moR-w1tK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **min_samples_leaf**  \n",
        "  Minimum number of samples required in a leaf node\n"
      ],
      "metadata": {
        "id": "nLzpdTZ_w2X4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **min_weight_fraction_leaf**  \n",
        "  Same as `min_samples_leaf`, but expressed as a fraction\n",
        "  of the total number of weighted samples\n"
      ],
      "metadata": {
        "id": "9IDA9ryzw3vp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **min_impurity_decrease**  \n",
        "  A node will only be split if it reduces impurity\n",
        "  by at least this amount\n"
      ],
      "metadata": {
        "id": "VWeGP9e1w41L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ccp_alpha**  \n",
        "  Controls **minimal cost-complexity pruning (MCCP)**\n",
        "\n",
        "  Larger values → more pruning → smaller trees\n"
      ],
      "metadata": {
        "id": "oKRKDnmjw6LX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To reduce model complexity:\n",
        "\n",
        "- Increase `min_*` hyperparameters or `ccp_alpha`\n",
        "- Decrease `max_*` hyperparameters\n"
      ],
      "metadata": {
        "id": "OWaNC32iw78I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice:\n",
        "\n",
        "- Tuning `max_depth` is a great default\n",
        "- `min_samples_leaf` is especially useful for small datasets\n",
        "- `max_features` helps with high-dimensional data\n"
      ],
      "metadata": {
        "id": "mn8eyxVBw9ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note on Pruning\n"
      ],
      "metadata": {
        "id": "xaus-cmkw-uA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some algorithms first grow the tree fully,\n",
        "then **prune unnecessary nodes** afterward.\n"
      ],
      "metadata": {
        "id": "usOf_Ow-w_XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A node may be pruned if its purity improvement\n",
        "is not statistically significant.\n"
      ],
      "metadata": {
        "id": "nH2j6Q8OxAUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical tests such as the χ² (chi-squared) test\n",
        "are used to estimate whether an improvement\n",
        "could be due to chance.\n"
      ],
      "metadata": {
        "id": "fyxaA6FdxBCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the p-value exceeds a threshold (typically 5%),\n",
        "the node is removed.\n"
      ],
      "metadata": {
        "id": "0_p5ZbMXxB2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn primarily uses **pre-pruning**\n",
        "via hyperparameters like `max_depth` and `min_samples_leaf`.\n"
      ],
      "metadata": {
        "id": "38q7S3OTxCzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s test regularization on the **moons dataset**.\n",
        "\n",
        "This is a toy binary classification dataset shaped like\n",
        "two interleaving crescent moons.\n"
      ],
      "metadata": {
        "id": "Uu1igjjwxDka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.tree import DecisionTreeClassifier\n"
      ],
      "metadata": {
        "id": "-o9moNU1xERv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_moons, y_moons = make_moons(\n",
        "    n_samples=150,\n",
        "    noise=0.2,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "jg_0LQAcxFHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train two decision trees:\n",
        "\n",
        "- One **without regularization**\n",
        "- One with `min_samples_leaf = 5`\n"
      ],
      "metadata": {
        "id": "YI8ZemZWxF-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_clf1 = DecisionTreeClassifier(random_state=42)\n",
        "tree_clf2 = DecisionTreeClassifier(min_samples_leaf=5, random_state=42)\n",
        "\n",
        "tree_clf1.fit(X_moons, y_moons)\n",
        "tree_clf2.fit(X_moons, y_moons)\n"
      ],
      "metadata": {
        "id": "ouSWGbjgxG1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The unregularized tree overfits the data,\n",
        "while the regularized tree produces smoother decision boundaries.\n"
      ],
      "metadata": {
        "id": "Oxd-Y1ZQxHqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify this, we evaluate both models on a **new test set**\n",
        "generated with a different random seed.\n"
      ],
      "metadata": {
        "id": "8gsBVRF8xItl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_moons_test, y_moons_test = make_moons(\n",
        "    n_samples=1000,\n",
        "    noise=0.2,\n",
        "    random_state=43\n",
        ")\n"
      ],
      "metadata": {
        "id": "HZIr-Oo0xUoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_clf1.score(X_moons_test, y_moons_test)\n"
      ],
      "metadata": {
        "id": "e3eXLQbXxWKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_clf2.score(X_moons_test, y_moons_test)\n"
      ],
      "metadata": {
        "id": "QPCtBj-MxXQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The regularized model achieves higher test accuracy,\n",
        "confirming that it generalizes better.\n"
      ],
      "metadata": {
        "id": "hs2ALatNxV7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression\n"
      ],
      "metadata": {
        "id": "mh3gcremxV4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees are also capable of performing **regression tasks**.\n",
        "\n",
        "Unlike linear regression, decision trees can model\n",
        "highly **nonlinear relationships**.\n"
      ],
      "metadata": {
        "id": "FVsFtmVoxmqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "They can fit complex datasets by recursively splitting\n",
        "the feature space into regions and predicting a constant\n",
        "value in each region.\n"
      ],
      "metadata": {
        "id": "BHkZKzXNxnWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s build a **regression tree** using Scikit-Learn’s\n",
        "`DecisionTreeRegressor`.\n"
      ],
      "metadata": {
        "id": "19NKob-axoDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train it on a **noisy quadratic dataset**\n",
        "and restrict the tree depth to avoid overfitting.\n"
      ],
      "metadata": {
        "id": "RwFaJ0kKxoyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n"
      ],
      "metadata": {
        "id": "U-OXVk-XxpZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "X_quad = rng.random((200, 1)) - 0.5  # one input feature\n",
        "y_quad = X_quad ** 2 + 0.025 * rng.standard_normal((200, 1))\n"
      ],
      "metadata": {
        "id": "EcBaGV5jxqaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now train a regression tree with `max_depth = 2`.\n"
      ],
      "metadata": {
        "id": "VCjelwZIxrDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg.fit(X_quad, y_quad)\n"
      ],
      "metadata": {
        "id": "QLV1V76Nxrts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting tree structure is shown in Figure 5-4.\n"
      ],
      "metadata": {
        "id": "EdAQO-GkxsdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This regression tree is very similar to a classification tree.\n",
        "\n",
        "The key difference is that **each node predicts a numerical value**\n",
        "instead of a class label.\n"
      ],
      "metadata": {
        "id": "nJqaai6rxtU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each prediction is the **average target value**\n",
        "of the training instances that reach the leaf node.\n"
      ],
      "metadata": {
        "id": "0rUu-8NdxuPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Prediction\n"
      ],
      "metadata": {
        "id": "g0H3XyJmxu8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we want to predict the value for a new instance with:\n",
        "\n",
        "x₁ = 0.2\n"
      ],
      "metadata": {
        "id": "orCve8a9xviw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tree traversal works as follows:\n",
        "\n",
        "1. Root node checks whether x₁ ≤ 0.343 → True\n",
        "2. Move to left child\n",
        "3. Check whether x₁ ≤ −0.302 → False\n",
        "4. Move to right child (leaf)\n"
      ],
      "metadata": {
        "id": "Sx1V5fVCxwYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The leaf node predicts:\n",
        "\n",
        "value = 0.038\n"
      ],
      "metadata": {
        "id": "g1hnAmNsxxYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The leaf node predicts:\n",
        "\n",
        "value = 0.038\n"
      ],
      "metadata": {
        "id": "zrQ50vyhxyBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This value is the **mean of the target values**\n",
        "of the 133 training instances in that leaf.\n"
      ],
      "metadata": {
        "id": "ZdqcyIqdxyxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mean squared error (MSE) for these instances\n",
        "is approximately 0.002.\n"
      ],
      "metadata": {
        "id": "cHL2kleoxzY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 5-5 shows the predictions produced by two regression trees:\n",
        "\n",
        "- Left: `max_depth = 2`\n",
        "- Right: `max_depth = 3`\n"
      ],
      "metadata": {
        "id": "-_7r4jcGx0Vx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In each region, the predicted value is always the **average**\n",
        "of the target values in that region.\n"
      ],
      "metadata": {
        "id": "3g5pX5Kkx1Ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CART algorithm splits regions so that most training instances\n",
        "are as close as possible to the predicted value.\n"
      ],
      "metadata": {
        "id": "K0-U_t0_x16j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CART Cost Function for Regression\n"
      ],
      "metadata": {
        "id": "d-MIG-Fgx2qO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For regression tasks, CART does **not** minimize impurity.\n",
        "\n",
        "Instead, it minimizes the **mean squared error (MSE)**.\n"
      ],
      "metadata": {
        "id": "mZ7jd4Bwx3cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At each split, the algorithm searches for the feature and threshold\n",
        "that produce child nodes with the **lowest weighted MSE**.\n"
      ],
      "metadata": {
        "id": "WzbxlCtxx4Se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is known as the CART cost function for regression\n",
        "(Equation 5-4).\n"
      ],
      "metadata": {
        "id": "kuUQyIirx5ny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overfitting in Regression Trees\n"
      ],
      "metadata": {
        "id": "9YX6D4FZx6bp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like classification trees, regression trees are\n",
        "**prone to overfitting**.\n"
      ],
      "metadata": {
        "id": "42WuOgYPx7Rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If no regularization is applied, the tree will often fit\n",
        "the training data extremely closely.\n"
      ],
      "metadata": {
        "id": "UCROk5qUx8YH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 5-6 (left) shows predictions from an **unregularized**\n",
        "regression tree.\n"
      ],
      "metadata": {
        "id": "EfLbx37zx9uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These predictions clearly overfit the training data.\n"
      ],
      "metadata": {
        "id": "G9L7X9gOx-6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By setting `min_samples_leaf = 10`,\n",
        "we force each leaf to contain at least 10 samples.\n"
      ],
      "metadata": {
        "id": "Z8NDAiFJyAHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This produces a **much smoother and more reasonable model**,\n",
        "shown on the right in Figure 5-6.\n"
      ],
      "metadata": {
        "id": "NCtHMUDVyA26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_reg_regularized = DecisionTreeRegressor(\n",
        "    min_samples_leaf=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "tree_reg_regularized.fit(X_quad, y_quad)\n"
      ],
      "metadata": {
        "id": "ZFOStp03x5VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization significantly improves generalization\n",
        "by preventing overly fine splits.\n"
      ],
      "metadata": {
        "id": "serV-XvmyDGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sensitivity to Axis Orientation\n"
      ],
      "metadata": {
        "id": "MuIi1LZ0yMI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees have many advantages: they are easy to interpret,\n",
        "simple to use, versatile, and powerful.\n"
      ],
      "metadata": {
        "id": "XGZtZSm_yOyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, they also have important **limitations**.\n"
      ],
      "metadata": {
        "id": "_PaErXR0yQeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One key limitation is that decision trees prefer **orthogonal decision boundaries**.\n",
        "\n",
        "All splits are perpendicular to a feature axis.\n"
      ],
      "metadata": {
        "id": "LsR2VuiIyRVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because of this, decision trees are **sensitive to the orientation of the data**.\n"
      ],
      "metadata": {
        "id": "YQ1y0Z8BySG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 5-7 illustrates this problem using a simple linearly separable dataset.\n"
      ],
      "metadata": {
        "id": "lyuFG-8QyS0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- On the left, the dataset is aligned with the axes.\n",
        "  A decision tree splits it cleanly.\n",
        "- On the right, the dataset is rotated by 45°.\n",
        "  The resulting decision boundary becomes unnecessarily complex.\n"
      ],
      "metadata": {
        "id": "nZhwPES9yVii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although both trees fit the training data perfectly,\n",
        "the rotated version is **much more likely to overfit**\n",
        "and generalize poorly.\n"
      ],
      "metadata": {
        "id": "71Rex2B_yYIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reducing Sensitivity with PCA\n"
      ],
      "metadata": {
        "id": "h9UyZsL4ybxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to reduce this problem is to:\n",
        "1. Scale the data\n",
        "2. Apply **Principal Component Analysis (PCA)**\n"
      ],
      "metadata": {
        "id": "dubDgAvBydRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA rotates the dataset to reduce feature correlation.\n",
        "\n",
        "This often (though not always) makes it easier for decision trees\n",
        "to find simple splits.\n"
      ],
      "metadata": {
        "id": "EC-nFsaeyeE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will study PCA in detail in Chapter 7.\n",
        "\n",
        "For now, it is enough to know that PCA creates new features that\n",
        "are linear combinations of the original ones.\n"
      ],
      "metadata": {
        "id": "Mp5wxUfUyfEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s build a pipeline that:\n",
        "- Scales the data\n",
        "- Applies PCA\n",
        "- Trains a decision tree\n"
      ],
      "metadata": {
        "id": "1RAxxGsWygAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n"
      ],
      "metadata": {
        "id": "rb__CT9wygn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We reuse the Iris dataset features from earlier:\n",
        "petal length and petal width.\n"
      ],
      "metadata": {
        "id": "42D72TXHywx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_pipeline = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    PCA()\n",
        ")\n"
      ],
      "metadata": {
        "id": "gxU856XUyxuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now transform (rotate) the original feature space.\n"
      ],
      "metadata": {
        "id": "tfg9fdwVyy1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_iris_rotated = pca_pipeline.fit_transform(X_iris)\n"
      ],
      "metadata": {
        "id": "CM4meG_-y0EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we train a decision tree on the PCA-transformed data.\n"
      ],
      "metadata": {
        "id": "M-zQm8D1y1IZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_clf_pca = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
        "tree_clf_pca.fit(X_iris_rotated, y_iris)\n"
      ],
      "metadata": {
        "id": "UJj3t_Suy2X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 5-8 shows the resulting decision boundaries.\n"
      ],
      "metadata": {
        "id": "u2XthCXuy4qM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to PCA, the dataset can now be separated effectively\n",
        "using just **one principal component**, z₁.\n"
      ],
      "metadata": {
        "id": "9JvriSxIy6Jg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature z₁ is a linear combination of:\n",
        "- petal length\n",
        "- petal width\n"
      ],
      "metadata": {
        "id": "MruPWbg7y7d4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This rotation makes it possible for the tree to use\n",
        "simple, clean splits again.\n"
      ],
      "metadata": {
        "id": "9Yn-QQQAy8X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practical Tip\n"
      ],
      "metadata": {
        "id": "kiPqPPw5y9IT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both `DecisionTreeClassifier` and `DecisionTreeRegressor`\n",
        "support **missing values natively**.\n",
        "\n",
        "You do **not** need an imputer when using decision trees.\n"
      ],
      "metadata": {
        "id": "1kPW8BGCy9_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees Have a High Variance\n"
      ],
      "metadata": {
        "id": "IhCLNkWSzGyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main drawback of decision trees is that they suffer from **high variance**.\n"
      ],
      "metadata": {
        "id": "JfKK_K2tzJig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "High variance means that **small changes** in:\n",
        "- the training data, or\n",
        "- the hyperparameters\n",
        "\n",
        "can result in **very different models**.\n"
      ],
      "metadata": {
        "id": "dBkR1EuFzKPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even retraining the *same* decision tree on the *same* dataset\n",
        "can produce a noticeably different tree.\n"
      ],
      "metadata": {
        "id": "OJrIGYmLzLNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This happens because Scikit-Learn’s training algorithm is **stochastic**.\n"
      ],
      "metadata": {
        "id": "4Zl_VjuXzMDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At each node, the algorithm may randomly select\n",
        "which subset of features to evaluate for splitting.\n"
      ],
      "metadata": {
        "id": "qTGX82eLzM6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unless you explicitly fix the `random_state` hyperparameter,\n",
        "the training process is not fully deterministic.\n"
      ],
      "metadata": {
        "id": "Vz1vx5FTzN1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 5-9 shows an example of this effect.\n"
      ],
      "metadata": {
        "id": "GQBZGZMFzOkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the new tree is trained on the same data\n",
        "and uses the same hyperparameters,\n",
        "its structure looks very different from the earlier one.\n"
      ],
      "metadata": {
        "id": "IwInqMM2zPte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both trees may fit the training data well,\n",
        "but they can generalize differently to unseen data.\n"
      ],
      "metadata": {
        "id": "-zVlT-TrzQ3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This sensitivity is what makes decision trees unstable on their own.\n"
      ],
      "metadata": {
        "id": "11gIEKs8zR7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reducing Variance with Ensembles\n"
      ],
      "metadata": {
        "id": "LPqrkwkczSzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fortunately, there is a powerful way to reduce variance:\n",
        "**averaging many decision trees together**.\n"
      ],
      "metadata": {
        "id": "L5udbRE_zT1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of relying on a single high-variance tree,\n",
        "we train many trees and average their predictions.\n"
      ],
      "metadata": {
        "id": "0VxXPJyYzVTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This approach dramatically reduces variance\n",
        "while preserving the low bias of decision trees.\n"
      ],
      "metadata": {
        "id": "UHJY1K9AzWKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An ensemble of decision trees trained this way\n",
        "is called a **random forest**.\n"
      ],
      "metadata": {
        "id": "6cLdHv1wzXKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forests are among the most powerful\n",
        "and widely used machine learning models today.\n"
      ],
      "metadata": {
        "id": "oV1JQ9I-zYCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next chapter, we will study **random forests**\n",
        "and other ensemble methods in detail.\n"
      ],
      "metadata": {
        "id": "PWkygvjFzY9m"
      }
    }
  ]
}